,Unnamed: 0,Number,OR Link,Title,Authors,Session,Unnamed: 5,inoue,oki,rama,ishii,thibault,mukai,hashimoto,Keyword,Abstract,BS Session,Order,Unnamed: 6,Oral Session,base_txt,dim1,dim2,best_kw,category,id,disp_txt
0,0,1,https://openreview.net/forum?id=sWBqOL5Nh4P,Learning Density Distribution of Reachable States for Autonomous Systems,"Yue Meng, Dawei Sun, Zeng Qiu, Md Tawhid Bin Waez, Chuchu Fan",1.0,,,,,,,False,,"Reachability Density Distribution, Learning Density Distribution, Liouville Theorem","State density distribution, in contrast to worst-case reachability, can be leveraged for safety-related problems to better quantify the likelihood of the risk for potentially hazardous situations. In this work, we propose a data-driven method to compute the density distribution of reachable states for nonlinear and even black-box systems. Our semi-supervised approach learns system dynamics and the state density jointly from trajectory data, guided by the fact that the state density evolution follows the Liouville partial differential equation. With the help of neural network reachability tools, our approach can estimate the set of all possible future states as well as their density. Moreover, we could perform online safety verification with probability ranges for unsafe behaviors to occur. We use an extensive set of experiments to show that our learned solution can produce a much more accurate estimate on density distribution, and can quantify risks less conservatively and flexibly comparing with worst-case analysis.",,,,,"learning density distribution of reachable states for autonomous systems reachability density distribution, learning density distribution, liouville theorem state density distribution, in contrast to worst-case reachability, can be leveraged for safety-related problems to better quantify the likelihood of the risk for potentially hazardous situations. in this work, we propose a data-driven method to compute the density distribution of reachable states for nonlinear and even black-box systems. our semi-supervised approach learns system dynamics and the state density jointly from trajectory data, guided by the fact that the state density evolution follows the liouville partial differential equation. with the help of neural network reachability tools, our approach can estimate the set of all possible future states as well as their density. moreover, we could perform online safety verification with probability ranges for unsafe behaviors to occur. we use an extensive set of experiments to show that our learned solution can produce a much more accurate estimate on density distribution, and can quantify risks less conservatively and flexibly comparing with worst-case analysis.",-1.9929007,4.2241416,density,no reinforce,0,"id:0 (density)<br><b>Learning Density Distribution of Reachable States for Autonomous Systems</b><br>kw:Reachability Density Distribution, Learning Density Distribution, Liouville Theorem"
1,1,47,https://openreview.net/forum?id=2NcPgLa7yqD,Optical Tactile Sim-to-Real Policy Transfer via Real-to-Sim Tactile Image Translation,"Alex Church, John Lloyd, raia hadsell, Nathan F. Lepora",1.0,,,,,,,False,,"Tactile Robotics, Sim2Real, Reinforcement Learning","Simulation has recently become key for deep reinforcement learning to safely and efficiently acquire general and complex control policies from visual and proprioceptive inputs. Tactile information is not usually considered despite its direct relation to environment interaction. In this work, we present a suite of simulated environments tailored towards tactile robotics and reinforcement learning. A simple and fast method of simulating optical tactile sensors is provided, where high-resolution contact geometry is represented as depth images. Proximal Policy Optimisation (PPO) is used to learn successful policies across all considered tasks. A data-driven approach enables translation of the current state of a real tactile sensor to corresponding simulated depth images. This policy is implemented within a real-time control loop on a physical robot to demonstrate zero-shot sim-to-real policy transfer on several physically-interactive tasks requiring a sense of touch.",,,,,"optical tactile sim-to-real policy transfer via real-to-sim tactile image translation tactile robotics, sim2real, reinforcement learning simulation has recently become key for deep reinforcement learning to safely and efficiently acquire general and complex control policies from visual and proprioceptive inputs. tactile information is not usually considered despite its direct relation to environment interaction. in this work, we present a suite of simulated environments tailored towards tactile robotics and reinforcement learning. a simple and fast method of simulating optical tactile sensors is provided, where high-resolution contact geometry is represented as depth images. proximal policy optimisation (ppo) is used to learn successful policies across all considered tasks. a data-driven approach enables translation of the current state of a real tactile sensor to corresponding simulated depth images. this policy is implemented within a real-time control loop on a physical robot to demonstrate zero-shot sim-to-real policy transfer on several physically-interactive tasks requiring a sense of touch.",-2.6828387,3.2578013,tactile,reinforce,1,"id:1 (tactile)<br><b>Optical Tactile Sim-to-Real Policy Transfer via Real-to-Sim Tactile Image Translation</b><br>kw:Tactile Robotics, Sim2Real, Reinforcement Learning"
2,2,104,https://openreview.net/forum?id=4u25M570Iji,Motion Forecasting with Unlikelihood Training in Continuous Space,"Deyao Zhu, Mohamed Zahran, Li Erran Li, Mohamed Elhoseiny",1.0,,,,,,,False,,,"Motion forecasting is essential for making safe and intelligent decisions in robotic applications such as autonomous driving. Existing methods often formulate it as a sequence-to-sequence prediction problem, solved in an encoder-decoder framework with a maximum likelihood estimation objective. State-of-the-art models leverage contextual information including the map and states of surrounding agents. However,  we observe that they still assign a high probability to unlikely trajectories resulting in unsafe behaviors including road boundary violations. Orthogonally, we propose a new objective, unlikelihood training, which forces predicted trajectories that conflict with contextual information to be assigned a lower probability. We demonstrate that our method can improve state-of-art models' performance on challenging real-world trajectory forecasting datasets (nuScenes and Argoverse) by avoiding up to 56% context-violated prediction and improving up to 9% prediction accuracy.  Code will be made available.",,,,,"motion forecasting with unlikelihood training in continuous space nan motion forecasting is essential for making safe and intelligent decisions in robotic applications such as autonomous driving. existing methods often formulate it as a sequence-to-sequence prediction problem, solved in an encoder-decoder framework with a maximum likelihood estimation objective. state-of-the-art models leverage contextual information including the map and states of surrounding agents. however,  we observe that they still assign a high probability to unlikely trajectories resulting in unsafe behaviors including road boundary violations. orthogonally, we propose a new objective, unlikelihood training, which forces predicted trajectories that conflict with contextual information to be assigned a lower probability. we demonstrate that our method can improve state-of-art models' performance on challenging real-world trajectory forecasting datasets (nuscenes and argoverse) by avoiding up to 56% context-violated prediction and improving up to 9% prediction accuracy.  code will be made available.",-2.131907,3.644637,forecasting,no reinforce,2,id:2 (forecasting)<br><b>Motion Forecasting with Unlikelihood Training in Continuous Space</b><br>kw:nan
3,3,108,https://openreview.net/forum?id=vm8Hr9YJHZ1,Fast and Efficient Locomotion via Learned Gait Transitions,"Yuxiang Yang, Tingnan Zhang, Erwin Coumans, Jie Tan, Byron Boots",1.0,,,,,,,False,,"Legged Locomotion, Hierarchical Control, Reinforcement Learning","We focus on the problem of developing energy efficient controllers for quadrupedal robots. Animals can actively switch gaits at different speeds to lower their energy consumption. In this paper, we devise a hierarchical learning framework, in which distinctive locomotion gaits and natural gait transitions emerge automatically with a simple reward of energy minimization.  We use evolutionary strategies (ES) to train a high-level gait policy that specifies gait patterns of each foot, while the low-level convex MPC controller optimizes the motor commands so that the robot can walk at a desired velocity using that gait pattern. We test our learning framework on a quadruped robot and demonstrate automatic gait transitions, from walking to trotting and to fly-trotting, as the robot increases its speed. We show that the learned hierarchical controller consumes much less energy across a wide range of locomotion speed than baseline controllers.",,,,,"fast and efficient locomotion via learned gait transitions legged locomotion, hierarchical control, reinforcement learning we focus on the problem of developing energy efficient controllers for quadrupedal robots. animals can actively switch gaits at different speeds to lower their energy consumption. in this paper, we devise a hierarchical learning framework, in which distinctive locomotion gaits and natural gait transitions emerge automatically with a simple reward of energy minimization.  we use evolutionary strategies (es) to train a high-level gait policy that specifies gait patterns of each foot, while the low-level convex mpc controller optimizes the motor commands so that the robot can walk at a desired velocity using that gait pattern. we test our learning framework on a quadruped robot and demonstrate automatic gait transitions, from walking to trotting and to fly-trotting, as the robot increases its speed. we show that the learned hierarchical controller consumes much less energy across a wide range of locomotion speed than baseline controllers.",-2.148177,3.3691323,gait,reinforce,3,"id:3 (gait)<br><b>Fast and Efficient Locomotion via Learned Gait Transitions</b><br>kw:Legged Locomotion, Hierarchical Control, Reinforcement Learning"
4,4,153,https://openreview.net/forum?id=zwo1-MdMl1P,Robot Reinforcement Learning on the Constraint Manifold,"Puze Liu, Davide Tateo, Haitham Bou Ammar, Jan Peters",1.0,,,,,,,False,,"Robot Learning, Reinforcement Learning, Constrained Markov Decision Process, Safe Exploration","Reinforcement learning in robotics is extremely challenging due to many practical issues, including safety, mechanical constraints, and wear and tear. Typically, these issues are not considered in the machine learning literature. One crucial problem in applying reinforcement learning in the real world is Safe Exploration, which requires physical and safety constraints satisfaction throughout the learning process.  To explore in such a safety-critical environment, leveraging known information such as robot models and constraints is beneficial to provide more robust safety guarantees. Exploiting this knowledge, we propose a novel method to learn robotics tasks in simulation efficiently while satisfying the constraints during the learning process.",,,,,"robot reinforcement learning on the constraint manifold robot learning, reinforcement learning, constrained markov decision process, safe exploration reinforcement learning in robotics is extremely challenging due to many practical issues, including safety, mechanical constraints, and wear and tear. typically, these issues are not considered in the machine learning literature. one crucial problem in applying reinforcement learning in the real world is safe exploration, which requires physical and safety constraints satisfaction throughout the learning process.  to explore in such a safety-critical environment, leveraging known information such as robot models and constraints is beneficial to provide more robust safety guarantees. exploiting this knowledge, we propose a novel method to learn robotics tasks in simulation efficiently while satisfying the constraints during the learning process.",-2.6404147,2.2869692,constraints,reinforce,4,"id:4 (constraints)<br><b>Robot Reinforcement Learning on the Constraint Manifold</b><br>kw:Robot Learning, Reinforcement Learning, Constrained Markov Decision Process, Safe Exploration"
5,5,209,https://openreview.net/forum?id=87_OJU4sw3V,"ReSkin: versatile, replaceable, lasting tactile skins","Raunaq Bhirangi, Tess Hellebrekers, Carmel Majidi, Abhinav Gupta",1.0,,,,,,,False,,"Tactile Skin, Self-supervised Learning, Magnetic Sensing, Soft Sensors","Soft sensors have continued growing interest in robotics, due to their ability to enable both passive conformal contact from the material properties and active contact data from the sensor properties. However, the same properties of conformal contact result in faster deterioration of soft sensors and larger variations in their response characteristics over time and across samples, inhibiting their ability to be long-lasting and replaceable. ReSkin is a tactile soft sensor that leverages machine learning and magnetic sensing to offer a low-cost, diverse and compact solution for long-term use. Magnetic sensing separates the electronic circuitry from the passive interface, making it easier to replace interfaces as they wear out while allowing for a wide variety of form factors. Machine learning allows us to learn sensor response models that are robust to variations across fabrication and time, and our self-supervised learning algorithm enables finer performance enhancement with small, inexpensive data collection procedures. We believe that ReSkin opens the door to more versatile, scalable and inexpensive tactile sensation modules than existing alternatives.",,,,,"reskin: versatile, replaceable, lasting tactile skins tactile skin, self-supervised learning, magnetic sensing, soft sensors soft sensors have continued growing interest in robotics, due to their ability to enable both passive conformal contact from the material properties and active contact data from the sensor properties. however, the same properties of conformal contact result in faster deterioration of soft sensors and larger variations in their response characteristics over time and across samples, inhibiting their ability to be long-lasting and replaceable. reskin is a tactile soft sensor that leverages machine learning and magnetic sensing to offer a low-cost, diverse and compact solution for long-term use. magnetic sensing separates the electronic circuitry from the passive interface, making it easier to replace interfaces as they wear out while allowing for a wide variety of form factors. machine learning allows us to learn sensor response models that are robust to variations across fabrication and time, and our self-supervised learning algorithm enables finer performance enhancement with small, inexpensive data collection procedures. we believe that reskin opens the door to more versatile, scalable and inexpensive tactile sensation modules than existing alternatives.",-1.3720126,4.616495,soft,no reinforce,5,"id:5 (soft)<br><b>ReSkin: versatile, replaceable, lasting tactile skins</b><br>kw:Tactile Skin, Self-supervised Learning, Magnetic Sensing, Soft Sensors"
6,6,217,https://openreview.net/forum?id=8ZL7Jh1r8WX,Task-Driven Out-of-Distribution Detection with Statistical Guarantees for Robot Learning,"Alec Farid, Sushant Veer, Anirudha Majumdar",1.0,,,,,,,False,,"Out-of-distribution detection, generalization, PAC-Bayes","Our goal is to perform out-of-distribution (OOD) detection, i.e., to detect when a robot is operating in environments that are drawn from a different distribution than the environments used to train the robot. We leverage Probably Approximately Correct (PAC)-Bayes theory in order to train a policy with a guaranteed bound on performance on the training distribution. Our key idea for OOD detection then relies on the following intuition: violation of the performance bound on test environments provides evidence that the robot is operating OOD. We formalize this via statistical techniques based on p-values and concentration inequalities. The resulting approach (i) provides guaranteed confidence bounds on OOD detection, and (ii) is task-driven and sensitive only to changes that impact the robot’s performance. We demonstrate our approach on a simulated example of grasping objects with unfamiliar poses or shapes. We also present both simulation and hardware experiments for a drone performing vision-based obstacle avoidance in unfamiliar environments (including wind disturbances and different obstacle densities). Our examples demonstrate that we can perform task-driven OOD detection within just a handful of trials. Comparisons with baselines also demonstrate the advantages of our approach in terms of providing statistical guarantees and being insensitive to task-irrelevant distribution shifts.",,,,,"task-driven out-of-distribution detection with statistical guarantees for robot learning out-of-distribution detection, generalization, pac-bayes our goal is to perform out-of-distribution (ood) detection, i.e., to detect when a robot is operating in environments that are drawn from a different distribution than the environments used to train the robot. we leverage probably approximately correct (pac)-bayes theory in order to train a policy with a guaranteed bound on performance on the training distribution. our key idea for ood detection then relies on the following intuition: violation of the performance bound on test environments provides evidence that the robot is operating ood. we formalize this via statistical techniques based on p-values and concentration inequalities. the resulting approach (i) provides guaranteed confidence bounds on ood detection, and (ii) is task-driven and sensitive only to changes that impact the robot’s performance. we demonstrate our approach on a simulated example of grasping objects with unfamiliar poses or shapes. we also present both simulation and hardware experiments for a drone performing vision-based obstacle avoidance in unfamiliar environments (including wind disturbances and different obstacle densities). our examples demonstrate that we can perform task-driven ood detection within just a handful of trials. comparisons with baselines also demonstrate the advantages of our approach in terms of providing statistical guarantees and being insensitive to task-irrelevant distribution shifts.",-2.729727,3.9463959,task-driven,no reinforce,6,"id:6 (task-driven)<br><b>Task-Driven Out-of-Distribution Detection with Statistical Guarantees for Robot Learning</b><br>kw:Out-of-distribution detection, generalization, PAC-Bayes"
7,7,249,https://openreview.net/forum?id=1mDC24WX8Yh,Tactile Image-to-Image Disentanglement of Contact Geometry from Motion-Induced Shear,"Anupam K. Gupta, Laurence Aitchison, Nathan F. Lepora",1.0,,,,,,,False,,"Reinforcement Learning, Imitation Learning, Skill-Based Transfer Learning","Demonstration-guided reinforcement learning (RL) is a promising approach for learning complex behaviors by leveraging both reward feedback and a set of target task demonstrations. Prior approaches for demonstration-guided RL treat every new task as an independent learning problem and attempt to follow the provided demonstrations step-by-step, akin to a human trying to imitate a completely unseen behavior by following the demonstrator's exact muscle movements. Naturally, such learning will be slow, but often new behaviors are not completely unseen: they share subtasks with behaviors we have previously learned. In this work, we aim to exploit this shared subtask structure to increase the efficiency of demonstration-guided RL. We first learn a set of reusable skills from large offline datasets of prior experience collected across many tasks. We then propose Skill-based Learning with Demonstrations (SkiLD), an algorithm for demonstration-guided RL that efficiently leverages the provided demonstrations by following the demonstrated skills instead of the primitive actions, resulting in substantial performance improvements over prior demonstration-guided RL approaches. We validate the effectiveness of our approach on long-horizon maze navigation and complex robot manipulation tasks.",,,,,"tactile image-to-image disentanglement of contact geometry from motion-induced shear reinforcement learning, imitation learning, skill-based transfer learning demonstration-guided reinforcement learning (rl) is a promising approach for learning complex behaviors by leveraging both reward feedback and a set of target task demonstrations. prior approaches for demonstration-guided rl treat every new task as an independent learning problem and attempt to follow the provided demonstrations step-by-step, akin to a human trying to imitate a completely unseen behavior by following the demonstrator's exact muscle movements. naturally, such learning will be slow, but often new behaviors are not completely unseen: they share subtasks with behaviors we have previously learned. in this work, we aim to exploit this shared subtask structure to increase the efficiency of demonstration-guided rl. we first learn a set of reusable skills from large offline datasets of prior experience collected across many tasks. we then propose skill-based learning with demonstrations (skild), an algorithm for demonstration-guided rl that efficiently leverages the provided demonstrations by following the demonstrated skills instead of the primitive actions, resulting in substantial performance improvements over prior demonstration-guided rl approaches. we validate the effectiveness of our approach on long-horizon maze navigation and complex robot manipulation tasks.",-3.4063478,3.1848412,demonstration-guided,reinforce,7,"id:7 (demonstration-guided)<br><b>Tactile Image-to-Image Disentanglement of Contact Geometry from Motion-Induced Shear</b><br>kw:Reinforcement Learning, Imitation Learning, Skill-Based Transfer Learning"
8,8,254,https://openreview.net/forum?id=a5ZiDzL0enJ,Learning Inertial Odometry for Dynamic Legged Robot State Estimation,"Russell Buchanan, Marco Camurri, Frank Dellaert, Maurice Fallon",1.0,,,,,,,False,,"Legged Robots, Inertial Navigation, Deep Neural Networks","This paper introduces a novel proprioceptive state estimator for legged robots based on a learned displacement measurement from IMU data. Recent research in pedestrian tracking has shown that motion can be inferred from inertial data using convolutional neural networks. A learned inertial displacement measurement can improve state estimation in challenging scenarios where leg odometry is unreliable, such as slipping and compressible terrains. Our work learns to estimate a displacement measurement from IMU data which is then fused with traditional leg odometry. Our approach greatly reduces the drift of proprioceptive state estimation, which is critical for legged robots deployed in vision and lidar denied environments such as foggy sewers or dusty mines. We compared results from an EKF and an incremental fixed-lag factor graph estimator using data from several real robot experiments crossing challenging terrains. Our results show a reduction of relative pose error by 37% in challenging scenarios when compared to a traditional kinematic-inertial estimator without learned measurement. We also demonstrate a 22% reduction in error when used with vision systems in visually degraded environments such as an underground mine.",,,,,"learning inertial odometry for dynamic legged robot state estimation legged robots, inertial navigation, deep neural networks this paper introduces a novel proprioceptive state estimator for legged robots based on a learned displacement measurement from imu data. recent research in pedestrian tracking has shown that motion can be inferred from inertial data using convolutional neural networks. a learned inertial displacement measurement can improve state estimation in challenging scenarios where leg odometry is unreliable, such as slipping and compressible terrains. our work learns to estimate a displacement measurement from imu data which is then fused with traditional leg odometry. our approach greatly reduces the drift of proprioceptive state estimation, which is critical for legged robots deployed in vision and lidar denied environments such as foggy sewers or dusty mines. we compared results from an ekf and an incremental fixed-lag factor graph estimator using data from several real robot experiments crossing challenging terrains. our results show a reduction of relative pose error by 37% in challenging scenarios when compared to a traditional kinematic-inertial estimator without learned measurement. we also demonstrate a 22% reduction in error when used with vision systems in visually degraded environments such as an underground mine.",-1.3318719,4.839923,inertial,no reinforce,8,"id:8 (inertial)<br><b>Learning Inertial Odometry for Dynamic Legged Robot State Estimation</b><br>kw:Legged Robots, Inertial Navigation, Deep Neural Networks"
9,9,268,https://openreview.net/forum?id=jV0n2wJocXr,CLASP: Constrained Latent Shape Projection for Refining Object Shape from Robot Contact,"Brad Saund, Dmitry Berenson",1.0,,,,,,,False,,"Shape Completion, Contact Sensing","Robots need both visual and contact sensing to effectively estimate the state of their environment. Camera RGBD data provides rich information of the objects surrounding the robot, and shape priors can help correct noise and fill in gaps and occluded regions. However, when the robot senses unexpected contact, the estimate should be updated to explain the contact. To address this need, we propose CLASP: Constrained Latent Shape Projection. This approach consists of a shape completion network that generates a prior from RGBD data and a procedure to generate shapes consistent with both the network prior and robot contact observations. We find CLASP consistently decreases the Chamfer Distance between the predicted and ground truth scenes, while other approaches do not benefit from contact information.",,,,,"clasp: constrained latent shape projection for refining object shape from robot contact shape completion, contact sensing robots need both visual and contact sensing to effectively estimate the state of their environment. camera rgbd data provides rich information of the objects surrounding the robot, and shape priors can help correct noise and fill in gaps and occluded regions. however, when the robot senses unexpected contact, the estimate should be updated to explain the contact. to address this need, we propose clasp: constrained latent shape projection. this approach consists of a shape completion network that generates a prior from rgbd data and a procedure to generate shapes consistent with both the network prior and robot contact observations. we find clasp consistently decreases the chamfer distance between the predicted and ground truth scenes, while other approaches do not benefit from contact information.",-2.0605524,3.7707472,shape,no reinforce,9,"id:9 (shape)<br><b>CLASP: Constrained Latent Shape Projection for Refining Object Shape from Robot Contact</b><br>kw:Shape Completion, Contact Sensing"
10,10,282,https://openreview.net/forum?id=yt3tDB67lc5,Deep Multi-Modal Contact Estimation for Invariant Observer Design on Quadruped Robots,"Tzu-Yuan Lin, Ray Zhang, Justin Yu, Maani Ghaffari",1.0,,,,,,,False,,"State Estimation, Deep Learning, Quadruped Robot","This work reports on developing a deep learning-based contact estimator for legged robots that bypasses the need for physical contact sensors and takes multi-modal proprioceptive sensory data from joint encoders, kinematics, and an inertial measurement unit as input. Unlike vision-based state estimators, proprioceptive state estimators are agnostic to perceptually degraded situations such as dark or foggy scenes. For legged robots, reliable kinematics and contact data are necessary to develop a proprioceptive state estimator. While some robots are equipped with dedicated contact sensors or springs to detect contact, some robots do not have dedicated contact sensors, and the addition of such sensors is non-trivial without redesigning the hardware.         The trained deep network can accurately estimate contacts on different terrains and robot gaits and is deployed along a contact-aided invariant extended Kalman filter to generate odometry trajectories. The filter performs comparably to a state-of-the-art visual SLAM system.",,,,,"deep multi-modal contact estimation for invariant observer design on quadruped robots state estimation, deep learning, quadruped robot this work reports on developing a deep learning-based contact estimator for legged robots that bypasses the need for physical contact sensors and takes multi-modal proprioceptive sensory data from joint encoders, kinematics, and an inertial measurement unit as input. unlike vision-based state estimators, proprioceptive state estimators are agnostic to perceptually degraded situations such as dark or foggy scenes. for legged robots, reliable kinematics and contact data are necessary to develop a proprioceptive state estimator. while some robots are equipped with dedicated contact sensors or springs to detect contact, some robots do not have dedicated contact sensors, and the addition of such sensors is non-trivial without redesigning the hardware.         the trained deep network can accurately estimate contacts on different terrains and robot gaits and is deployed along a contact-aided invariant extended kalman filter to generate odometry trajectories. the filter performs comparably to a state-of-the-art visual slam system.",-1.9977369,4.4121447,contact,no reinforce,10,"id:10 (contact)<br><b>Deep Multi-Modal Contact Estimation for Invariant Observer Design on Quadruped Robots</b><br>kw:State Estimation, Deep Learning, Quadruped Robot"
11,11,286,https://openreview.net/forum?id=8K5kisAnb_p,Safe Nonlinear Control Using Robust Neural Lyapunov-Barrier Functions,"Charles Dawson, Zengyi Qin, Sicun Gao, Chuchu Fan",1.0,,,,,,,False,,"Certified control, learning for control","Safety and stability are common requirements for robotic control systems; however, designing safe, stable controllers remains difficult for nonlinear and uncertain models. We develop a model-based learning approach to synthesize robust feedback controllers with safety and stability guarantees. We take inspiration from robust convex optimization and Lyapunov theory to define robust control Lyapunov barrier functions that generalize despite model uncertainty. We demonstrate our approach in simulation on problems including car trajectory tracking, nonlinear control with obstacle avoidance, satellite rendezvous with safety constraints, and flight control with a learned ground effect model. Simulation results show that our approach yields controllers that match or exceed the capabilities of robust MPC while reducing computational costs by an order of magnitude.",,,,,"safe nonlinear control using robust neural lyapunov-barrier functions certified control, learning for control safety and stability are common requirements for robotic control systems; however, designing safe, stable controllers remains difficult for nonlinear and uncertain models. we develop a model-based learning approach to synthesize robust feedback controllers with safety and stability guarantees. we take inspiration from robust convex optimization and lyapunov theory to define robust control lyapunov barrier functions that generalize despite model uncertainty. we demonstrate our approach in simulation on problems including car trajectory tracking, nonlinear control with obstacle avoidance, satellite rendezvous with safety constraints, and flight control with a learned ground effect model. simulation results show that our approach yields controllers that match or exceed the capabilities of robust mpc while reducing computational costs by an order of magnitude.",-2.4201994,3.619384,nonlinear,no reinforce,11,"id:11 (nonlinear)<br><b>Safe Nonlinear Control Using Robust Neural Lyapunov-Barrier Functions</b><br>kw:Certified control, learning for control"
12,12,299,https://openreview.net/forum?id=VfGk0ELQ4LC,Touch-based Curiosity for Sparse-Reward Tasks,"sai rajeswar mudumba, Cyril Ibrahim, Nitin Surya, Florian Golemo, David Vazquez, Aaron Courville, Pedro O. Pinheiro",1.0,,,,,,,False,,"Intrinsic Motivation, Touch, Curiosity, Manipulation","Robots in many real-world settings have access to force/torque sensors in their gripper and tactile sensing is often necessary for tasks that involve contact-rich motion. In this work, we leverage surprise from mismatches in haptics feedback to guide exploration in hard sparse-reward reinforcement learning tasks. Our approach, Haptics-based Curiosity (\method{}), learns what visible objects interactions are supposed to ``feel"" like. We encourage exploration by rewarding interactions where the expectation and the experience do not match. We test our approach on a range of haptics-intensive robot arm tasks (e.g. pushing objects, opening doors), which we also release as part of this work. Across multiple experiments in a simulated setting, we demonstrate that our method is able to learn these difficult tasks through sparse reward and curiosity alone. We compare our cross-modal approach to single-modality (haptics- or vision-only) approaches as well as other curiosity-based methods and find that our method performs better and is more sample-efficient.",,,,,"touch-based curiosity for sparse-reward tasks intrinsic motivation, touch, curiosity, manipulation robots in many real-world settings have access to force/torque sensors in their gripper and tactile sensing is often necessary for tasks that involve contact-rich motion. in this work, we leverage surprise from mismatches in haptics feedback to guide exploration in hard sparse-reward reinforcement learning tasks. our approach, haptics-based curiosity (\method{}), learns what visible objects interactions are supposed to ``feel"" like. we encourage exploration by rewarding interactions where the expectation and the experience do not match. we test our approach on a range of haptics-intensive robot arm tasks (e.g. pushing objects, opening doors), which we also release as part of this work. across multiple experiments in a simulated setting, we demonstrate that our method is able to learn these difficult tasks through sparse reward and curiosity alone. we compare our cross-modal approach to single-modality (haptics- or vision-only) approaches as well as other curiosity-based methods and find that our method performs better and is more sample-efficient.",-1.7815307,4.186985,curiosity,reinforce,12,"id:12 (curiosity)<br><b>Touch-based Curiosity for Sparse-Reward Tasks</b><br>kw:Intrinsic Motivation, Touch, Curiosity, Manipulation"
13,13,305,https://openreview.net/forum?id=WqUl7sNkDre,Risk-Averse Zero-Order Trajectory Optimization,"Marin Vlastelica, Sebastian Blaes, Cristina Pinneri, Georg Martius",1.0,,,,,,,False,,"CEM, data-driven MPC, uncertainty, model-based RL",We introduce a simple but effective method for managing risk in zero-order trajectory optimization that involves probabilistic safety constraints and balancing of optimism in the face of epistemic uncertainty and pessimism in the face of aleatoric uncertainty of an ensemble of stochastic neural networks. Various experiments indicate that the separation of uncertainties is essential to performing well with data-driven MPC approaches in uncertain and safety-critical control environments.,,,,,"risk-averse zero-order trajectory optimization cem, data-driven mpc, uncertainty, model-based rl we introduce a simple but effective method for managing risk in zero-order trajectory optimization that involves probabilistic safety constraints and balancing of optimism in the face of epistemic uncertainty and pessimism in the face of aleatoric uncertainty of an ensemble of stochastic neural networks. various experiments indicate that the separation of uncertainties is essential to performing well with data-driven mpc approaches in uncertain and safety-critical control environments.",-2.5350244,2.6566622,zero-order,no reinforce,13,"id:13 (zero-order)<br><b>Risk-Averse Zero-Order Trajectory Optimization</b><br>kw:CEM, data-driven MPC, uncertainty, model-based RL"
14,14,320,https://openreview.net/forum?id=knObbYqSowX,Group-based Motion Prediction for Navigation in Crowded Environments,"Allan Wang, Christoforos Mavrogiannis, Aaron Steinfeld",1.0,,,,,,,False,,"Social Navigation, Group-based Navigation, Applications of Robot Learning in Navigation","We focus on the problem of planning the motion of a robot in a dynamic multiagent environment such as a pedestrian scene. Enabling the robot to navigate safely and in a socially compliant fashion in such scenes requires a representation that accounts for the unfolding multiagent dynamics. Existing approaches to this problem tend to employ microscopic models of motion prediction that reason about the individual behavior of other agents. While such models may achieve high tracking accuracy in trajectory prediction benchmarks, they often lack an understanding of the group structures unfolding in crowded scenes. Inspired by the Gestalt theory from psychology, we build a Model Predictive Control framework (G-MPC) that leverages group-based prediction for robot motion planning. We conduct an extensive simulation study involving a series of challenging navigation tasks in scenes extracted from two real-world pedestrian datasets. We illustrate that G-MPC enables a robot to achieve statistically significantly higher safety and lower number of group intrusions than a series of baselines featuring individual pedestrian motion prediction models. Finally, we show that G-MPC can handle noisy lidar-scan estimates without significant performance losses.",,,,,"group-based motion prediction for navigation in crowded environments social navigation, group-based navigation, applications of robot learning in navigation we focus on the problem of planning the motion of a robot in a dynamic multiagent environment such as a pedestrian scene. enabling the robot to navigate safely and in a socially compliant fashion in such scenes requires a representation that accounts for the unfolding multiagent dynamics. existing approaches to this problem tend to employ microscopic models of motion prediction that reason about the individual behavior of other agents. while such models may achieve high tracking accuracy in trajectory prediction benchmarks, they often lack an understanding of the group structures unfolding in crowded scenes. inspired by the gestalt theory from psychology, we build a model predictive control framework (g-mpc) that leverages group-based prediction for robot motion planning. we conduct an extensive simulation study involving a series of challenging navigation tasks in scenes extracted from two real-world pedestrian datasets. we illustrate that g-mpc enables a robot to achieve statistically significantly higher safety and lower number of group intrusions than a series of baselines featuring individual pedestrian motion prediction models. finally, we show that g-mpc can handle noisy lidar-scan estimates without significant performance losses.",-2.500647,3.44253,group-based,no reinforce,14,"id:14 (group-based)<br><b>Group-based Motion Prediction for Navigation in Crowded Environments</b><br>kw:Social Navigation, Group-based Navigation, Applications of Robot Learning in Navigation"
15,15,323,https://openreview.net/forum?id=R4E8wTUtxdl,Learning to Jump from Pixels,"Gabriel B Margolis, Tao Chen, Kartik Paigwar, Xiang Fu, Donghyun Kim, Sang bae Kim, Pulkit Agrawal",1.0,,,,,,,False,,"Locomotion, Vision, Hierarchical Control","Today's robotic quadruped systems can robustly walk over a diverse range of natural but continuous terrains involving snow, rain, slip, rubble, etc. Locomotion on discontinuous terrains such as one with gaps or obstacles presents a complementary set of challenges. It becomes necessary to plan ahead using visual inputs and execute agile behaviors such as jumps to cross gaps. Such dynamic motion results in significant motion of on-board camera that introduces a new set of challenges for real-time visual processing. The need for agility and the operation from vision reinforce the need for robust control. We present a system that can, in real-time, process visual observations from an onboard RGBD camera to command a quadruped robot to jump over wide gaps. The proposed method brings together the flexibility of model-free learning and the robustness of model-based control. We evaluate performance both in simulation and in the real world.",,,,,"learning to jump from pixels locomotion, vision, hierarchical control today's robotic quadruped systems can robustly walk over a diverse range of natural but continuous terrains involving snow, rain, slip, rubble, etc. locomotion on discontinuous terrains such as one with gaps or obstacles presents a complementary set of challenges. it becomes necessary to plan ahead using visual inputs and execute agile behaviors such as jumps to cross gaps. such dynamic motion results in significant motion of on-board camera that introduces a new set of challenges for real-time visual processing. the need for agility and the operation from vision reinforce the need for robust control. we present a system that can, in real-time, process visual observations from an onboard rgbd camera to command a quadruped robot to jump over wide gaps. the proposed method brings together the flexibility of model-free learning and the robustness of model-based control. we evaluate performance both in simulation and in the real world.",-2.0379417,3.8588855,jump,reinforce,15,"id:15 (jump)<br><b>Learning to Jump from Pixels</b><br>kw:Locomotion, Vision, Hierarchical Control"
16,16,326,https://openreview.net/forum?id=Q9VHQw594zx,Learning A Risk-Aware Trajectory Planner From Demonstrations Using Logic Monitor,"Xiao Li, Jonathan DeCastro, Cristian Ioan Vasile, Sertac Karaman, Daniela Rus",1.0,,,,,,,False,,"Learning from demonstrations, temporal logic, trajectory planning, autonomous driving, real-time verification","Risk awareness is an important factor to consider when deploying policies on robots in the real-world. Defining the right set of risk metrics can be difficult. In this work, we use a differentiable logic monitor that keeps track of the environmental agents' behaviors and provides a risk metric that the controlled agent can incorporate during planning. We introduce LogicRiskNet, a learning structure that can be constructed from temporal logic formulas describing rules governing a safe agent's behaviors. The network's parameters can be learned from demonstration data. By using temporal logic, the network provides an interpretable architecture that can explain what risk metrics are important to the human.  We integrate LogicRiskNet in an inverse optimal control (IOC) framework and show that we can learn to generate trajectory plans that accurately mimic the expert's risk handling behaviors solely from demonstration data. We evaluate our method on a real-world driving dataset.",,,,,"learning a risk-aware trajectory planner from demonstrations using logic monitor learning from demonstrations, temporal logic, trajectory planning, autonomous driving, real-time verification risk awareness is an important factor to consider when deploying policies on robots in the real-world. defining the right set of risk metrics can be difficult. in this work, we use a differentiable logic monitor that keeps track of the environmental agents' behaviors and provides a risk metric that the controlled agent can incorporate during planning. we introduce logicrisknet, a learning structure that can be constructed from temporal logic formulas describing rules governing a safe agent's behaviors. the network's parameters can be learned from demonstration data. by using temporal logic, the network provides an interpretable architecture that can explain what risk metrics are important to the human.  we integrate logicrisknet in an inverse optimal control (ioc) framework and show that we can learn to generate trajectory plans that accurately mimic the expert's risk handling behaviors solely from demonstration data. we evaluate our method on a real-world driving dataset.",-1.9393157,3.581882,risk,no reinforce,16,"id:16 (risk)<br><b>Learning A Risk-Aware Trajectory Planner From Demonstrations Using Logic Monitor</b><br>kw:Learning from demonstrations, temporal logic, trajectory planning, autonomous driving, real-time verification"
17,17,348,https://openreview.net/forum?id=UGp6FDaxB0f,Model-free Safe Control for Zero-Violation Reinforcement Learning,"Weiye Zhao, Tairan He, Changliu Liu",1.0,,,,,,,False,,"While deep reinforcement learning (DRL) has impressive performance in a variety of continuous control tasks, one critical hurdle that limits the application of DRL to physical world is the lack of safety guarantees. It is challenging for DRL agents to persistently satisfy a hard state constraint (known as the safety specification) during training. On the other hand, safe control methods with safety guarantees have been extensively studied. However, to synthesize safe control, these methods require explicit analytical models of the dynamic system; but these models are usually not available in DRL. This paper presents a model-free safe control strategy to synthesize safeguards for DRL agents, which will ensure zero safety violation during training. In particular, we present an implicit safe set algorithm, which synthesizes the safety index (also called the barrier certificate) and the subsequent safe control law only by querying a black-box dynamic function (e.g., a digital twin simulator). The theoretical results indicate the implicit safe set algorithm guarantees forward invariance and finite-time convergence to the safe set. We validate the proposed method on the state-of-the-art safety benchmark Safety Gym. Results show that the proposed method achieves zero safety violation and gains","cumulative reward compared to state-of-the-art safe DRL methods. Moreover, it can easily scale to high-dimensional systems.",,,,,"model-free safe control for zero-violation reinforcement learning while deep reinforcement learning (drl) has impressive performance in a variety of continuous control tasks, one critical hurdle that limits the application of drl to physical world is the lack of safety guarantees. it is challenging for drl agents to persistently satisfy a hard state constraint (known as the safety specification) during training. on the other hand, safe control methods with safety guarantees have been extensively studied. however, to synthesize safe control, these methods require explicit analytical models of the dynamic system; but these models are usually not available in drl. this paper presents a model-free safe control strategy to synthesize safeguards for drl agents, which will ensure zero safety violation during training. in particular, we present an implicit safe set algorithm, which synthesizes the safety index (also called the barrier certificate) and the subsequent safe control law only by querying a black-box dynamic function (e.g., a digital twin simulator). the theoretical results indicate the implicit safe set algorithm guarantees forward invariance and finite-time convergence to the safe set. we validate the proposed method on the state-of-the-art safety benchmark safety gym. results show that the proposed method achieves zero safety violation and gains cumulative reward compared to state-of-the-art safe drl methods. moreover, it can easily scale to high-dimensional systems.",-3.2867942,3.5396168,safe,reinforce,17,"id:17 (safe)<br><b>Model-free Safe Control for Zero-Violation Reinforcement Learning</b><br>kw:While deep reinforcement learning (DRL) has impressive performance in a variety of continuous control tasks, one critical hurdle that limits the application of DRL to physical world is the lack of safety guarantees. It is challenging for DRL agents to persistently satisfy a hard state constraint (known as the safety specification) during training. On the other hand, safe control methods with safety guarantees have been extensively studied. However, to synthesize safe control, these methods require explicit analytical models of the dynamic system; but these models are usually not available in DRL. This paper presents a model-free safe control strategy to synthesize safeguards for DRL agents, which will ensure zero safety violation during training. In particular, we present an implicit safe set algorithm, which synthesizes the safety index (also called the barrier certificate) and the subsequent safe control law only by querying a black-box dynamic function (e.g., a digital twin simulator). The theoretical results indicate the implicit safe set algorithm guarantees forward invariance and finite-time convergence to the safe set. We validate the proposed method on the state-of-the-art safety benchmark Safety Gym. Results show that the proposed method achieves zero safety violation and gains"
18,18,358,https://openreview.net/forum?id=wK2fDDJ5VcF,Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning,"Nikita Rudin, David Hoeller, Philipp Reist, Marco Hutter",1.0,,,,,,,False,,"Reinforcement Learning, Legged Robots, Sim-to-Real","In this work, we present and study a training set-up that achieves fast policy generation for real-world robotic tasks by using massive parallelism on a single workstation GPU. We analyze and discuss the impact of different training algorithm components in the massively parallel regime on the final policy performance and training times. In addition, we present a novel game-inspired curriculum that is well suited for training with thousands of simulated robots in parallel. We evaluate the approach by training the quadrupedal robot ANYmal to walk on challenging terrain. The parallel approach allows training policies for flat terrain in under four minutes, and in twenty minutes for uneven terrain. This represents a speedup of multiple orders of magnitude compared to previous work. Finally, we transfer the policies to the real robot to validate the approach.",,,,,"learning to walk in minutes using massively parallel deep reinforcement learning reinforcement learning, legged robots, sim-to-real in this work, we present and study a training set-up that achieves fast policy generation for real-world robotic tasks by using massive parallelism on a single workstation gpu. we analyze and discuss the impact of different training algorithm components in the massively parallel regime on the final policy performance and training times. in addition, we present a novel game-inspired curriculum that is well suited for training with thousands of simulated robots in parallel. we evaluate the approach by training the quadrupedal robot anymal to walk on challenging terrain. the parallel approach allows training policies for flat terrain in under four minutes, and in twenty minutes for uneven terrain. this represents a speedup of multiple orders of magnitude compared to previous work. finally, we transfer the policies to the real robot to validate the approach.",-2.0901005,3.5160027,parallel,reinforce,18,"id:18 (parallel)<br><b>Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning</b><br>kw:Reinforcement Learning, Legged Robots, Sim-to-Real"
19,19,400,https://openreview.net/forum?id=PfC1Jr6gvuP,Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots,"Zipeng Fu, Ashish Kumar, Jitendra Malik, Deepak Pathak",1.0,,,,,,,False,,"Locomotion, Gaits, Biomechanics, Energetics, Reinforcement Learning","Legged locomotion is commonly studied and expressed as a discrete set of gait patterns, like walk, trot, gallop, which are usually treated as given and pre-programmed in legged robots for efficient locomotion at different speeds. However, fixing a set of pre-programmed gaits limits the generality of locomotion. Recent animal motor studies show that these conventional gaits are only prevalent in ideal flat terrain conditions while real-world locomotion is unstructured and more like bouts of intermittent steps. What principles could lead to both structured and unstructured patterns across mammals and how to synthesize them in robots? In this work, we take an analysis-by-synthesis approach and learn to move by minimizing mechanical energy. We demonstrate that learning to minimize energy consumption is sufficient for the emergence of natural locomotion gaits at different speeds in real quadruped robots. The emergent gaits are structured in ideal terrains and look similar to that of horses and sheep. The same approach leads to unstructured gaits in rough terrains which is consistent with the findings in animal motor control. We validate our hypothesis in both simulation and real hardware across natural terrains. Videos at https://sites.google.com/view/energy-loco/",,,,,"minimizing energy consumption leads to the emergence of gaits in legged robots locomotion, gaits, biomechanics, energetics, reinforcement learning legged locomotion is commonly studied and expressed as a discrete set of gait patterns, like walk, trot, gallop, which are usually treated as given and pre-programmed in legged robots for efficient locomotion at different speeds. however, fixing a set of pre-programmed gaits limits the generality of locomotion. recent animal motor studies show that these conventional gaits are only prevalent in ideal flat terrain conditions while real-world locomotion is unstructured and more like bouts of intermittent steps. what principles could lead to both structured and unstructured patterns across mammals and how to synthesize them in robots? in this work, we take an analysis-by-synthesis approach and learn to move by minimizing mechanical energy. we demonstrate that learning to minimize energy consumption is sufficient for the emergence of natural locomotion gaits at different speeds in real quadruped robots. the emergent gaits are structured in ideal terrains and look similar to that of horses and sheep. the same approach leads to unstructured gaits in rough terrains which is consistent with the findings in animal motor control. we validate our hypothesis in both simulation and real hardware across natural terrains. videos at https://sites.google.com/view/energy-loco/",-2.1528997,4.620533,gaits,reinforce,19,"id:19 (gaits)<br><b>Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots</b><br>kw:Locomotion, Gaits, Biomechanics, Energetics, Reinforcement Learning"
20,20,18,https://openreview.net/forum?id=ht3aHpc1hUt,Structure from Silence: Learning Scene Structure from Ambient Sound,"Ziyang Chen, Xixi Hu, Andrew Owens",2.0,,,,,,,False,,"audio perception, multi-modal learning, self-supervision, navigation","The sounds that a robot hears within a scene subtly vary as it moves through it. In this paper, we ask whether these ambient sounds convey information about scene structure. To study this, we collect a dataset of ``in-the-wild'' paired audio and RGB-D recordings in a variety of quiet, indoor scenes, typical of what a robot would encounter when performing navigation tasks. Using this data, we train models that successfully estimate depth in a number of evaluation settings. Finally, we show that these sounds provide a useful learning signal, and that we can obtain a useful representation by associating a visual signal with sound. These results suggest that ambient sound conveys a surprising amount of information about scene structure, and that this information can be successfully exploited by learning systems.",,,,,"structure from silence: learning scene structure from ambient sound audio perception, multi-modal learning, self-supervision, navigation the sounds that a robot hears within a scene subtly vary as it moves through it. in this paper, we ask whether these ambient sounds convey information about scene structure. to study this, we collect a dataset of ``in-the-wild'' paired audio and rgb-d recordings in a variety of quiet, indoor scenes, typical of what a robot would encounter when performing navigation tasks. using this data, we train models that successfully estimate depth in a number of evaluation settings. finally, we show that these sounds provide a useful learning signal, and that we can obtain a useful representation by associating a visual signal with sound. these results suggest that ambient sound conveys a surprising amount of information about scene structure, and that this information can be successfully exploited by learning systems.",-1.5205578,3.7102966,ambient,no reinforce,20,"id:20 (ambient)<br><b>Structure from Silence: Learning Scene Structure from Ambient Sound</b><br>kw:audio perception, multi-modal learning, self-supervision, navigation"
21,21,19,https://openreview.net/forum?id=d_SWJhyKfVw,Rapid Exploration for Open-World Navigation with Latent Goal Models,"Dhruv Shah, Benjamin Eysenbach, Nicholas Rhinehart, Sergey Levine",2.0,,,,,,,False,,"inverse reinforcement learning, imitation learning, self-supervised learning","We investigate the visual cross-embodiment imitation setting, in which agents learn policies from videos of other agents (such as humans) demonstrating the same task, but with stark differences in their embodiments -- shape, actions, end-effector dynamics, etc. In this work, we demonstrate that it is possible to automatically discover and learn vision-based reward functions from cross-embodiment demonstration videos that are robust to these differences. Specifically, we present a self-supervised method for Cross-embodiment Inverse Reinforcement Learning (XIRL) that leverages temporal cycle-consistency constraints to learn deep visual embeddings that capture task progression from offline videos of demonstrations across multiple expert agents, each performing the same task differently due to embodiment differences. Prior to our work, producing rewards from self-supervised embeddings typically required alignment with a reference trajectory, which may be difficult to acquire under stark embodiment differences. We show empirically that if the embeddings are aware of task-progress, simply taking the negative distance between the current state and goal state in the learned embedding space is useful as a reward for training policies with reinforcement learning. We find our learned reward function not only works for embodiments seen during training, but also generalizes to entirely new embodiments. Additionally, when transferring real-world human demonstrations to a simulated robot, we find that XIRL is more sample efficient than current best methods.",,,,,"rapid exploration for open-world navigation with latent goal models inverse reinforcement learning, imitation learning, self-supervised learning we investigate the visual cross-embodiment imitation setting, in which agents learn policies from videos of other agents (such as humans) demonstrating the same task, but with stark differences in their embodiments -- shape, actions, end-effector dynamics, etc. in this work, we demonstrate that it is possible to automatically discover and learn vision-based reward functions from cross-embodiment demonstration videos that are robust to these differences. specifically, we present a self-supervised method for cross-embodiment inverse reinforcement learning (xirl) that leverages temporal cycle-consistency constraints to learn deep visual embeddings that capture task progression from offline videos of demonstrations across multiple expert agents, each performing the same task differently due to embodiment differences. prior to our work, producing rewards from self-supervised embeddings typically required alignment with a reference trajectory, which may be difficult to acquire under stark embodiment differences. we show empirically that if the embeddings are aware of task-progress, simply taking the negative distance between the current state and goal state in the learned embedding space is useful as a reward for training policies with reinforcement learning. we find our learned reward function not only works for embodiments seen during training, but also generalizes to entirely new embodiments. additionally, when transferring real-world human demonstrations to a simulated robot, we find that xirl is more sample efficient than current best methods.",-2.3265395,3.6927671,differences.,reinforce,21,"id:21 (differences.)<br><b>Rapid Exploration for Open-World Navigation with Latent Goal Models</b><br>kw:inverse reinforcement learning, imitation learning, self-supervised learning"
22,22,20,https://openreview.net/forum?id=tmZsuYPmQ9m,Anomaly Detection in Multi-Agent Trajectories for Automated Driving,"Julian Wiederer, Arij Bouazizi, Marco Troina, Ulrich Kressel, Vasileios Belagiannis",2.0,,,,,,,False,,"Anomaly Detection, Multi-Agent Trajectory, Graph Neural Networks, Automated Driving","Human drivers can recognise fast abnormal driving situations to avoid accidents. Similar to humans, automated vehicles are supposed to perform anomaly detection. In this work, we propose the spatio-temporal graph auto-encoder for learning normal driving behaviours. Our innovation is the ability to jointly learn multiple trajectories of a dynamic number of agents. To perform anomaly detection, we first estimate a density function of the learned trajectory feature representation and then detect anomalies in low-density regions. Due to the lack of multi-agent trajectory datasets for anomaly detection in automated driving, we introduce our dataset using a driving simulator for normal and abnormal manoeuvres. Our evaluations show that our approach learns the relation between different agents and delivers promising results compared to the related works. The code, simulation and the dataset are publicly available.",,,,,"anomaly detection in multi-agent trajectories for automated driving anomaly detection, multi-agent trajectory, graph neural networks, automated driving human drivers can recognise fast abnormal driving situations to avoid accidents. similar to humans, automated vehicles are supposed to perform anomaly detection. in this work, we propose the spatio-temporal graph auto-encoder for learning normal driving behaviours. our innovation is the ability to jointly learn multiple trajectories of a dynamic number of agents. to perform anomaly detection, we first estimate a density function of the learned trajectory feature representation and then detect anomalies in low-density regions. due to the lack of multi-agent trajectory datasets for anomaly detection in automated driving, we introduce our dataset using a driving simulator for normal and abnormal manoeuvres. our evaluations show that our approach learns the relation between different agents and delivers promising results compared to the related works. the code, simulation and the dataset are publicly available.",-2.191108,3.889241,anomaly,no reinforce,22,"id:22 (anomaly)<br><b>Anomaly Detection in Multi-Agent Trajectories for Automated Driving</b><br>kw:Anomaly Detection, Multi-Agent Trajectory, Graph Neural Networks, Automated Driving"
23,23,28,https://openreview.net/forum?id=zv3NYgRZ7Qo,3D Neural Scene Representations for Visuomotor Control,"Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, Antonio Torralba",2.0,,,,,,,False,,"learning-based dynamics modeling, 3d-aware representation learning, neural radiance field, robotic manipulation","Humans have a strong intuitive understanding of the 3D environment around us. The mental model of the physics in our brain applies to objects of different materials and enables us to perform a wide range of manipulation tasks that are far beyond the reach of current robots. In this work, we desire to learn models for dynamic 3D scenes purely from 2D visual observations. Our model combines Neural Radiance Fields (NeRF) and time contrastive learning with an autoencoding framework, which learns viewpoint-invariant 3D-aware scene representations. We show that a dynamics model, constructed over the learned representation space, enables visuomotor control for challenging manipulation tasks involving both rigid bodies and fluids, where the target is specified in a viewpoint different from what the robot operates on. When coupled with an auto-decoding framework, it can even support goal specification from camera viewpoints that are outside the training distribution. We further demonstrate the richness of the learned 3D dynamics model by performing future prediction and novel view synthesis. Finally, we provide detailed ablation studies regarding different system designs and qualitative analysis of the learned representations.",,,,,"3d neural scene representations for visuomotor control learning-based dynamics modeling, 3d-aware representation learning, neural radiance field, robotic manipulation humans have a strong intuitive understanding of the 3d environment around us. the mental model of the physics in our brain applies to objects of different materials and enables us to perform a wide range of manipulation tasks that are far beyond the reach of current robots. in this work, we desire to learn models for dynamic 3d scenes purely from 2d visual observations. our model combines neural radiance fields (nerf) and time contrastive learning with an autoencoding framework, which learns viewpoint-invariant 3d-aware scene representations. we show that a dynamics model, constructed over the learned representation space, enables visuomotor control for challenging manipulation tasks involving both rigid bodies and fluids, where the target is specified in a viewpoint different from what the robot operates on. when coupled with an auto-decoding framework, it can even support goal specification from camera viewpoints that are outside the training distribution. we further demonstrate the richness of the learned 3d dynamics model by performing future prediction and novel view synthesis. finally, we provide detailed ablation studies regarding different system designs and qualitative analysis of the learned representations.",-1.5046816,4.076398,3d-aware,no reinforce,23,"id:23 (3d-aware)<br><b>3D Neural Scene Representations for Visuomotor Control</b><br>kw:learning-based dynamics modeling, 3d-aware representation learning, neural radiance field, robotic manipulation"
24,24,55,https://openreview.net/forum?id=X2KJq-S11BC,Self-Improving Semantic Perception for Indoor Localisation,"Hermann Blum, Francesco Milano, Ren? Zurbr?gg, Roland Siegwart, Cesar Cadena, Abel Gawel",2.0,,,,,,,False,,"continual learning, self-supervised learning, online learning","We propose a novel robotic system that can improve its perception during deployment. Contrary to the established approach of learning semantics from large datasets and deploying fixed models, we propose a framework in which semantic models are continuously updated on the robot to adapt to the deployment environments. By combining continual learning with self-supervision, our robotic system learns online during deployment without external supervision. We conduct real-world experiments with robots localising in 3D floorplans. Our experiments show how the robot's semantic perception improves during deployment and how this translates into improved localisation, even across drastically different environments. We further study the risk of catastrophic forgetting that such a continuous learning setting poses. We find memory replay an effective measure to reduce forgetting and show how the robotic system can improve even when switching between different environments. On average, our system improves by 60% in segmentation and 10% in localisation accuracy compared to deployment of a fixed model, and it maintains this improvement while adapting to further environments.",,,,,"self-improving semantic perception for indoor localisation continual learning, self-supervised learning, online learning we propose a novel robotic system that can improve its perception during deployment. contrary to the established approach of learning semantics from large datasets and deploying fixed models, we propose a framework in which semantic models are continuously updated on the robot to adapt to the deployment environments. by combining continual learning with self-supervision, our robotic system learns online during deployment without external supervision. we conduct real-world experiments with robots localising in 3d floorplans. our experiments show how the robot's semantic perception improves during deployment and how this translates into improved localisation, even across drastically different environments. we further study the risk of catastrophic forgetting that such a continuous learning setting poses. we find memory replay an effective measure to reduce forgetting and show how the robotic system can improve even when switching between different environments. on average, our system improves by 60% in segmentation and 10% in localisation accuracy compared to deployment of a fixed model, and it maintains this improvement while adapting to further environments.",-1.3468788,3.9734538,deployment,no reinforce,24,"id:24 (deployment)<br><b>Self-Improving Semantic Perception for Indoor Localisation</b><br>kw:continual learning, self-supervised learning, online learning"
25,25,69,https://openreview.net/forum?id=kgoWLlA33-U,LS3: Latent Space Safe Sets for Long-Horizon Visuomotor Control of Sparse Reward Iterative Tasks,"Albert Wilcox, Ashwin Balakrishna, Brijen Thananjeyan, Joseph E. Gonzalez, Ken Goldberg",2.0,,,,,,,False,,"Reinforcement Learning, Imitation Learning, Safety","Reinforcement learning (RL) has shown impressive success in exploring high-dimensional environments to learn complex tasks, but can often exhibit unsafe behaviors and require extensive environment interaction when exploration is unconstrained. A promising strategy for learning in dynamically uncertain environments is requiring that the agent can robustly return to learned Safe Sets, where task success (and therefore safety) can be guaranteed. While this approach has been successful in low-dimensions, enforcing this constraint in environments with visual observation spaces is exceedingly challenging. We present a novel continuous representation for Safe Sets framed as a binary classification problem in a learned latent space, which flexibly scales to high-dimensional image observations. We then present a new algorithm, Latent Space Safe Sets (LS3), which uses this representation for long-horizon control. We evaluate LS3 on 4 domains, including a challenging sequential pushing task in simulation and a physical cable routing task. We find that LS3 can use prior task successes to restrict exploration and learn more efficiently than prior algorithms while satisfying constraints. See https://tinyurl.com/latent-safe-sets for supplementary material.",,,,,"ls3: latent space safe sets for long-horizon visuomotor control of sparse reward iterative tasks reinforcement learning, imitation learning, safety reinforcement learning (rl) has shown impressive success in exploring high-dimensional environments to learn complex tasks, but can often exhibit unsafe behaviors and require extensive environment interaction when exploration is unconstrained. a promising strategy for learning in dynamically uncertain environments is requiring that the agent can robustly return to learned safe sets, where task success (and therefore safety) can be guaranteed. while this approach has been successful in low-dimensions, enforcing this constraint in environments with visual observation spaces is exceedingly challenging. we present a novel continuous representation for safe sets framed as a binary classification problem in a learned latent space, which flexibly scales to high-dimensional image observations. we then present a new algorithm, latent space safe sets (ls3), which uses this representation for long-horizon control. we evaluate ls3 on 4 domains, including a challenging sequential pushing task in simulation and a physical cable routing task. we find that ls3 can use prior task successes to restrict exploration and learn more efficiently than prior algorithms while satisfying constraints. see https://tinyurl.com/latent-safe-sets for supplementary material.",-2.6053665,3.5970662,sets,reinforce,25,"id:25 (sets)<br><b>LS3: Latent Space Safe Sets for Long-Horizon Visuomotor Control of Sparse Reward Iterative Tasks</b><br>kw:Reinforcement Learning, Imitation Learning, Safety"
26,26,77,https://openreview.net/forum?id=0CE82_hBPzA,Learning Eye-in-Hand Camera Calibration from a Single Image,"Eugene Valassakis, Kamil Dreczkowski, Edward Johns",2.0,,,,☆,,,False,,"Camera calibration, Robot manipulation, Sim-to-real","Eye-in-hand camera calibration is a fundamental and long-studied problem in robotics. We present a study on using learning-based methods for solving the problem online from a single RGB image, whilst training our models with entirely synthetic data. We study three main approaches, one using a direct regression model predicting the extrinsic matrix directly from images, one sparse correspondence model regressing 2D keypoints and using PnP, and one dense correspondence model that uses regressed depth and segmentation maps to enable ICP pose estimation. In our experiments, we benchmark these methods against each other and against well established classical methods to find the surprising result that direct regression outperforms other approaches, and perform noise-sensitivity analysis to gain further insights into those results.",,,,,"learning eye-in-hand camera calibration from a single image camera calibration, robot manipulation, sim-to-real eye-in-hand camera calibration is a fundamental and long-studied problem in robotics. we present a study on using learning-based methods for solving the problem online from a single rgb image, whilst training our models with entirely synthetic data. we study three main approaches, one using a direct regression model predicting the extrinsic matrix directly from images, one sparse correspondence model regressing 2d keypoints and using pnp, and one dense correspondence model that uses regressed depth and segmentation maps to enable icp pose estimation. in our experiments, we benchmark these methods against each other and against well established classical methods to find the surprising result that direct regression outperforms other approaches, and perform noise-sensitivity analysis to gain further insights into those results.",-2.1166003,3.8319602,eye-in-hand,no reinforce,26,"id:26 (eye-in-hand)<br><b>Learning Eye-in-Hand Camera Calibration from a Single Image</b><br>kw:Camera calibration, Robot manipulation, Sim-to-real"
27,27,78,https://openreview.net/forum?id=ibktAcINCaj,Urban Driver: Learning to Drive from Real-world Demonstrations Using Policy Gradients,"Oliver Scheel, B?a?ej Osi?ski, Luca Bergamini, Peter Ondruska",2.0,,,,,,,False,,"Self-driving, Learning from Demonstrations, Simulation",In this work we are the first to present an offline policy gradient method for learning imitative policies for complex urban driving from a large corpus of real-world demonstrations. This is achieved by building a differentiable data-driven simulator on top of perception outputs and high-fidelity HD maps of the area. It allows us to synthesize new driving experiences from existing demonstrations using mid-level representations. Using this simulator we then train a policy network in closed-loop employing policy gradients.        We train our proposed method on 100 hours of expert demonstrations on urban roads and show that it learns complex driving policies that generalize well and can perform a variety of driving maneuvers. We demonstrate this in simulation as well as deploy our model to self-driving vehicles in the real-world. Our method outperforms previously demonstrated state-of-the-art for urban driving scenarios - all this without the need for complex state perturbations or collecting additional on-policy data during training. We make code and data publicly available.,,,,,"urban driver: learning to drive from real-world demonstrations using policy gradients self-driving, learning from demonstrations, simulation in this work we are the first to present an offline policy gradient method for learning imitative policies for complex urban driving from a large corpus of real-world demonstrations. this is achieved by building a differentiable data-driven simulator on top of perception outputs and high-fidelity hd maps of the area. it allows us to synthesize new driving experiences from existing demonstrations using mid-level representations. using this simulator we then train a policy network in closed-loop employing policy gradients.        we train our proposed method on 100 hours of expert demonstrations on urban roads and show that it learns complex driving policies that generalize well and can perform a variety of driving maneuvers. we demonstrate this in simulation as well as deploy our model to self-driving vehicles in the real-world. our method outperforms previously demonstrated state-of-the-art for urban driving scenarios - all this without the need for complex state perturbations or collecting additional on-policy data during training. we make code and data publicly available.",-1.9249458,3.917405,urban,no reinforce,27,"id:27 (urban)<br><b>Urban Driver: Learning to Drive from Real-world Demonstrations Using Policy Gradients</b><br>kw:Self-driving, Learning from Demonstrations, Simulation"
28,28,84,https://openreview.net/forum?id=FzMHiDesj0I,TANDEM: Tracking and Dense Mapping in Real-time using Deep Multi-view Stereo,"Lukas Koestler, Nan Yang, Niclas Zeller, Daniel Cremers",2.0,,,,,,,False,,"SLAM, Dense Mapping, Multi-view Stereo, Deep Learning","In this paper, we present TANDEM a real-time monocular tracking and dense mapping framework. For pose estimation, TANDEM performs photometric bundle adjustment based on a sliding window of keyframes. To increase the robustness, we propose a novel tracking front-end that performs dense direct image alignment using depth maps rendered from a global model that is built incrementally from dense depth predictions. To predict the dense depth maps, we propose Cascade View-Aggregation MVSNet (CVA-MVSNet) that utilizes the entire active keyframe window by hierarchically constructing 3D cost volumes with adaptive view aggregation to balance the different stereo baselines between the keyframes. Finally, the predicted depth maps are fused into a consistent global map represented as a truncated signed distance function (TSDF) voxel grid. Our experimental results show that TANDEM outperforms other state-of-the-art traditional and learning-based monocular visual odometry (VO) methods in terms of camera tracking. Moreover, TANDEM shows state-of-the-art real-time 3D reconstruction performance. Code will be published upon acceptance.",,,,,"tandem: tracking and dense mapping in real-time using deep multi-view stereo slam, dense mapping, multi-view stereo, deep learning in this paper, we present tandem a real-time monocular tracking and dense mapping framework. for pose estimation, tandem performs photometric bundle adjustment based on a sliding window of keyframes. to increase the robustness, we propose a novel tracking front-end that performs dense direct image alignment using depth maps rendered from a global model that is built incrementally from dense depth predictions. to predict the dense depth maps, we propose cascade view-aggregation mvsnet (cva-mvsnet) that utilizes the entire active keyframe window by hierarchically constructing 3d cost volumes with adaptive view aggregation to balance the different stereo baselines between the keyframes. finally, the predicted depth maps are fused into a consistent global map represented as a truncated signed distance function (tsdf) voxel grid. our experimental results show that tandem outperforms other state-of-the-art traditional and learning-based monocular visual odometry (vo) methods in terms of camera tracking. moreover, tandem shows state-of-the-art real-time 3d reconstruction performance. code will be published upon acceptance.",-2.5764298,4.503433,dense,no reinforce,28,"id:28 (dense)<br><b>TANDEM: Tracking and Dense Mapping in Real-time using Deep Multi-view Stereo</b><br>kw:SLAM, Dense Mapping, Multi-view Stereo, Deep Learning"
29,29,92,https://openreview.net/forum?id=YFF73MRGUIH,LENS: Localization enhanced by NeRF synthesis,"Arthur Moreau, Nathan Piasco, Dzmitry Tsishkou, Bogdan Stanciulescu, Arnaud de La Fortelle",2.0,,,,,,,False,,"visual localization, camera pose regression, novel view synthesis","Neural Radiance Fields (NeRF) have recently demonstrated photorealistic results for the task of novel view synthesis. In this paper, we propose to apply novel view synthesis to the robot relocalization problem: we demonstrate improvement of camera pose regression thanks to an additional synthetic dataset rendered by the NeRF class of algorithm. To avoid spawning novel views in irrelevant places we selected virtual camera locations from NeRF internal representation of the 3D geometry of the scene. We further improved localization accuracy of pose regressors using synthesized realistic and geometry consistent images as data augmentation during training. At the time of publication, our approach improved state of the art with a 60% lower error on Cambridge Landmarks and 7-scenes datasets. Hence, the resulting accuracy becomes comparable to structure-based methods, without any architecture modification or domain adaptation constraints. Since our method allows almost infinite generation of training data, we investigated limitations of camera pose regression depending on size and distribution of data used for training on public benchmarks. We concluded that pose regression accuracy is mostly bounded by relatively small and biased datasets rather than capacity of the pose regression model to solve the localization task.",,,,,"lens: localization enhanced by nerf synthesis visual localization, camera pose regression, novel view synthesis neural radiance fields (nerf) have recently demonstrated photorealistic results for the task of novel view synthesis. in this paper, we propose to apply novel view synthesis to the robot relocalization problem: we demonstrate improvement of camera pose regression thanks to an additional synthetic dataset rendered by the nerf class of algorithm. to avoid spawning novel views in irrelevant places we selected virtual camera locations from nerf internal representation of the 3d geometry of the scene. we further improved localization accuracy of pose regressors using synthesized realistic and geometry consistent images as data augmentation during training. at the time of publication, our approach improved state of the art with a 60% lower error on cambridge landmarks and 7-scenes datasets. hence, the resulting accuracy becomes comparable to structure-based methods, without any architecture modification or domain adaptation constraints. since our method allows almost infinite generation of training data, we investigated limitations of camera pose regression depending on size and distribution of data used for training on public benchmarks. we concluded that pose regression accuracy is mostly bounded by relatively small and biased datasets rather than capacity of the pose regression model to solve the localization task.",-2.1465127,4.5287848,pose,no reinforce,29,"id:29 (pose)<br><b>LENS: Localization enhanced by NeRF synthesis</b><br>kw:visual localization, camera pose regression, novel view synthesis"
30,30,102,https://openreview.net/forum?id=2g_c8okBd_2,Hierarchically Integrated Models: Learning to Navigate from Heterogeneous Robots,"Katie Kang, Gregory Kahn, Sergey Levine",2.0,,,,,,,False,,"deep reinforcement learning, multi-robot learning, mobile navigation","Deep reinforcement learning algorithms require large and diverse datasets in order to learn successful policies for perception-based mobile navigation. However, gathering such datasets with a single robot can be prohibitively expensive. Collecting data with multiple different robotic platforms with possibly different dynamics is a more scalable approach to large-scale data collection. But how can deep reinforcement learning algorithms leverage such heterogeneous datasets? In this work, we propose a deep reinforcement learning algorithm with hierarchically integrated models (HInt). At training time, HInt learns separate perception and dynamics models, and at test time, HInt integrates the two models in a hierarchical manner and plans actions with the integrated model. This method of planning with hierarchically integrated models allows the algorithm to train on datasets gathered by a variety of different platforms, while respecting the physical capabilities of the deployment robot at test time. Our mobile navigation experiments show that HInt outperforms conventional hierarchical policies and single-source approaches.",,,,,"hierarchically integrated models: learning to navigate from heterogeneous robots deep reinforcement learning, multi-robot learning, mobile navigation deep reinforcement learning algorithms require large and diverse datasets in order to learn successful policies for perception-based mobile navigation. however, gathering such datasets with a single robot can be prohibitively expensive. collecting data with multiple different robotic platforms with possibly different dynamics is a more scalable approach to large-scale data collection. but how can deep reinforcement learning algorithms leverage such heterogeneous datasets? in this work, we propose a deep reinforcement learning algorithm with hierarchically integrated models (hint). at training time, hint learns separate perception and dynamics models, and at test time, hint integrates the two models in a hierarchical manner and plans actions with the integrated model. this method of planning with hierarchically integrated models allows the algorithm to train on datasets gathered by a variety of different platforms, while respecting the physical capabilities of the deployment robot at test time. our mobile navigation experiments show that hint outperforms conventional hierarchical policies and single-source approaches.",-1.8704225,3.7302146,hint,reinforce,30,"id:30 (hint)<br><b>Hierarchically Integrated Models: Learning to Navigate from Heterogeneous Robots</b><br>kw:deep reinforcement learning, multi-robot learning, mobile navigation"
31,31,134,https://openreview.net/forum?id=ofioIEZvJRG,Decentralized Control of Quadrotor Swarms with End-to-end Deep Reinforcement Learning,"Sumeet Batra, Zhehui Huang, Aleksei Petrenko, Tushar Kumar, Artem Molchanov, Gaurav S. Sukhatme",2.0,,,,,,,False,,"Deep Reinforcement Learning, Robot Learning, Multi-Agent Systems, Quadrotors","We demonstrate the possibility of learning drone swarm controllers that are zero-shot transferable to real quadrotors via large-scale multi-agent end-to-end reinforcement learning. We train policies parameterized by neural networks that are capable of controlling individual drones in a swarm in a fully decentralized manner. Our policies, trained in simulated environments with realistic quadrotor physics, demonstrate advanced flocking behaviors, perform aggressive maneuvers in tight formations while avoiding collisions with each other, break and re-establish formations to avoid collisions with moving obstacles, and efficiently coordinate in pursuit-evasion tasks. We analyze, in simulation, how different model architectures and parameters of the training regime influence the final performance of neural swarms. We demonstrate the successful deployment of the model learned in simulation to highly resource-constrained physical quadrotors performing station keeping and goal swapping behaviors.         Video demonstrations are available at the project website https://sites.google.com/view/swarm-rl. Source code is available in the supplementary materials.",,,,,"decentralized control of quadrotor swarms with end-to-end deep reinforcement learning deep reinforcement learning, robot learning, multi-agent systems, quadrotors we demonstrate the possibility of learning drone swarm controllers that are zero-shot transferable to real quadrotors via large-scale multi-agent end-to-end reinforcement learning. we train policies parameterized by neural networks that are capable of controlling individual drones in a swarm in a fully decentralized manner. our policies, trained in simulated environments with realistic quadrotor physics, demonstrate advanced flocking behaviors, perform aggressive maneuvers in tight formations while avoiding collisions with each other, break and re-establish formations to avoid collisions with moving obstacles, and efficiently coordinate in pursuit-evasion tasks. we analyze, in simulation, how different model architectures and parameters of the training regime influence the final performance of neural swarms. we demonstrate the successful deployment of the model learned in simulation to highly resource-constrained physical quadrotors performing station keeping and goal swapping behaviors.         video demonstrations are available at the project website https://sites.google.com/view/swarm-rl. source code is available in the supplementary materials.",-2.133646,4.389368,quadrotors,reinforce,31,"id:31 (quadrotors)<br><b>Decentralized Control of Quadrotor Swarms with End-to-end Deep Reinforcement Learning</b><br>kw:Deep Reinforcement Learning, Robot Learning, Multi-Agent Systems, Quadrotors"
32,32,148,https://openreview.net/forum?id=1yj7yMTtna,Towards Real Robot Learning in the Wild: A Case Study in Bipedal Locomotion,"Michael Bloesch, Jan Humplik, Viorica Patraucean, Roland Hafner, Tuomas Haarnoja, Arunkumar Byravan, Noah Yamamoto Siegel, Saran Tunyasuvunakool, Federico Casarini, Nathan Batchelor, Francesco Romano, Stefano Saliceti, Martin Riedmiller, S. M. Ali Eslami, Nicolas Heess",2.0,,,,,,,False,,"Multi-Task Reinforcement Learning, Robot Learning","General-purpose robotic systems must master a large repertoire of diverse skills. While reinforcement learning provides a powerful framework for acquiring individual behaviors, the time needed to acquire each skill makes the prospect of a generalist robot trained with RL daunting. In this paper, we study how a large-scale collective robotic learning system can acquire a repertoire of behaviors simultaneously, sharing exploration, experience, and representations across tasks. In this framework, new tasks can be continuously instantiated from previously learned tasks improving overall performance and capabilities of the system. To instantiate this system, we develop a scalable and intuitive framework for specifying new tasks through user-provided examples of desired outcomes, devise a multi-robot collective learning system for data collection that simultaneously collects experience for multiple tasks, and develop a scalable and generalizable multi-task deep reinforcement learning method, which we call MT-Opt. We demonstrate how MT-Opt can learn a wide range of skills, including semantic picking (i.e., picking an object from a particular category), placing into various fixtures (e.g., placing a food item onto a plate), covering, aligning, and rearranging.  We train and evaluate our system on a set of 12 real-world tasks with data collected from 7 robots, and demonstrate the performance of our system both in terms of its ability to generalize to structurally similar new tasks, and acquire distinct new tasks more quickly by leveraging past experience.  We recommend viewing the videos at https://karolhausman.github.io/mt-opt/.",,,,,"towards real robot learning in the wild: a case study in bipedal locomotion multi-task reinforcement learning, robot learning general-purpose robotic systems must master a large repertoire of diverse skills. while reinforcement learning provides a powerful framework for acquiring individual behaviors, the time needed to acquire each skill makes the prospect of a generalist robot trained with rl daunting. in this paper, we study how a large-scale collective robotic learning system can acquire a repertoire of behaviors simultaneously, sharing exploration, experience, and representations across tasks. in this framework, new tasks can be continuously instantiated from previously learned tasks improving overall performance and capabilities of the system. to instantiate this system, we develop a scalable and intuitive framework for specifying new tasks through user-provided examples of desired outcomes, devise a multi-robot collective learning system for data collection that simultaneously collects experience for multiple tasks, and develop a scalable and generalizable multi-task deep reinforcement learning method, which we call mt-opt. we demonstrate how mt-opt can learn a wide range of skills, including semantic picking (i.e., picking an object from a particular category), placing into various fixtures (e.g., placing a food item onto a plate), covering, aligning, and rearranging.  we train and evaluate our system on a set of 12 real-world tasks with data collected from 7 robots, and demonstrate the performance of our system both in terms of its ability to generalize to structurally similar new tasks, and acquire distinct new tasks more quickly by leveraging past experience.  we recommend viewing the videos at https://karolhausman.github.io/mt-opt/.",-2.089122,4.384065,picking,reinforce,32,"id:32 (picking)<br><b>Towards Real Robot Learning in the Wild: A Case Study in Bipedal Locomotion</b><br>kw:Multi-Task Reinforcement Learning, Robot Learning"
33,33,182,https://openreview.net/forum?id=I6DLxqk9J0A,Anytime Depth Estimation with Limited Sensing and Computation Capabilities on Mobile Devices,"Yuedong Yang, Zihui Xue, Radu Marculescu",2.0,,,,,,,False,,"Depth estimation is a safety critical and energy sensitive method for environment sensing. However, in real applications, the depth estimation may be halted at any time, due to the frequent interruptions and low remaining energy capacity of battery when using powerful sensors like 3D LiDAR. To address this problem, we propose a depth estimation method that is robust to random halts and relies on energy-saving 2D LiDAR and a monocular camera. To this end, we formulate the depth estimation as an anytime problem and propose a new metric to evaluate its robustness under dynamic interruptions. Our final model has only 2M parameters with a marginal accuracy loss compared to state-of-the-art baselines. Indeed, our experiments on NYU Depth v2 dataset show that our model is capable of processing 224",224 resolution images and 2D point clouds with any computation budget larger than 6.37ms (157 FPS) and 0.2J on an NVIDIA Jetson TX2 system. Evaluations on KITTI dataset under supervised and self-supervised training show similar results.,,,,,"anytime depth estimation with limited sensing and computation capabilities on mobile devices depth estimation is a safety critical and energy sensitive method for environment sensing. however, in real applications, the depth estimation may be halted at any time, due to the frequent interruptions and low remaining energy capacity of battery when using powerful sensors like 3d lidar. to address this problem, we propose a depth estimation method that is robust to random halts and relies on energy-saving 2d lidar and a monocular camera. to this end, we formulate the depth estimation as an anytime problem and propose a new metric to evaluate its robustness under dynamic interruptions. our final model has only 2m parameters with a marginal accuracy loss compared to state-of-the-art baselines. indeed, our experiments on nyu depth v2 dataset show that our model is capable of processing 224 224 resolution images and 2d point clouds with any computation budget larger than 6.37ms (157 fps) and 0.2j on an nvidia jetson tx2 system. evaluations on kitti dataset under supervised and self-supervised training show similar results.",-1.267888,4.764092,depth,no reinforce,33,"id:33 (depth)<br><b>Anytime Depth Estimation with Limited Sensing and Computation Capabilities on Mobile Devices</b><br>kw:Depth estimation is a safety critical and energy sensitive method for environment sensing. However, in real applications, the depth estimation may be halted at any time, due to the frequent interruptions and low remaining energy capacity of battery when using powerful sensors like 3D LiDAR. To address this problem, we propose a depth estimation method that is robust to random halts and relies on energy-saving 2D LiDAR and a monocular camera. To this end, we formulate the depth estimation as an anytime problem and propose a new metric to evaluate its robustness under dynamic interruptions. Our final model has only 2M parameters with a marginal accuracy loss compared to state-of-the-art baselines. Indeed, our experiments on NYU Depth v2 dataset show that our model is capable of processing 224"
34,34,278,https://openreview.net/forum?id=EdmeHU4WVjJ,Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR,"Ziyue Feng, Longlong Jing, Peng Yin, Yingli Tian, Bing Li",2.0,,,,,,,False,,"Self-supervised, Monocular, Depth Prediction, Sparse LiDAR","Self-supervised monocular depth prediction provides a cost-effective solution to obtain the 3D location of each pixel. However, the existing approaches usually lead to unsatisfactory accuracy, which is critical for autonomous robots. In this paper, we propose a novel two-stage network to advance the self-supervised monocular dense depth learning by leveraging low-cost sparse (e.g. 4-beam) LiDAR. Unlike the existing methods that use sparse LiDAR mainly in a manner of time-consuming iterative post-processing, our model fuses monocular image features and sparse LiDAR features to predict initial depth maps. Then, an efficient feed-forward refine network is further designed to correct the errors in these initial depth maps in pseudo-3D space with real-time performance. Extensive experiments show that our proposed model significantly outperforms all the state-of-the-art self-supervised methods, as well as the sparse-LiDAR-based methods on both self-supervised monocular depth prediction and completion tasks. With the accurate dense depth prediction, our model outperforms the state-of-the-art sparse-LiDAR-based method (Pseudo-LiDAR++) by more than 68% for the downstream task monocular 3D object detection on the KITTI Leaderboard.",,,,,"advancing self-supervised monocular depth learning with sparse lidar self-supervised, monocular, depth prediction, sparse lidar self-supervised monocular depth prediction provides a cost-effective solution to obtain the 3d location of each pixel. however, the existing approaches usually lead to unsatisfactory accuracy, which is critical for autonomous robots. in this paper, we propose a novel two-stage network to advance the self-supervised monocular dense depth learning by leveraging low-cost sparse (e.g. 4-beam) lidar. unlike the existing methods that use sparse lidar mainly in a manner of time-consuming iterative post-processing, our model fuses monocular image features and sparse lidar features to predict initial depth maps. then, an efficient feed-forward refine network is further designed to correct the errors in these initial depth maps in pseudo-3d space with real-time performance. extensive experiments show that our proposed model significantly outperforms all the state-of-the-art self-supervised methods, as well as the sparse-lidar-based methods on both self-supervised monocular depth prediction and completion tasks. with the accurate dense depth prediction, our model outperforms the state-of-the-art sparse-lidar-based method (pseudo-lidar++) by more than 68% for the downstream task monocular 3d object detection on the kitti leaderboard.",-2.5020905,3.6694555,monocular,no reinforce,34,"id:34 (monocular)<br><b>Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR</b><br>kw:Self-supervised, Monocular, Depth Prediction, Sparse LiDAR"
35,35,283,https://openreview.net/forum?id=av44VzmMKwW,Learning Model Preconditions for Planning with Multiple Models,"Alex Licari LaGrassa, Oliver Kroemer",2.0,,,,,,,False,,"manipulation, planning","Different types of models can provide differing levels of fidelity when planning for a robot's task execution. Analytical models are often fast to evaluate but only work in a limited range of conditions. Meanwhile physics simulators are effective at modeling interactions between objects but are typically more computationally expensive. Learning when to switch between the various models can greatly improve the speed of planning without sacrificing task success. In this work, we learn model deviation estimators (MDEs) to predict the error between real-world states and the states outputted from skill effect models. Afterwards, we use the prediction from MDEs to switch between various models in order to speed up planning when possible. We evaluate our method on two real-world tasks: placing a rod into a box, and placing a rod into a closed drawer.",,,,,"learning model preconditions for planning with multiple models manipulation, planning different types of models can provide differing levels of fidelity when planning for a robot's task execution. analytical models are often fast to evaluate but only work in a limited range of conditions. meanwhile physics simulators are effective at modeling interactions between objects but are typically more computationally expensive. learning when to switch between the various models can greatly improve the speed of planning without sacrificing task success. in this work, we learn model deviation estimators (mdes) to predict the error between real-world states and the states outputted from skill effect models. afterwards, we use the prediction from mdes to switch between various models in order to speed up planning when possible. we evaluate our method on two real-world tasks: placing a rod into a box, and placing a rod into a closed drawer.",-1.7154636,3.8158827,rod,no reinforce,35,"id:35 (rod)<br><b>Learning Model Preconditions for Planning with Multiple Models</b><br>kw:manipulation, planning"
36,36,300,https://openreview.net/forum?id=Wt3GLZYFvEQ,Rough Terrain Navigation Using Divergence Constrained Model Based Reinforcement Learning,"Sean J Wang, Samuel Triest, Wenshan Wang, Sebastian Scherer, Aaron Johnson",2.0,,,,,,,False,,"Rough Terrain Navigation, Model-Based Reinforcement Learning, Model Uncertainty","Autonomous navigation of wheeled robots in rough terrain environments has been a long standing challenge. In these environments, predicting the robot's trajectory can be challenging due to the complexity of terrain interactions, as well as the divergent dynamics that cause model uncertainty to compound and propagate poorly. This inhibits the robot's long horizon decision making capabilities and often lead to shortsighted navigation strategies. We propose a model-based reinforcement learning algorithm for rough terrain traversal that trains a probabilistic dynamics model to consider the propagating effects of uncertainty. During trajectory predictions, a trajectory tracking controller is considered to predict closed-loop trajectories. Our method further increases prediction accuracy and precision by using constrained optimization to find trajectories with low divergence. Using this method, wheeled robots can find non-myopic control strategies to reach destinations with higher probability of success. We show results on simulated and real world robots navigating through rough terrain environments.",,,,,"rough terrain navigation using divergence constrained model based reinforcement learning rough terrain navigation, model-based reinforcement learning, model uncertainty autonomous navigation of wheeled robots in rough terrain environments has been a long standing challenge. in these environments, predicting the robot's trajectory can be challenging due to the complexity of terrain interactions, as well as the divergent dynamics that cause model uncertainty to compound and propagate poorly. this inhibits the robot's long horizon decision making capabilities and often lead to shortsighted navigation strategies. we propose a model-based reinforcement learning algorithm for rough terrain traversal that trains a probabilistic dynamics model to consider the propagating effects of uncertainty. during trajectory predictions, a trajectory tracking controller is considered to predict closed-loop trajectories. our method further increases prediction accuracy and precision by using constrained optimization to find trajectories with low divergence. using this method, wheeled robots can find non-myopic control strategies to reach destinations with higher probability of success. we show results on simulated and real world robots navigating through rough terrain environments.",-2.8160045,3.460461,rough,reinforce,36,"id:36 (rough)<br><b>Rough Terrain Navigation Using Divergence Constrained Model Based Reinforcement Learning</b><br>kw:Rough Terrain Navigation, Model-Based Reinforcement Learning, Model Uncertainty"
37,37,303,https://openreview.net/forum?id=NDYbXf-DvwZ,Visual-Locomotion: Learning to Walk on Complex Terrains with Vision,"Wenhao Yu, Deepali Jain, Alejandro Escontrela, Atil Iscen, Peng Xu, Erwin Coumans, Sehoon Ha, Jie Tan, Tingnan Zhang",2.0,,,,,,,False,,"Legged Robot, Reinforcement Learning, Visual Locomotion","Vision is one of the most important perception modalities for legged robots to safely and efficiently navigate uneven terrains, such as stairs and stepping stones. However,  training robots to effectively understand high-dimensional visual input for locomotion is a challenging problem. In this work, we propose a framework to train a vision-based locomotion controller which enables a quadrupedal robot to traverse uneven environments. The key idea is to introduce a hierarchical structure with a high-level vision policy and a low-level motion controller. The high-level vision policy takes as inputs the perceived vision signals as well as robot states and outputs the desired footholds and base movement of the robot. These are then realized by the low level motion controller composed of a position controller for swing legs and a MPC-based torque controller for stance legs. We train the vision policy using Deep Reinforcement Learning and demonstrate our approach on a variety of uneven environments such as randomly placed stepping stones, quincuncial piles, stairs, and moving platforms. We also validate our method on a real robot to walk over a series of gaps and climbing up a platform.",,,,,"visual-locomotion: learning to walk on complex terrains with vision legged robot, reinforcement learning, visual locomotion vision is one of the most important perception modalities for legged robots to safely and efficiently navigate uneven terrains, such as stairs and stepping stones. however,  training robots to effectively understand high-dimensional visual input for locomotion is a challenging problem. in this work, we propose a framework to train a vision-based locomotion controller which enables a quadrupedal robot to traverse uneven environments. the key idea is to introduce a hierarchical structure with a high-level vision policy and a low-level motion controller. the high-level vision policy takes as inputs the perceived vision signals as well as robot states and outputs the desired footholds and base movement of the robot. these are then realized by the low level motion controller composed of a position controller for swing legs and a mpc-based torque controller for stance legs. we train the vision policy using deep reinforcement learning and demonstrate our approach on a variety of uneven environments such as randomly placed stepping stones, quincuncial piles, stairs, and moving platforms. we also validate our method on a real robot to walk over a series of gaps and climbing up a platform.",-2.7286277,3.6357923,vision,reinforce,37,"id:37 (vision)<br><b>Visual-Locomotion: Learning to Walk on Complex Terrains with Vision</b><br>kw:Legged Robot, Reinforcement Learning, Visual Locomotion"
38,38,406,https://openreview.net/forum?id=OQMXb0xiCrt,SeqMatchNet: Contrastive Learning with Sequence Matching for Place Recognition & Relocalization,"Sourav Garg, Madhu Vankadari, Michael Milford",2.0,,,,,,,False,,"Relocalization, Localization, Visual Place Recognition, Sequence Matching, Contrastive Learning","Visual Place Recognition (VPR) for mobile robot global relocalization is a well-studied problem, where contrastive learning based representation training methods have led to state-of-the-art performance. However, these methods are mainly designed for single image based VPR, where sequential information, which is ubiquitous in robotics, is only used as a post-processing step for filtering single image match scores, but is never used to guide the representation learning process itself. In this work, for the first time, we bridge the gap between single image representation learning and sequence matching through SeqMatchNet which transforms the single image descriptors such that they become more responsive to the sequence matching metric. First, we propose a novel triplet loss formulation where the distance metric is based on sequence matching, that is, the aggregation of temporal order-based single-image Euclidean distances. We use the same metric for mining negatives online during the training which helps the optimization process by selecting appropriate positives and harder negatives. To overcome the computational overhead of sequence matching for negative mining, we propose a 2D convolution based formulation of sequence matching for efficiently aggregating distances within a distance matrix computed using single images. We show that our proposed method achieves consistent gains in performance as demonstrated on four benchmark datasets. Source code will be made publicly available.",,,,,"seqmatchnet: contrastive learning with sequence matching for place recognition & relocalization relocalization, localization, visual place recognition, sequence matching, contrastive learning visual place recognition (vpr) for mobile robot global relocalization is a well-studied problem, where contrastive learning based representation training methods have led to state-of-the-art performance. however, these methods are mainly designed for single image based vpr, where sequential information, which is ubiquitous in robotics, is only used as a post-processing step for filtering single image match scores, but is never used to guide the representation learning process itself. in this work, for the first time, we bridge the gap between single image representation learning and sequence matching through seqmatchnet which transforms the single image descriptors such that they become more responsive to the sequence matching metric. first, we propose a novel triplet loss formulation where the distance metric is based on sequence matching, that is, the aggregation of temporal order-based single-image euclidean distances. we use the same metric for mining negatives online during the training which helps the optimization process by selecting appropriate positives and harder negatives. to overcome the computational overhead of sequence matching for negative mining, we propose a 2d convolution based formulation of sequence matching for efficiently aggregating distances within a distance matrix computed using single images. we show that our proposed method achieves consistent gains in performance as demonstrated on four benchmark datasets. source code will be made publicly available.",-3.6805537,3.6190526,matching,no reinforce,38,"id:38 (matching)<br><b>SeqMatchNet: Contrastive Learning with Sequence Matching for Place Recognition & Relocalization</b><br>kw:Relocalization, Localization, Visual Place Recognition, Sequence Matching, Contrastive Learning"
39,39,21,https://openreview.net/forum?id=L0tXWRrB9yw,SCAPE: Learning Stiffness Control from Augmented Position Control Experiences,"Mincheol Kim, Scott Niekum, Ashish Deshpande",3.0,,,,☆,,,False,,"Manipulation, stiffness control, reinforcement learning","We introduce a sample-efficient method for learning state-dependent stiffness control policies for dexterous manipulation. The ability to control stiffness facilitates safe and reliable manipulation by providing compliance and robustness to uncertainties. Most current reinforcement learning approaches to achieve robotic manipulation have exclusively focused on position control, often due to the difficulty of learning high-dimensional stiffness control policies. This difficulty can be partially mitigated via policy guidance such as imitation learning. However, expert stiffness control demonstrations are often expensive or infeasible to record. Therefore, we present an approach to learn Stiffness Control from Augmented Position control Experiences (SCAPE) that bypasses this difficulty by transforming position control demonstrations into approximate, suboptimal stiffness control demonstrations. Then, the suboptimality of the augmented demonstrations is addressed by using complementary techniques that help the agent safely learn from both the demonstrations and reinforcement learning. By using simulation tools and experiments on a robotic testbed, we show that the proposed approach efficiently learns safe manipulation policies and outperforms learned position control policies and several other baseline learning algorithms.",,,,,"scape: learning stiffness control from augmented position control experiences manipulation, stiffness control, reinforcement learning we introduce a sample-efficient method for learning state-dependent stiffness control policies for dexterous manipulation. the ability to control stiffness facilitates safe and reliable manipulation by providing compliance and robustness to uncertainties. most current reinforcement learning approaches to achieve robotic manipulation have exclusively focused on position control, often due to the difficulty of learning high-dimensional stiffness control policies. this difficulty can be partially mitigated via policy guidance such as imitation learning. however, expert stiffness control demonstrations are often expensive or infeasible to record. therefore, we present an approach to learn stiffness control from augmented position control experiences (scape) that bypasses this difficulty by transforming position control demonstrations into approximate, suboptimal stiffness control demonstrations. then, the suboptimality of the augmented demonstrations is addressed by using complementary techniques that help the agent safely learn from both the demonstrations and reinforcement learning. by using simulation tools and experiments on a robotic testbed, we show that the proposed approach efficiently learns safe manipulation policies and outperforms learned position control policies and several other baseline learning algorithms.",-2.843297,3.2503412,stiffness,reinforce,39,"id:39 (stiffness)<br><b>SCAPE: Learning Stiffness Control from Augmented Position Control Experiences</b><br>kw:Manipulation, stiffness control, reinforcement learning"
40,40,32,https://openreview.net/forum?id=Qdb1ODTQTnL,Learning to Regrasp by Learning to Place,"Shuo Cheng, Kaichun Mo, Lin Shao",3.0,,,,☆,,,False,,"Regrasping, Deep Learning, Robotic Manipulation","In this paper, we explore whether a robot can learn to regrasp a diverse set of objects to achieve various desired grasp poses. Regrasping is needed whenever a robot's current grasp pose fails to perform desired manipulation tasks. Endowing robots with such an ability has applications in many domains such as manufacturing or domestic services. Yet, it is a challenging task due to the large diversity of geometry in everyday objects and the high dimensionality of the state and action space. In this paper, we propose a system for robots to take partial point clouds of an object and the supporting environment as inputs and output a sequence of pick-and-place operations to transform an initial object grasp pose to the desired object grasp poses. The key technique includes a neural stable placement predictor and a regrasp graph based solution through leveraging and changing surrounding environment. We introduce a new and challenging synthetic dataset for learning and evaluating the proposed approach.  In this dataset, we show that our system is able to achieve 73.3 % success rate of regrasping divese objects.",,,,,"learning to regrasp by learning to place regrasping, deep learning, robotic manipulation in this paper, we explore whether a robot can learn to regrasp a diverse set of objects to achieve various desired grasp poses. regrasping is needed whenever a robot's current grasp pose fails to perform desired manipulation tasks. endowing robots with such an ability has applications in many domains such as manufacturing or domestic services. yet, it is a challenging task due to the large diversity of geometry in everyday objects and the high dimensionality of the state and action space. in this paper, we propose a system for robots to take partial point clouds of an object and the supporting environment as inputs and output a sequence of pick-and-place operations to transform an initial object grasp pose to the desired object grasp poses. the key technique includes a neural stable placement predictor and a regrasp graph based solution through leveraging and changing surrounding environment. we introduce a new and challenging synthetic dataset for learning and evaluating the proposed approach.  in this dataset, we show that our system is able to achieve 73.3 % success rate of regrasping divese objects.",-0.97665346,4.366064,regrasp,no reinforce,40,"id:40 (regrasp)<br><b>Learning to Regrasp by Learning to Place</b><br>kw:Regrasping, Deep Learning, Robotic Manipulation"
41,41,45,https://openreview.net/forum?id=0QJeE5hkyFZ,FlingBot: The Unreasonable Effectiveness of Dynamic Manipulation for Cloth Unfolding ,"Huy Ha, Shuran Song",3.0,,,,,,,False,,"Dynamic manipulation, Cloth manipulation, Self-supervised learning","High-velocity dynamic actions (e.g., fling or throw) play a crucial role in our everyday interaction with deformable objects by improving our efficiency and effectively expanding our physical reach range. Yet, most prior works have tackled cloth manipulation using exclusively single-arm quasi-static actions, which requires a large number of interactions for challenging initial cloth configurations and strictly limits the maximum cloth size by the robot's reach range. In this work, we demonstrate the effectiveness of dynamic flinging actions for cloth unfolding with our proposed self-supervised learning framework, FlingBot.        Our approach learns how to unfold a piece of fabric from arbitrary initial configurations using a pick, stretch, and fling primitive for a dual-arm setup from visual observations. The final system achieves over 80% coverage within 3 actions on novel cloths,  can unfold cloths larger than the system's reach range, and generalizes to T-shirts despite being trained on only rectangular cloths.        We also finetuned FlingBot on a real-world dual-arm robot platform, where it increased the cloth coverage over 4 times more than the quasi-static baseline did. The simplicity of FlingBot combined with its superior performance over quasi-static baselines demonstrates the effectiveness of dynamic actions for deformable object manipulation.",,,,,"flingbot: the unreasonable effectiveness of dynamic manipulation for cloth unfolding  dynamic manipulation, cloth manipulation, self-supervised learning high-velocity dynamic actions (e.g., fling or throw) play a crucial role in our everyday interaction with deformable objects by improving our efficiency and effectively expanding our physical reach range. yet, most prior works have tackled cloth manipulation using exclusively single-arm quasi-static actions, which requires a large number of interactions for challenging initial cloth configurations and strictly limits the maximum cloth size by the robot's reach range. in this work, we demonstrate the effectiveness of dynamic flinging actions for cloth unfolding with our proposed self-supervised learning framework, flingbot.        our approach learns how to unfold a piece of fabric from arbitrary initial configurations using a pick, stretch, and fling primitive for a dual-arm setup from visual observations. the final system achieves over 80% coverage within 3 actions on novel cloths,  can unfold cloths larger than the system's reach range, and generalizes to t-shirts despite being trained on only rectangular cloths.        we also finetuned flingbot on a real-world dual-arm robot platform, where it increased the cloth coverage over 4 times more than the quasi-static baseline did. the simplicity of flingbot combined with its superior performance over quasi-static baselines demonstrates the effectiveness of dynamic actions for deformable object manipulation.",-3.146498,3.6580212,cloth,no reinforce,41,"id:41 (cloth)<br><b>FlingBot: The Unreasonable Effectiveness of Dynamic Manipulation for Cloth Unfolding </b><br>kw:Dynamic manipulation, Cloth manipulation, Self-supervised learning"
42,42,46,https://openreview.net/forum?id=9uFiX_HRsIL,CLIPort: What and Where Pathways for Robotic Manipulation,"Mohit Shridhar, Lucas Manuelli, Dieter Fox",3.0,,,,,,,False,,"Manipulation, Pre-trained Models, Vision Language Grounding, CLIP","How can we imbue robots with the ability to manipulate objects precisely but also to reason about them in terms of abstract concepts? Recent works in manipulation have shown that end-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. In parallel, there has been great progress in learning generalizable semantic representations for vision and language by training on large-scale internet data, however these representations lack the spatial understanding necessary for fine-grained manipulation. To this end, we propose a framework that combines the best of both worlds: a two-stream architecture with semantic and spatial pathways for vision-based manipulation. Specifically, we present CLIPort, a language-conditioned imitation-learning agent that combines the broad semantic understanding (what) of CLIP [1] with the spatial precision (where) of Transporter [2]. Our end-to-end framework is capable of solving a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. Experiments in simulated and real-world settings show that our approach is data efficient in few-shot settings and generalizes effectively to seen and unseen semantic concepts. We even learn one multi-task policy for 10 simulated and 9 real-world tasks that is better or comparable to single-task policies.",,,,,"cliport: what and where pathways for robotic manipulation manipulation, pre-trained models, vision language grounding, clip how can we imbue robots with the ability to manipulate objects precisely but also to reason about them in terms of abstract concepts? recent works in manipulation have shown that end-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. in parallel, there has been great progress in learning generalizable semantic representations for vision and language by training on large-scale internet data, however these representations lack the spatial understanding necessary for fine-grained manipulation. to this end, we propose a framework that combines the best of both worlds: a two-stream architecture with semantic and spatial pathways for vision-based manipulation. specifically, we present cliport, a language-conditioned imitation-learning agent that combines the broad semantic understanding (what) of clip [1] with the spatial precision (where) of transporter [2]. our end-to-end framework is capable of solving a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. experiments in simulated and real-world settings show that our approach is data efficient in few-shot settings and generalizes effectively to seen and unseen semantic concepts. we even learn one multi-task policy for 10 simulated and 9 real-world tasks that is better or comparable to single-task policies.",-1.611903,4.576036,semantic,no reinforce,42,"id:42 (semantic)<br><b>CLIPort: What and Where Pathways for Robotic Manipulation</b><br>kw:Manipulation, Pre-trained Models, Vision Language Grounding, CLIP"
43,43,60,https://openreview.net/forum?id=DgCWxJyERoQ,The Boombox: Visual Reconstruction from Acoustic Vibrations,"Boyuan Chen, Mia Chiquier, Hod Lipson, Carl Vondrick",3.0,,,,,,,False,,"Multimodal Perception, Object State Estimation, Audio","Interacting with bins and containers is a fundamental task in robotics, making state estimation of the objects inside the bin critical.         While robots often use cameras for state estimation, the visual modality is not always ideal due to occlusions and poor illumination. We introduce The Boombox, a container that uses sound to estimate the state of the contents inside a box. Based on the observation that the collision between objects and its containers will cause an acoustic vibration, we present a convolutional network for learning to reconstruct visual scenes. Although we use low-cost and low-power contact microphones to detect the vibrations, our results show that learning from multimodal data enables state estimation from affordable audio sensors. Due to the many ways that robots use containers, we believe the box will have a number of applications in robotics.",,,,,"the boombox: visual reconstruction from acoustic vibrations multimodal perception, object state estimation, audio interacting with bins and containers is a fundamental task in robotics, making state estimation of the objects inside the bin critical.         while robots often use cameras for state estimation, the visual modality is not always ideal due to occlusions and poor illumination. we introduce the boombox, a container that uses sound to estimate the state of the contents inside a box. based on the observation that the collision between objects and its containers will cause an acoustic vibration, we present a convolutional network for learning to reconstruct visual scenes. although we use low-cost and low-power contact microphones to detect the vibrations, our results show that learning from multimodal data enables state estimation from affordable audio sensors. due to the many ways that robots use containers, we believe the box will have a number of applications in robotics.",-1.1938218,4.2092156,containers,no reinforce,43,"id:43 (containers)<br><b>The Boombox: Visual Reconstruction from Acoustic Vibrations</b><br>kw:Multimodal Perception, Object State Estimation, Audio"
44,44,85,https://openreview.net/forum?id=f7KaqYLO3iE,A Differentiable Recipe for Learning Visual Non-Prehensile Planar Manipulation,"Bernardo Aceituno, Alberto Rodriguez, Shubham Tulsiani, Abhinav Gupta, Mustafa Mukadam",3.0,,,,,,,False,,"Manipulation, Visual learning, Differentiable optimization","Specifying tasks with videos is a powerful technique towards acquiring novel and general robot skills. However, reasoning over mechanics and dexterous interactions can make it challenging to scale visual learning for contact-rich manipulation. In this work, we focus on the problem of visual dexterous planar manipulation: given a video of an object in planar motion, find contact-aware robot actions that reproduce the same object motion. We propose a novel learning architecture that combines video decoding neural models with priors from contact mechanics by leveraging differentiable optimization and differentiable simulation. Through extensive simulated experiments, we investigate the interplay between traditional model-based techniques and modern deep learning approaches. We find that our modular and fully differentiable architecture outperforms learning-only methods on unseen objects and motions.",,,,,"a differentiable recipe for learning visual non-prehensile planar manipulation manipulation, visual learning, differentiable optimization specifying tasks with videos is a powerful technique towards acquiring novel and general robot skills. however, reasoning over mechanics and dexterous interactions can make it challenging to scale visual learning for contact-rich manipulation. in this work, we focus on the problem of visual dexterous planar manipulation: given a video of an object in planar motion, find contact-aware robot actions that reproduce the same object motion. we propose a novel learning architecture that combines video decoding neural models with priors from contact mechanics by leveraging differentiable optimization and differentiable simulation. through extensive simulated experiments, we investigate the interplay between traditional model-based techniques and modern deep learning approaches. we find that our modular and fully differentiable architecture outperforms learning-only methods on unseen objects and motions.",-1.7429674,3.6221328,differentiable,no reinforce,44,"id:44 (differentiable)<br><b>A Differentiable Recipe for Learning Visual Non-Prehensile Planar Manipulation</b><br>kw:Manipulation, Visual learning, Differentiable optimization"
45,45,87,https://openreview.net/forum?id=TavPBk4Zs9m,"BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments","Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart?n-Mart?n, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, Silvio Savarese, Hyowon Gweon, Jiajun Wu, L. Fei-Fei",3.0,,,,,,,False,,"Embodied AI, Benchmarking, Household activities","We introduce BEHAVIOR, a benchmark for embodied AI with 100 activities in simulation, spanning a range of everyday household chores such as cleaning, maintenance, and food preparation. These activities are designed to be realistic, diverse and complex, aiming to reproduce the challenges that agents must face in the real world. Building such a benchmark poses three fundamental difficulties for each activity: definition (it can differ by time, place, or person), instantiation in a simulator, and evaluation. BEHAVIOR addresses these with three innovations. First, we propose an object-centric, predicate logic–based description language for expressing an activity’s initial and goal conditions, enabling generation of diverse instances for any activity.  Second, we identify the simulator-agnostic features required by an underlying environment to support BEHAVIOR, and demonstrate its realization in one such simulator. Third, we introduce a set of metrics to measure task progress and efficiency, absolute and relative to human demonstrators.  We include 500 human demonstrations in virtual reality (VR) to serve as the human ground truth. Our experiments demonstrate that even state-of-the-art embodied AI solutions struggle with the level of realism, diversity, and complexity imposed by the activities in our benchmark. We will make BEHAVIOR publicly available to facilitate and calibrate the development of new embodied AI solutions.",,,,,"behavior: benchmark for everyday household activities in virtual, interactive, and ecological environments embodied ai, benchmarking, household activities we introduce behavior, a benchmark for embodied ai with 100 activities in simulation, spanning a range of everyday household chores such as cleaning, maintenance, and food preparation. these activities are designed to be realistic, diverse and complex, aiming to reproduce the challenges that agents must face in the real world. building such a benchmark poses three fundamental difficulties for each activity: definition (it can differ by time, place, or person), instantiation in a simulator, and evaluation. behavior addresses these with three innovations. first, we propose an object-centric, predicate logic–based description language for expressing an activity’s initial and goal conditions, enabling generation of diverse instances for any activity.  second, we identify the simulator-agnostic features required by an underlying environment to support behavior, and demonstrate its realization in one such simulator. third, we introduce a set of metrics to measure task progress and efficiency, absolute and relative to human demonstrators.  we include 500 human demonstrations in virtual reality (vr) to serve as the human ground truth. our experiments demonstrate that even state-of-the-art embodied ai solutions struggle with the level of realism, diversity, and complexity imposed by the activities in our benchmark. we will make behavior publicly available to facilitate and calibrate the development of new embodied ai solutions.",-2.7424464,4.2459235,activities,no reinforce,45,"id:45 (activities)<br><b>BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments</b><br>kw:Embodied AI, Benchmarking, Household activities"
46,46,90,https://openreview.net/forum?id=jOSWHddP1fZ,Goal-Auxiliary Actor-Critic for 6D Robotic Grasping with Point Clouds,"Lirui Wang, Yu Xiang, Wei Yang, Arsalan Mousavian, Dieter Fox",3.0,,,,,,,False,,"6D Robotic Grasping, Imitation Learning, Reinforcement Learning","6D robotic grasping beyond top-down bin-picking scenarios is a challenging task. Previous solutions based on 6D grasp synthesis with robot motion planning usually operate in an open-loop setting, which are sensitive to grasp synthesis errors. In this work, we propose a new method for learning closed-loop control policies for 6D grasping. Our policy takes a segmented point cloud of an object from an egocentric camera as input, and outputs continuous 6D control actions of the robot gripper for grasping the object. We combine imitation learning and reinforcement learning and introduce a goal-auxiliary actor-critic algorithm for policy learning. We demonstrate that our learned policy can be integrated into a tabletop 6D grasping system and a human-robot handover system to improve the grasping performance of unseen objects. Videos and code are available at https://sites.google.com/view/gaddpg.",,,,,"goal-auxiliary actor-critic for 6d robotic grasping with point clouds 6d robotic grasping, imitation learning, reinforcement learning 6d robotic grasping beyond top-down bin-picking scenarios is a challenging task. previous solutions based on 6d grasp synthesis with robot motion planning usually operate in an open-loop setting, which are sensitive to grasp synthesis errors. in this work, we propose a new method for learning closed-loop control policies for 6d grasping. our policy takes a segmented point cloud of an object from an egocentric camera as input, and outputs continuous 6d control actions of the robot gripper for grasping the object. we combine imitation learning and reinforcement learning and introduce a goal-auxiliary actor-critic algorithm for policy learning. we demonstrate that our learned policy can be integrated into a tabletop 6d grasping system and a human-robot handover system to improve the grasping performance of unseen objects. videos and code are available at https://sites.google.com/view/gaddpg.",-2.8299963,3.0339575,6d,reinforce,46,"id:46 (6d)<br><b>Goal-Auxiliary Actor-Critic for 6D Robotic Grasping with Point Clouds</b><br>kw:6D Robotic Grasping, Imitation Learning, Reinforcement Learning"
47,47,100,https://openreview.net/forum?id=kSnfpHJBJOt,DexVIP: Learning Dexterous Grasping with Human Hand Pose Priors from Video,"Priyanka Mandikal, Kristen Grauman",3.0,,,,☆,,,False,,"dexterous manipulation, learning from observations, learning from demonstrations, computer vision","Dexterous multi-fingered robotic hands have a formidable action space, yet their morphological similarity to the human hand holds immense potential to accelerate robot learning. We propose DexVIP, an approach to learn dexterous robotic grasping from human-object interactions present in in-the-wild YouTube videos. We do this by curating grasp images from human-object interaction videos and imposing a prior over the agent's hand pose when learning to grasp with deep reinforcement learning. A key advantage of our method is that the learned policy is able to leverage free-form in-the-wild visual data.  As a result, it can easily scale  to new objects, and it sidesteps the standard practice of collecting human demonstrations in a lab---a much more expensive and indirect way to capture human expertise. Through experiments on 27 objects with a 30-DoF simulated robot hand, we demonstrate that DexVIP compares favorably to existing approaches that lack a hand pose prior or rely on specialized tele-operation equipment to obtain human demonstrations, while also being faster to train.",,,,,"dexvip: learning dexterous grasping with human hand pose priors from video dexterous manipulation, learning from observations, learning from demonstrations, computer vision dexterous multi-fingered robotic hands have a formidable action space, yet their morphological similarity to the human hand holds immense potential to accelerate robot learning. we propose dexvip, an approach to learn dexterous robotic grasping from human-object interactions present in in-the-wild youtube videos. we do this by curating grasp images from human-object interaction videos and imposing a prior over the agent's hand pose when learning to grasp with deep reinforcement learning. a key advantage of our method is that the learned policy is able to leverage free-form in-the-wild visual data.  as a result, it can easily scale  to new objects, and it sidesteps the standard practice of collecting human demonstrations in a lab---a much more expensive and indirect way to capture human expertise. through experiments on 27 objects with a 30-dof simulated robot hand, we demonstrate that dexvip compares favorably to existing approaches that lack a hand pose prior or rely on specialized tele-operation equipment to obtain human demonstrations, while also being faster to train.",-1.9967251,4.456215,hand,reinforce,47,"id:47 (hand)<br><b>DexVIP: Learning Dexterous Grasping with Human Hand Pose Priors from Video</b><br>kw:dexterous manipulation, learning from observations, learning from demonstrations, computer vision"
48,48,129,https://openreview.net/forum?id=JrsfBJtDFdI,What Matters in Learning from Offline Human Demonstrations for Robot Manipulation,"Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, Roberto Mart?n-Mart?n",3.0,,,,☆,,,False,,"Imitation Learning, Offline Reinforcement Learning, Robot Manipulation","Imitating human demonstrations is a promising approach to endow robots with various manipulation capabilities. While recent advances have been made in imitation learning and batch (offline) reinforcement learning, a lack of open-source human datasets and reproducible learning methods make assessing the state of the field difficult. In this paper, we conduct an extensive study of six offline learning algorithms for robot manipulation on five simulated and three real-world multi-stage manipulation tasks of varying complexity, and with datasets of varying quality. Our study analyzes the most critical challenges when learning from offline human data for manipulation. Based on the study, we derive a series of lessons including the sensitivity to different algorithmic design choices, the dependence on the quality of the demonstrations, and the variability based on the stopping criteria due to the different objectives in training and evaluation. We also highlight opportunities for learning from human datasets, such as the ability to learn proficient policies on challenging, multi-stage tasks beyond the scope of current reinforcement learning methods, and the ability to easily scale to natural, real-world manipulation scenarios where only raw sensory signals are available. We have open-sourced our datasets and all algorithm implementations to facilitate future research and fair comparisons in learning from human demonstration data at https://arise-initiative.github.io/robomimic-web/",,,,,"what matters in learning from offline human demonstrations for robot manipulation imitation learning, offline reinforcement learning, robot manipulation imitating human demonstrations is a promising approach to endow robots with various manipulation capabilities. while recent advances have been made in imitation learning and batch (offline) reinforcement learning, a lack of open-source human datasets and reproducible learning methods make assessing the state of the field difficult. in this paper, we conduct an extensive study of six offline learning algorithms for robot manipulation on five simulated and three real-world multi-stage manipulation tasks of varying complexity, and with datasets of varying quality. our study analyzes the most critical challenges when learning from offline human data for manipulation. based on the study, we derive a series of lessons including the sensitivity to different algorithmic design choices, the dependence on the quality of the demonstrations, and the variability based on the stopping criteria due to the different objectives in training and evaluation. we also highlight opportunities for learning from human datasets, such as the ability to learn proficient policies on challenging, multi-stage tasks beyond the scope of current reinforcement learning methods, and the ability to easily scale to natural, real-world manipulation scenarios where only raw sensory signals are available. we have open-sourced our datasets and all algorithm implementations to facilitate future research and fair comparisons in learning from human demonstration data at https://arise-initiative.github.io/robomimic-web/",-3.0872984,3.3266103,human,reinforce,48,"id:48 (human)<br><b>What Matters in Learning from Offline Human Demonstrations for Robot Manipulation</b><br>kw:Imitation Learning, Offline Reinforcement Learning, Robot Manipulation"
49,49,166,https://openreview.net/forum?id=YrRoft_OeKp,Evaluations of the Gap between Supervised and Reinforcement Lifelong Learning on Robotic Manipulation Tasks,"Fan Yang, Huaping Liu",3.0,,,,?,,,False,,"Lifelong Learning, Reinforcement Learning, Manipulation","Overcoming catastrophic forgetting is of great importance for deep learning and robotics. Recent lifelong learning research has great advances in supervised learning. However, little work focuses on reinforcement learning(RL). We focus on evaluating the performances of state-of-the-art lifelong learning algorithms on robotic reinforcement learning tasks. We mainly focus on the properties of overcoming catastrophic forgetting for these algorithms. We summarize the pros and cons for each category of lifelong learning algorithms when applied in RL scenarios. We propose a framework to modify supervised lifelong learning algorithms to be compatible with RL. We also develop a manipulation benchmark task set for our evaluations.",,,,,"evaluations of the gap between supervised and reinforcement lifelong learning on robotic manipulation tasks lifelong learning, reinforcement learning, manipulation overcoming catastrophic forgetting is of great importance for deep learning and robotics. recent lifelong learning research has great advances in supervised learning. however, little work focuses on reinforcement learning(rl). we focus on evaluating the performances of state-of-the-art lifelong learning algorithms on robotic reinforcement learning tasks. we mainly focus on the properties of overcoming catastrophic forgetting for these algorithms. we summarize the pros and cons for each category of lifelong learning algorithms when applied in rl scenarios. we propose a framework to modify supervised lifelong learning algorithms to be compatible with rl. we also develop a manipulation benchmark task set for our evaluations.",-2.7748876,2.764858,lifelong,reinforce,49,"id:49 (lifelong)<br><b>Evaluations of the Gap between Supervised and Reinforcement Lifelong Learning on Robotic Manipulation Tasks</b><br>kw:Lifelong Learning, Reinforcement Learning, Manipulation"
50,50,170,https://openreview.net/forum?id=U0Q8CrtBJxJ,Beyond Pick-and-Place: Tackling Robotic Stacking of Diverse Shapes,"Alex X. Lee, Coline Manon Devin, Yuxiang Zhou, Thomas Lampe, Konstantinos Bousmalis, Jost Tobias Springenberg, Arunkumar Byravan, Abbas Abdolmaleki, Nimrod Gileadi, David Khosid, Claudio Fantacci, Jose Enrique Chen, Akhil Raju, Rae Jeong, Michael Neunert, Antoine Laurens, Stefano Saliceti, Federico Casarini, Martin Riedmiller, raia hadsell, Francesco Nori",3.0,,,,?,,,False,,"sim-to-real, offline RL, manipulation, stacking, robot learning","We study the problem of robotic stacking with objects of complex geometry. We propose a challenging and diverse set of such objects that was carefully designed to require strategies beyond a simple “pick-and-place” solution. Our method is a reinforcement learning (RL) approach combined with vision-based interactive policy distillation and simulation-to-reality transfer. Our learned policies can efficiently handle multiple object combinations in the real world and exhibit a large variety of stacking skills. In a large experimental study, we investigate what choices matter for learning such general vision-based agents in simulation, and what affects optimal transfer to the real robot. We then leverage data collected by such policies and improve upon them with offline RL. A video and a blog post of our work are provided as supplementary material.",,,,,"beyond pick-and-place: tackling robotic stacking of diverse shapes sim-to-real, offline rl, manipulation, stacking, robot learning we study the problem of robotic stacking with objects of complex geometry. we propose a challenging and diverse set of such objects that was carefully designed to require strategies beyond a simple “pick-and-place” solution. our method is a reinforcement learning (rl) approach combined with vision-based interactive policy distillation and simulation-to-reality transfer. our learned policies can efficiently handle multiple object combinations in the real world and exhibit a large variety of stacking skills. in a large experimental study, we investigate what choices matter for learning such general vision-based agents in simulation, and what affects optimal transfer to the real robot. we then leverage data collected by such policies and improve upon them with offline rl. a video and a blog post of our work are provided as supplementary material.",-2.4649515,3.3217535,stacking,reinforce,50,"id:50 (stacking)<br><b>Beyond Pick-and-Place: Tackling Robotic Stacking of Diverse Shapes</b><br>kw:sim-to-real, offline RL, manipulation, stacking, robot learning"
51,51,202,https://openreview.net/forum?id=TsqkJJMgHkk,FabricFlowNet: Bimanual Cloth Manipulation with a Flow-based Policy,"Thomas Weng, Sujay Man Bajracharya, Yufei Wang, David Held",3.0,,,,,,,False,,"deformable object manipulation, optical flow, bimanual manipulation","We address the problem of goal-directed cloth manipulation, a challenging task due to the deformability of cloth. Our insight is that optical flow, a technique normally used for motion estimation in video, can also provide an effective representation for corresponding cloth poses across observation and goal images. We introduce FabricFlowNet (FFN), a cloth manipulation policy that leverages flow as both an input and as an action representation to improve performance. FabricFlowNet also elegantly switches between bimanual and single-arm actions based on the desired goal. We show that FabricFlowNet significantly outperforms state-of-the-art model-free and model-based cloth manipulation policies that take image input. We also present real-world experiments on a bimanual system, demonstrating effective sim-to-real transfer.  Finally, we show that our method generalizes when trained on a single square cloth to other cloth shapes, such as T-shirts and rectangular cloths. Video and other supplementary materials are available at: https://sites.google.com/view/fabricflownet.",,,,,"fabricflownet: bimanual cloth manipulation with a flow-based policy deformable object manipulation, optical flow, bimanual manipulation we address the problem of goal-directed cloth manipulation, a challenging task due to the deformability of cloth. our insight is that optical flow, a technique normally used for motion estimation in video, can also provide an effective representation for corresponding cloth poses across observation and goal images. we introduce fabricflownet (ffn), a cloth manipulation policy that leverages flow as both an input and as an action representation to improve performance. fabricflownet also elegantly switches between bimanual and single-arm actions based on the desired goal. we show that fabricflownet significantly outperforms state-of-the-art model-free and model-based cloth manipulation policies that take image input. we also present real-world experiments on a bimanual system, demonstrating effective sim-to-real transfer.  finally, we show that our method generalizes when trained on a single square cloth to other cloth shapes, such as t-shirts and rectangular cloths. video and other supplementary materials are available at: https://sites.google.com/view/fabricflownet.",-2.1462407,3.932374,cloth,no reinforce,51,"id:51 (cloth)<br><b>FabricFlowNet: Bimanual Cloth Manipulation with a Flow-based Policy</b><br>kw:deformable object manipulation, optical flow, bimanual manipulation"
52,52,284,https://openreview.net/forum?id=7uSBJDoP7tY,A Simple Method for Complex In-hand Manipulation,"Tao Chen, Jie Xu, Pulkit Agrawal",3.0,,,,☆,,,False,,"In-hand manipulation, dexterous manipulation, object reorientation",In-hand object reorientation has been a challenging problem in robotics due to high dimensional actuation space and the frequent change in contact state between the fingers and the objects. We present a simple model-free framework that can learn to reorient objects with both the hand facing upwards and downwards. We demonstrate the capability of reorienting over 2000 geometrically different objects in both cases. The learned policies show strong zero-shot transfer performance on new objects. We provide evidence that these policies are amenable to real-world operation by distilling them to use observations easily available in the real world. The videos of the learned policies are available at: https://sites.google.com/view/in-hand-reorientation.,,,,,"a simple method for complex in-hand manipulation in-hand manipulation, dexterous manipulation, object reorientation in-hand object reorientation has been a challenging problem in robotics due to high dimensional actuation space and the frequent change in contact state between the fingers and the objects. we present a simple model-free framework that can learn to reorient objects with both the hand facing upwards and downwards. we demonstrate the capability of reorienting over 2000 geometrically different objects in both cases. the learned policies show strong zero-shot transfer performance on new objects. we provide evidence that these policies are amenable to real-world operation by distilling them to use observations easily available in the real world. the videos of the learned policies are available at: https://sites.google.com/view/in-hand-reorientation.",-1.1606336,3.6012864,in-hand,no reinforce,52,"id:52 (in-hand)<br><b>A Simple Method for Complex In-hand Manipulation</b><br>kw:In-hand manipulation, dexterous manipulation, object reorientation"
53,53,287,https://openreview.net/forum?id=2WivNtnaFzx,SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo,"Mike Laskey, Brijen Thananjeyan, Kevin Stone, Thomas Kollar, Mark Tjersland",3.0,,,,,,,False,,"Sim-to-Real, Computer Vision, Manipulation","Robot manipulation of unknown objects in unstructured environments is a challenging problem due to the variety of shapes, materials, arrangements and lighting conditions.  Even with large-scale real-world data collection, robust perception and manipulation of transparent and reflective objects across various lighting conditions remains challenging.  To address these challenges we propose an approach to performing sim-to-real transfer of robotic perception.  The underlying model, SimNet, is trained as a single multi-headed neural network using simulated stereo data as input and simulated object segmentation masks, 3D oriented bounding boxes (OBBs), object keypoints and disparity as output.  A key component of SimNet is the incorporation of a learned stereo sub-network that predicts disparity.  SimNet is evaluated on 2D car detection, unknown object detection and deformable object keypoint detection and significantly outperforms a baseline that uses a structured light RGB-D sensor. By inferring grasp positions using the OBB and keypoint predictions, SimNet can be used to perform end-to-end manipulation of unknown objects in both ""easy"" and ""hard"" scenarios using our fleet of Toyota HSR robots in four home environments. In unknown object grasping experiments, the predictions from the baseline RGB-D network and SimNet enable successful grasps of most of the ""easy"" objects. However, the RGB-D baseline only grasps 35% of the ""hard"" (e.g., transparent) objects, while SimNet grasps 95% suggesting that SimNet can enable robust manipulation of unknown objects, including transparent objects, in unknown environments.",,,,,"simnet: enabling robust unknown object manipulation from pure synthetic data via stereo sim-to-real, computer vision, manipulation robot manipulation of unknown objects in unstructured environments is a challenging problem due to the variety of shapes, materials, arrangements and lighting conditions.  even with large-scale real-world data collection, robust perception and manipulation of transparent and reflective objects across various lighting conditions remains challenging.  to address these challenges we propose an approach to performing sim-to-real transfer of robotic perception.  the underlying model, simnet, is trained as a single multi-headed neural network using simulated stereo data as input and simulated object segmentation masks, 3d oriented bounding boxes (obbs), object keypoints and disparity as output.  a key component of simnet is the incorporation of a learned stereo sub-network that predicts disparity.  simnet is evaluated on 2d car detection, unknown object detection and deformable object keypoint detection and significantly outperforms a baseline that uses a structured light rgb-d sensor. by inferring grasp positions using the obb and keypoint predictions, simnet can be used to perform end-to-end manipulation of unknown objects in both ""easy"" and ""hard"" scenarios using our fleet of toyota hsr robots in four home environments. in unknown object grasping experiments, the predictions from the baseline rgb-d network and simnet enable successful grasps of most of the ""easy"" objects. however, the rgb-d baseline only grasps 35% of the ""hard"" (e.g., transparent) objects, while simnet grasps 95% suggesting that simnet can enable robust manipulation of unknown objects, including transparent objects, in unknown environments.",-1.7060484,4.70716,unknown,no reinforce,53,"id:53 (unknown)<br><b>SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo</b><br>kw:Sim-to-Real, Computer Vision, Manipulation"
54,54,293,https://openreview.net/forum?id=2uGN5jNJROR,iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks,"Chengshu Li, Fei Xia, Roberto Mart?n-Mart?n, Michael Lingelbach, Sanjana Srivastava, Bokui Shen, Kent Elliott Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, Andrey Kurenkov, Karen Liu, Hyowon Gweon, Jiajun Wu, L. Fei-Fei, Silvio Savarese",3.0,,,,,,,False,,"Simulation Environment, Embodied AI, Virtual Reality Interface","Recent research in embodied AI has been boosted by the use of simulation environments to develop and train robot learning approaches. However, the use of simulation has skewed the attention to tasks that only require what robotics simulators can simulate: motion and physical contact. We present iGibson 2.0, an open-source simulation environment that supports the simulation of a more diverse set of household tasks through three key innovations. First, iGibson 2.0 supports object states, including temperature, wetness level, cleanliness level, and toggled and sliced states, necessary to cover a wider range of tasks. Second, iGibson 2.0 implements a set of predicate logic functions that map the simulator states to logic states like Cooked or Soaked. Additionally, given a logic state, iGibson 2.0 can sample valid physical states that satisfy it. This functionality can generate potentially infinite instances of tasks with minimal effort from the users. The sampling mechanism allows our scenes to be more densely populated with small objects in semantically meaningful locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to immerse humans in its scenes to collect demonstrations. As a result, we can collect demonstrations from humans on these new types of tasks, and use them for imitation learning. We evaluate the new capabilities of iGibson 2.0 to enable robot learning of novel tasks, in the hope of demonstrating the potential of this new simulator to support new research in embodied AI. iGibson 2.0 and its new dataset will be publicly available after the anonymous reviewing process.",,,,,"igibson 2.0: object-centric simulation for robot learning of everyday household tasks simulation environment, embodied ai, virtual reality interface recent research in embodied ai has been boosted by the use of simulation environments to develop and train robot learning approaches. however, the use of simulation has skewed the attention to tasks that only require what robotics simulators can simulate: motion and physical contact. we present igibson 2.0, an open-source simulation environment that supports the simulation of a more diverse set of household tasks through three key innovations. first, igibson 2.0 supports object states, including temperature, wetness level, cleanliness level, and toggled and sliced states, necessary to cover a wider range of tasks. second, igibson 2.0 implements a set of predicate logic functions that map the simulator states to logic states like cooked or soaked. additionally, given a logic state, igibson 2.0 can sample valid physical states that satisfy it. this functionality can generate potentially infinite instances of tasks with minimal effort from the users. the sampling mechanism allows our scenes to be more densely populated with small objects in semantically meaningful locations. third, igibson 2.0 includes a virtual reality (vr) interface to immerse humans in its scenes to collect demonstrations. as a result, we can collect demonstrations from humans on these new types of tasks, and use them for imitation learning. we evaluate the new capabilities of igibson 2.0 to enable robot learning of novel tasks, in the hope of demonstrating the potential of this new simulator to support new research in embodied ai. igibson 2.0 and its new dataset will be publicly available after the anonymous reviewing process.",-1.4974475,4.6941214,igibson,no reinforce,54,"id:54 (igibson)<br><b>iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks</b><br>kw:Simulation Environment, Embodied AI, Virtual Reality Interface"
55,55,340,https://openreview.net/forum?id=K5-J-Espnaq,Adversarial Skill Chaining for Long-Horizon Robot Manipulation via Terminal State Regularization,"Youngwoon Lee, Joseph J Lim, Anima Anandkumar, Yuke Zhu",3.0,,,,,,,False,,"Long-Horizon Manipulation, Skill Chaining, Reinforcement Learning, Adversarial Imitation Learning","Skill chaining is a promising approach for synthesizing complex behaviors by sequentially combining previously learned skills. Yet, a naive composition of skills fails when a policy encounters a starting state never seen during its training. For successful skill chaining, prior approaches attempt to widen the policy's starting state distribution. However, these approaches require larger state distributions to be covered as more polices are sequenced, and thus are limited to short skill sequences. In this paper, we propose to chain multiple policies without excessively large initial state distributions by regularizing the terminal state distributions in an adversarial learning framework. We evaluate our approach on a set of complex long-horizon manipulation tasks of furniture assembly. Our results have shown that our method establishes the first model-free reinforcement learning algorithm to solve these tasks; whereas prior skill chaining approaches fail.",,,,,"adversarial skill chaining for long-horizon robot manipulation via terminal state regularization long-horizon manipulation, skill chaining, reinforcement learning, adversarial imitation learning skill chaining is a promising approach for synthesizing complex behaviors by sequentially combining previously learned skills. yet, a naive composition of skills fails when a policy encounters a starting state never seen during its training. for successful skill chaining, prior approaches attempt to widen the policy's starting state distribution. however, these approaches require larger state distributions to be covered as more polices are sequenced, and thus are limited to short skill sequences. in this paper, we propose to chain multiple policies without excessively large initial state distributions by regularizing the terminal state distributions in an adversarial learning framework. we evaluate our approach on a set of complex long-horizon manipulation tasks of furniture assembly. our results have shown that our method establishes the first model-free reinforcement learning algorithm to solve these tasks; whereas prior skill chaining approaches fail.",-2.348416,3.8065035,skill,reinforce,55,"id:55 (skill)<br><b>Adversarial Skill Chaining for Long-Horizon Robot Manipulation via Terminal State Regularization</b><br>kw:Long-Horizon Manipulation, Skill Chaining, Reinforcement Learning, Adversarial Imitation Learning"
56,56,344,https://openreview.net/forum?id=mOLu2rODIJF,SORNet: Spatial Object-Centric Representations for Sequential Manipulation,"Wentao Yuan, Chris Paxton, Karthik Desingh, Dieter Fox",3.0,,,,,,,False,,"Object-centric Representation, Spatial Reasoning, Manipulation","Sequential manipulation tasks require a robot to perceive the state of an environment and plan a sequence of actions leading to a desired goal state, where the ability to reason about spatial relationships among object entities from raw sensor inputs is crucial. Prior works relying on explicit state estimation or end-to-end learning struggle with novel objects.         In this work, we propose SORNet (Spatial Object-Centric Representation Network), which extracts object-centric representations from RGB images % trained from large amounts of simulated robotic manipulation data, conditioned on canonical views of the objects of interest. We show that the object embeddings learned by SORNet generalize zero-shot to unseen object entities on three spatial reasoning tasks: spatial relationship classification, skill precondition classification and relative direction regression, significantly outperforming baselines. Further, we present real-world robotic experiments demonstrating the usage of the learned object embeddings in task planning for sequential manipulation.",,,,,"sornet: spatial object-centric representations for sequential manipulation object-centric representation, spatial reasoning, manipulation sequential manipulation tasks require a robot to perceive the state of an environment and plan a sequence of actions leading to a desired goal state, where the ability to reason about spatial relationships among object entities from raw sensor inputs is crucial. prior works relying on explicit state estimation or end-to-end learning struggle with novel objects.         in this work, we propose sornet (spatial object-centric representation network), which extracts object-centric representations from rgb images % trained from large amounts of simulated robotic manipulation data, conditioned on canonical views of the objects of interest. we show that the object embeddings learned by sornet generalize zero-shot to unseen object entities on three spatial reasoning tasks: spatial relationship classification, skill precondition classification and relative direction regression, significantly outperforming baselines. further, we present real-world robotic experiments demonstrating the usage of the learned object embeddings in task planning for sequential manipulation.",-1.7535236,3.657376,spatial,no reinforce,56,"id:56 (spatial)<br><b>SORNet: Spatial Object-Centric Representations for Sequential Manipulation</b><br>kw:Object-centric Representation, Spatial Reasoning, Manipulation"
57,57,388,https://openreview.net/forum?id=NZnz3cExrDW,Distilling Motion-Planner Augmented Policies into Visual Control Policies for Robot Manipulation,"I-Chun Arthur Liu, Shagun Uppal, Gaurav Sukhatme, Joseph J Lim, Peter Englert, Youngwoon Lee",3.0,,,,,,,False,,"Reinforcement Learning, Motion Planning, Robot Manipulation","We address the problem of learning complex manipulation tasks in obstructed environments from visual observations. Learning such tasks directly in the visual domain is challenging and sample inefficient due to hard exploration in the presence of obstacles and high-dimensional observation inputs. We propose a framework to learn a visual control policy in a two-step procedure: 1) Learn a policy with long-horizon planning capabilities in state space using motion planning-augmented reinforcement learning; 2) Distilling this policy into the visual domain while removing the dependency on motion planner. In this step, we leverage the exploration and experiences accumulated by the state agent from the first step in an asymmetric actor-critic framework. We evaluate our method on three manipulation tasks in obstructed environments and compare it to various reinforcement and imitation learning baselines. The results indicate that our framework is highly sample-efficient and outperforms state-of-the-art algorithms. We also demonstrate our approach's zero-shot domain transfer capabilities to other environment settings with unseen distractors by coupling our method with domain randomization.",,,,,"distilling motion-planner augmented policies into visual control policies for robot manipulation reinforcement learning, motion planning, robot manipulation we address the problem of learning complex manipulation tasks in obstructed environments from visual observations. learning such tasks directly in the visual domain is challenging and sample inefficient due to hard exploration in the presence of obstacles and high-dimensional observation inputs. we propose a framework to learn a visual control policy in a two-step procedure: 1) learn a policy with long-horizon planning capabilities in state space using motion planning-augmented reinforcement learning; 2) distilling this policy into the visual domain while removing the dependency on motion planner. in this step, we leverage the exploration and experiences accumulated by the state agent from the first step in an asymmetric actor-critic framework. we evaluate our method on three manipulation tasks in obstructed environments and compare it to various reinforcement and imitation learning baselines. the results indicate that our framework is highly sample-efficient and outperforms state-of-the-art algorithms. we also demonstrate our approach's zero-shot domain transfer capabilities to other environment settings with unseen distractors by coupling our method with domain randomization.",-2.5602982,3.0761082,domain,reinforce,57,"id:57 (domain)<br><b>Distilling Motion-Planner Augmented Policies into Visual Control Policies for Robot Manipulation</b><br>kw:Reinforcement Learning, Motion Planning, Robot Manipulation"
58,58,399,https://openreview.net/forum?id=wMoHIYBsj2_,V-MAO: Generative Modeling for Multi-Arm Manipulation of Articulated Objects,"Xingyu Liu, Kris M. Kitani",3.0,,,,,,,False,,"Articulated object, generative model, variational inference","Manipulating articulated objects requires multiple robot arms in general. It is challenging to enable multiple robot arms to collaboratively complete manipulation tasks on articulated objects. In this paper, we present V-MAO, a framework for learning multi-arm manipulation of articulated objects. Our framework includes a variational generative model that learns contact point distribution over object rigid parts for each robot arm. The training signal is obtained from interaction with the simulation environment which is enabled by planning and object-centric control. We deploy our framework in a customized MuJoCo simulation environment and demonstrate that our framework achieves a high success rate on six different objects and two different robots. We also show that generative modeling can effectively learn the contact point distribution on articulated objects.",,,,,"v-mao: generative modeling for multi-arm manipulation of articulated objects articulated object, generative model, variational inference manipulating articulated objects requires multiple robot arms in general. it is challenging to enable multiple robot arms to collaboratively complete manipulation tasks on articulated objects. in this paper, we present v-mao, a framework for learning multi-arm manipulation of articulated objects. our framework includes a variational generative model that learns contact point distribution over object rigid parts for each robot arm. the training signal is obtained from interaction with the simulation environment which is enabled by planning and object-centric control. we deploy our framework in a customized mujoco simulation environment and demonstrate that our framework achieves a high success rate on six different objects and two different robots. we also show that generative modeling can effectively learn the contact point distribution on articulated objects.",-1.6709913,3.7096305,articulated,no reinforce,58,"id:58 (articulated)<br><b>V-MAO: Generative Modeling for Multi-Arm Manipulation of Articulated Objects</b><br>kw:Articulated object, generative model, variational inference"
59,59,5,https://openreview.net/forum?id=tfLu5W6SW5J,Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation,"Suraj Nair, Eric Mitchell, Kevin Chen, brian ichter, Silvio Savarese, Chelsea Finn",4.0,,,,,,,False,,"Natural Language, Offline Reinforcement Learning, Visuomotor Manipulation","We study the problem of learning a range of vision-based manipulation tasks from a large offline dataset of robot interaction. In order to accomplish this, humans need easy and effective ways of specifying tasks to the robot. Goal images are one popular form of task specification, as they are already grounded in the robot's observation space. However, goal images also have a number of drawbacks: they are inconvenient for humans to provide, they can over-specify the desired behavior leading to a sparse reward signal, or under-specify task information in the case of non-goal reaching tasks. Natural language provides a convenient and flexible alternative for task specification, but comes with the challenge of grounding language in the robot's observation space. To scalably learn this grounding we propose to leverage offline pre-collected robotic datasets (including highly sub-optimal, autonomously-collected data) with crowd-sourced natural language labels. With this data, we learn a simple classifier which predicts if a change in state completes a language instruction. This provides a language-conditioned reward function that can then be used for offline multi-task RL. In our experiments, we find that on language-conditioned manipulation tasks our approach outperforms both goal-image specifications and language conditioned imitation techniques by more than 25%,  and is able to perform a range of visuomotor tasks from natural language, such as “open the right drawer” and “move the stapler”, on a Franka Emika Panda robot.",,,,,"learning language-conditioned robot behavior from offline data and crowd-sourced annotation natural language, offline reinforcement learning, visuomotor manipulation we study the problem of learning a range of vision-based manipulation tasks from a large offline dataset of robot interaction. in order to accomplish this, humans need easy and effective ways of specifying tasks to the robot. goal images are one popular form of task specification, as they are already grounded in the robot's observation space. however, goal images also have a number of drawbacks: they are inconvenient for humans to provide, they can over-specify the desired behavior leading to a sparse reward signal, or under-specify task information in the case of non-goal reaching tasks. natural language provides a convenient and flexible alternative for task specification, but comes with the challenge of grounding language in the robot's observation space. to scalably learn this grounding we propose to leverage offline pre-collected robotic datasets (including highly sub-optimal, autonomously-collected data) with crowd-sourced natural language labels. with this data, we learn a simple classifier which predicts if a change in state completes a language instruction. this provides a language-conditioned reward function that can then be used for offline multi-task rl. in our experiments, we find that on language-conditioned manipulation tasks our approach outperforms both goal-image specifications and language conditioned imitation techniques by more than 25%,  and is able to perform a range of visuomotor tasks from natural language, such as “open the right drawer” and “move the stapler”, on a franka emika panda robot.",-2.3617666,3.7951217,language,reinforce,59,"id:59 (language)<br><b>Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation</b><br>kw:Natural Language, Offline Reinforcement Learning, Visuomotor Manipulation"
60,60,17,https://openreview.net/forum?id=eIk6eBz3Wlt,Trust Your Robots! Predictive Uncertainty Estimation of Neural Networks with Sparse Gaussian Processes,"Jongseok Lee, Jianxiang Feng, Matthias Humt, Marcus Gerhard M?ller, Rudolph Triebel",4.0,,,,,,,False,,"Robotic Introspection, Bayesian Deep Learning, Gaussian Processes","This paper presents a probabilistic framework to obtain both reliable and fast uncertainty estimates for predictions with Deep Neural Networks (DNNs). Our main contribution is a practical and principled combination of DNNs with sparse Gaussian Processes (GPs). We prove theoretically that DNNs can be seen as a special case of sparse GPs, namely mixtures of GP experts (MoE-GP), and we devise a learning algorithm that brings the derived theory into practice. In experiments from two different robotic tasks -- inverse dynamics of a manipulator and object detection on a micro-aerial vehicle (MAV) -- we show the effectiveness of our approach in terms of predictive uncertainty, improved scalability, and run-time efficiency on a Jetson TX2. We thus argue that our approach can pave the way towards reliable and fast robot learning systems with uncertainty awareness.",,,,,"trust your robots! predictive uncertainty estimation of neural networks with sparse gaussian processes robotic introspection, bayesian deep learning, gaussian processes this paper presents a probabilistic framework to obtain both reliable and fast uncertainty estimates for predictions with deep neural networks (dnns). our main contribution is a practical and principled combination of dnns with sparse gaussian processes (gps). we prove theoretically that dnns can be seen as a special case of sparse gps, namely mixtures of gp experts (moe-gp), and we devise a learning algorithm that brings the derived theory into practice. in experiments from two different robotic tasks -- inverse dynamics of a manipulator and object detection on a micro-aerial vehicle (mav) -- we show the effectiveness of our approach in terms of predictive uncertainty, improved scalability, and run-time efficiency on a jetson tx2. we thus argue that our approach can pave the way towards reliable and fast robot learning systems with uncertainty awareness.",-1.7633423,3.955461,processes,no reinforce,60,"id:60 (processes)<br><b>Trust Your Robots! Predictive Uncertainty Estimation of Neural Networks with Sparse Gaussian Processes</b><br>kw:Robotic Introspection, Bayesian Deep Learning, Gaussian Processes"
61,61,51,https://openreview.net/forum?id=p-TBwVowXRH,Learning Multi-Stage Tasks with One Demonstration via Self-Replay,"Norman Di Palo, Edward Johns",4.0,,,,,,,False,,"Imitation Learning, Multi-Stage Imitation Learning, Robot Manipulation","In this work, we introduce a novel method to learn everyday-like multi-stage tasks from a single human demonstration, without requiring any prior object knowledge. Inspired by the recent Coarse-to-Fine Imitation Learning, we model imitation learning as a learned object reaching phase followed by an open-loop replay of the operator's actions. We build upon this for multi-stage tasks where, following the human demonstration, the robot can autonomously collect image data for the entire multi-stage task, by reaching the next object in the sequence and then replaying the demonstration, repeating in a loop for all stages of the task. We evaluate with real-world experiments on a set of everyday multi-stage tasks, which we show that our method can solve from a single demonstration.        Videos and supplementary material can be found at this anonymous webpage: https://sites.google.com/view/self-replay .",,,,,"learning multi-stage tasks with one demonstration via self-replay imitation learning, multi-stage imitation learning, robot manipulation in this work, we introduce a novel method to learn everyday-like multi-stage tasks from a single human demonstration, without requiring any prior object knowledge. inspired by the recent coarse-to-fine imitation learning, we model imitation learning as a learned object reaching phase followed by an open-loop replay of the operator's actions. we build upon this for multi-stage tasks where, following the human demonstration, the robot can autonomously collect image data for the entire multi-stage task, by reaching the next object in the sequence and then replaying the demonstration, repeating in a loop for all stages of the task. we evaluate with real-world experiments on a set of everyday multi-stage tasks, which we show that our method can solve from a single demonstration.        videos and supplementary material can be found at this anonymous webpage: https://sites.google.com/view/self-replay .",-2.1411731,3.2002919,multi-stage,no reinforce,61,"id:61 (multi-stage)<br><b>Learning Multi-Stage Tasks with One Demonstration via Self-Replay</b><br>kw:Imitation Learning, Multi-Stage Imitation Learning, Robot Manipulation"
62,62,53,https://openreview.net/forum?id=TSuSGVkjuXd,Smooth Exploration for Robotic Reinforcement Learning,"Antonin Raffin, Jens Kober, Freek Stulp",4.0,,,,,,,False,,"Robotics, Reinforcement Learning, Exploration, Real World","Reinforcement learning (RL) enables robots to learn skills from interactions with the real world.        In practice, the unstructured step-based exploration used in Deep RL -- often very successful in simulation -- leads to jerky motion patterns on real robots.        Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot.        We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms.        To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE).        We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car.        The noise sampling interval of gSDE enables a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance.",,,,,"smooth exploration for robotic reinforcement learning robotics, reinforcement learning, exploration, real world reinforcement learning (rl) enables robots to learn skills from interactions with the real world.        in practice, the unstructured step-based exploration used in deep rl -- often very successful in simulation -- leads to jerky motion patterns on real robots.        consequences of the resulting shaky behavior are poor exploration, or even damage to the robot.        we address these issues by adapting state-dependent exploration (sde) to current deep rl algorithms.        to enable this adaptation, we propose two extensions to the original sde, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gsde).        we evaluate gsde both in simulation, on pybullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an rc car.        the noise sampling interval of gsde enables a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance.",-3.0792212,3.264242,exploration,reinforce,62,"id:62 (exploration)<br><b>Smooth Exploration for Robotic Reinforcement Learning</b><br>kw:Robotics, Reinforcement Learning, Exploration, Real World"
63,63,70,https://openreview.net/forum?id=KKBfrCzCVOn,ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,"Ryan Hoque, Ashwin Balakrishna, Ellen Novoseller, Daniel S. Brown, Albert Wilcox, Ken Goldberg",4.0,,,,,,,False,,"Imitation Learning, Fleet Learning, Human Robot Interaction","Effective robot learning often requires online human feedback and interventions that can cost significant human time, giving rise to the central challenge in interactive imitation learning: is it possible to control the timing and length of interventions to both facilitate learning and limit burden on the human supervisor? This paper presents ThriftyDAgger, an algorithm for actively querying a human supervisor given a desired budget of human interventions. ThriftyDAgger uses a learned switching policy to solicit interventions only at states that are sufficiently (1) novel, where the robot policy has no reference behavior to imitate, or (2) risky, where the robot has low confidence in task completion. To detect the latter, we introduce a novel metric for estimating risk under the current robot policy. Experiments in simulation and on a physical cable routing experiment suggest that ThriftyDAgger's intervention criteria balances task performance and supervisor burden more effectively than prior algorithms. ThriftyDAgger can also be applied at execution time, where it achieves a 100% success rate on both the simulation and physical tasks. A user study (N=10) in which users control a three-robot fleet while also performing a concentration task suggests that ThriftyDAgger increases human and robot performance by 58% and 80% respectively compared to the next best algorithm while reducing supervisor burden. See https://tinyurl.com/thrifty-dagger for supplementary material.",,,,,"thriftydagger: budget-aware novelty and risk gating for interactive imitation learning imitation learning, fleet learning, human robot interaction effective robot learning often requires online human feedback and interventions that can cost significant human time, giving rise to the central challenge in interactive imitation learning: is it possible to control the timing and length of interventions to both facilitate learning and limit burden on the human supervisor? this paper presents thriftydagger, an algorithm for actively querying a human supervisor given a desired budget of human interventions. thriftydagger uses a learned switching policy to solicit interventions only at states that are sufficiently (1) novel, where the robot policy has no reference behavior to imitate, or (2) risky, where the robot has low confidence in task completion. to detect the latter, we introduce a novel metric for estimating risk under the current robot policy. experiments in simulation and on a physical cable routing experiment suggest that thriftydagger's intervention criteria balances task performance and supervisor burden more effectively than prior algorithms. thriftydagger can also be applied at execution time, where it achieves a 100% success rate on both the simulation and physical tasks. a user study (n=10) in which users control a three-robot fleet while also performing a concentration task suggests that thriftydagger increases human and robot performance by 58% and 80% respectively compared to the next best algorithm while reducing supervisor burden. see https://tinyurl.com/thrifty-dagger for supplementary material.",-3.3962321,3.3284495,thriftydagger,no reinforce,63,"id:63 (thriftydagger)<br><b>ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning</b><br>kw:Imitation Learning, Fleet Learning, Human Robot Interaction"
64,64,73,https://openreview.net/forum?id=Ei3MOY2rDHB,"My House, My Rules: Learning Tidying Preferences with Graph Neural Networks","Ivan Kapelyukh, Edward Johns",4.0,,,,,,,False,,"graph neural networks, preference learning, rearrangement tasks","Robots that arrange household objects should do so according to the user's preferences, which are inherently subjective and difficult to model. We present NeatNet: a novel Variational Autoencoder architecture using Graph Neural Network layers, which can extract a low-dimensional latent preference vector from a user by observing how they arrange scenes. Given any set of objects, this vector can then be used to generate an arrangement which is tailored to that user's spatial preferences, with word embeddings used for generalisation to new objects. We develop a tidying simulator to gather rearrangement examples from 75 users, and demonstrate empirically that our method consistently produces neat and personalised arrangements across a variety of rearrangement scenarios.",,,,,"my house, my rules: learning tidying preferences with graph neural networks graph neural networks, preference learning, rearrangement tasks robots that arrange household objects should do so according to the user's preferences, which are inherently subjective and difficult to model. we present neatnet: a novel variational autoencoder architecture using graph neural network layers, which can extract a low-dimensional latent preference vector from a user by observing how they arrange scenes. given any set of objects, this vector can then be used to generate an arrangement which is tailored to that user's spatial preferences, with word embeddings used for generalisation to new objects. we develop a tidying simulator to gather rearrangement examples from 75 users, and demonstrate empirically that our method consistently produces neat and personalised arrangements across a variety of rearrangement scenarios.",-1.526874,4.3181925,rearrangement,no reinforce,64,"id:64 (rearrangement)<br><b>My House, My Rules: Learning Tidying Preferences with Graph Neural Networks</b><br>kw:graph neural networks, preference learning, rearrangement tasks"
65,65,81,https://openreview.net/forum?id=YwDvofEWlEx,Learning Behaviors through Physics-driven Latent Imagination,"Antoine Richard, Stephanie ARAVECCHIA, Matthieu Geist, C?dric Pradalier",4.0,,,,,,,True,,"Model-Based Reinforcement Learning, Field Robotics, Latent Models","Model-based reinforcement learning (MBRL) consists in learning a so-called world model, a representation of the environment through interactions with it, then use it to train an agent. This approach is particularly interesting in the context of field robotics, as it alleviates the need to train online, and reduces the risks inherent to directly training agents on real robots. However, in such approaches, the world encompasses both the part related to the robot itself and the rest of the environment.  We argue that decoupling the environment representation (for example, images or laser scans) from the dynamics of the physical system (that is, the robot and its physical state) may be beneficial:it can increase the flexibility of world models and open the door to further robustification. In this paper, we apply this concept to a strong latent-agent, Dreamer. We then showcase the increase of flexibility by transferring the environmental part of the world model from one robot (a boat) to another (a rover), simply by adapting the physical model in the imagination. We also demonstrate the robustness of our method through real-world experiments on a boat.",,,,,"learning behaviors through physics-driven latent imagination model-based reinforcement learning, field robotics, latent models model-based reinforcement learning (mbrl) consists in learning a so-called world model, a representation of the environment through interactions with it, then use it to train an agent. this approach is particularly interesting in the context of field robotics, as it alleviates the need to train online, and reduces the risks inherent to directly training agents on real robots. however, in such approaches, the world encompasses both the part related to the robot itself and the rest of the environment.  we argue that decoupling the environment representation (for example, images or laser scans) from the dynamics of the physical system (that is, the robot and its physical state) may be beneficial:it can increase the flexibility of world models and open the door to further robustification. in this paper, we apply this concept to a strong latent-agent, dreamer. we then showcase the increase of flexibility by transferring the environmental part of the world model from one robot (a boat) to another (a rover), simply by adapting the physical model in the imagination. we also demonstrate the robustness of our method through real-world experiments on a boat.",-1.5583315,3.456281,world,reinforce,65,"id:65 (world)<br><b>Learning Behaviors through Physics-driven Latent Imagination</b><br>kw:Model-Based Reinforcement Learning, Field Robotics, Latent Models"
66,66,121,https://openreview.net/forum?id=JSC4KMlENqF,Demonstration-Guided Reinforcement Learning with Learned Skills,"Karl Pertsch, Youngwoon Lee, Yue Wu, Joseph J Lim",4.0,,,,,,,False,,--,--,,,,,demonstration-guided reinforcement learning with learned skills -- --,-0.21146014,0.48766297,demonstration-guided,reinforce,66,id:66 (demonstration-guided)<br><b>Demonstration-Guided Reinforcement Learning with Learned Skills</b><br>kw:--
67,67,149,https://openreview.net/forum?id=YeJaZBXlhPX,A Constrained Multi-Objective Reinforcement Learning Framework,"Sandy Huang, Abbas Abdolmaleki, Giulia Vezzani, Philemon Brakel, Daniel J Mankowitz, Michael Neunert, Steven Bohez, Nicolas Heess, Martin Riedmiller, raia hadsell",4.0,,,,,,,False,,"constrained RL, multi-objective RL, deep RL","Many real-world problems, especially in robotics, require that reinforcement learning (RL) agents learn policies that not only maximize an environment reward, but also satisfy constraints. We propose a high-level framework for solving such problems, that treats the environment reward and costs as separate objectives, and learns what preference over objectives the policy should optimize for in order to meet the constraints. We call this Learning Preferences and Policies in Parallel (LP3). By making different choices for how to learn the preference and how to optimize for the policy given the preference, we can obtain existing approaches (e.g., Lagrangian relaxation) and derive novel approaches that lead to better performance. One of these is an algorithm that learns a set of constraint-satisfying policies, useful for when we do not know the exact constraint a priori.",,,,,"a constrained multi-objective reinforcement learning framework constrained rl, multi-objective rl, deep rl many real-world problems, especially in robotics, require that reinforcement learning (rl) agents learn policies that not only maximize an environment reward, but also satisfy constraints. we propose a high-level framework for solving such problems, that treats the environment reward and costs as separate objectives, and learns what preference over objectives the policy should optimize for in order to meet the constraints. we call this learning preferences and policies in parallel (lp3). by making different choices for how to learn the preference and how to optimize for the policy given the preference, we can obtain existing approaches (e.g., lagrangian relaxation) and derive novel approaches that lead to better performance. one of these is an algorithm that learns a set of constraint-satisfying policies, useful for when we do not know the exact constraint a priori.",-2.5206532,2.8316205,multi-objective,reinforce,67,"id:67 (multi-objective)<br><b>A Constrained Multi-Objective Reinforcement Learning Framework</b><br>kw:constrained RL, multi-objective RL, deep RL"
68,68,150,https://openreview.net/forum?id=a7DcA81EV4t,GRAC: Self-Guided and Self-Regularized Actor-Critic,"Lin Shao, Yifan You, Mengyuan Yan, Shenli Yuan, Qingyun Sun, Jeannette Bohg",4.0,,,,,,,False,,"Deep Reinforcement Learning, Q-learning","Deep reinforcement learning (DRL) algorithms have successfully been demonstrated on a range of challenging decision making and control tasks. One dominant component of recent deep reinforcement learning algorithms is the target network which mitigates the divergence when learning the Q function. However, target networks can slow down the learning process due to delayed function updates. Our main contribution in this work is a self-regularized TD-learning method to address divergence without requiring a target network. Additionally, we propose a self-guided policy improvement method by combining policy-gradient with zero-order optimization to search for actions associated with higher Q-values in a broad neighborhood. This makes learning more robust to local noise in the Q function approximation and guides the updates of our actor network. Taken together, these components define GRAC, a novel self-guided and self-regularized actor-critic algorithm. We evaluate GRAC on the OpenAI gym tasks, outperforming state of the art on four tasks and achieving competitive results on two environments. We also apply  GRAC to enable a non-anthropomorphic robotic hand to successfully accomplish an in-hand manipulation task in the real world.",,,,,"grac: self-guided and self-regularized actor-critic deep reinforcement learning, q-learning deep reinforcement learning (drl) algorithms have successfully been demonstrated on a range of challenging decision making and control tasks. one dominant component of recent deep reinforcement learning algorithms is the target network which mitigates the divergence when learning the q function. however, target networks can slow down the learning process due to delayed function updates. our main contribution in this work is a self-regularized td-learning method to address divergence without requiring a target network. additionally, we propose a self-guided policy improvement method by combining policy-gradient with zero-order optimization to search for actions associated with higher q-values in a broad neighborhood. this makes learning more robust to local noise in the q function approximation and guides the updates of our actor network. taken together, these components define grac, a novel self-guided and self-regularized actor-critic algorithm. we evaluate grac on the openai gym tasks, outperforming state of the art on four tasks and achieving competitive results on two environments. we also apply  grac to enable a non-anthropomorphic robotic hand to successfully accomplish an in-hand manipulation task in the real world.",-2.4484508,3.8718572,self-regularized,reinforce,68,"id:68 (self-regularized)<br><b>GRAC: Self-Guided and Self-Regularized Actor-Critic</b><br>kw:Deep Reinforcement Learning, Q-learning"
69,69,161,https://openreview.net/forum?id=A9P78VBwYKM,"ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations","Ruohan Gao, Yen-Yu Chang, Shivani Mall, L. Fei-Fei, Jiajun Wu",4.0,,,,,,,False,,"object dataset, multisensory learning, implicit representations","Multisensory object-centric perception, reasoning, and interaction have been a key research topic in recent years. However, the progress in these directions is limited by the small set of objects available---synthetic objects are not realistic enough and are mostly centered around geometry, while real object datasets such as YCB are often practically challenging and unstable to acquire due to international shipping, inventory, and financial cost.  We present ObjectFolder, a dataset of 100 virtualized objects that addresses both challenges with two key innovations. First, ObjectFolder encodes the visual, auditory, and tactile sensory data for all objects, enabling a number of multisensory object recognition tasks, beyond existing datasets that focus purely on object geometry. Second, ObjectFolder employs a uniform, object-centric, and implicit representation for each object’s visual textures, acoustic simulations, and tactile readings, making the dataset flexible to use and easy to share. We demonstrate the usefulness of our dataset as a testbed for multisensory perception and control by evaluating it on a variety of benchmark tasks, including instance recognition, cross-sensory retrieval, 3D reconstruction, and robotic grasping.",,,,,"objectfolder: a dataset of objects with implicit visual, auditory, and tactile representations object dataset, multisensory learning, implicit representations multisensory object-centric perception, reasoning, and interaction have been a key research topic in recent years. however, the progress in these directions is limited by the small set of objects available---synthetic objects are not realistic enough and are mostly centered around geometry, while real object datasets such as ycb are often practically challenging and unstable to acquire due to international shipping, inventory, and financial cost.  we present objectfolder, a dataset of 100 virtualized objects that addresses both challenges with two key innovations. first, objectfolder encodes the visual, auditory, and tactile sensory data for all objects, enabling a number of multisensory object recognition tasks, beyond existing datasets that focus purely on object geometry. second, objectfolder employs a uniform, object-centric, and implicit representation for each object’s visual textures, acoustic simulations, and tactile readings, making the dataset flexible to use and easy to share. we demonstrate the usefulness of our dataset as a testbed for multisensory perception and control by evaluating it on a variety of benchmark tasks, including instance recognition, cross-sensory retrieval, 3d reconstruction, and robotic grasping.",-2.2950516,3.99522,multisensory,no reinforce,69,"id:69 (multisensory)<br><b>ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations</b><br>kw:object dataset, multisensory learning, implicit representations"
70,70,185,https://openreview.net/forum?id=59aUaAbVfMA,Neural Posterior Domain Randomization,"Fabio Muratore, Theo Gruner, Florian Wiese, Boris Belousov, Michael Gienger, Jan Peters",4.0,,,,,,,False,,"sim-to-real, domain randomization, likelihood-free inference","Combining domain randomization and reinforcement learning is a widely used approach to obtain control policies that can bridge the gap between simulation and reality. However, existing methods make limiting assumptions on the form of the domain parameter distribution which prevents them from utilizing the full power of domain randomization. Typically, a restricted family of probability distributions (e.g., normal or uniform) is chosen a priori for every parameter. Furthermore, straightforward approaches based on deep learning require differentiable simulators, which are either not available or can only simulate a limited class of systems. Such rigid assumptions diminish the applicability of domain randomization in robotics. Building upon recently proposed neural likelihood-free inference methods, we introduce Neural Posterior Domain Randomization (NPDR), an algorithm that alternates between learning a policy from a randomized simulator and adapting the posterior distribution over the simulator’s parameters in a Bayesian fashion. Our approach only requires a parameterized simulator, coarse prior ranges, a policy (optionally with optimization routine), and a small set of real-world observations. Most importantly, the domain parameter distribution is not restricted to a specific family, parameters can be correlated, and the simulator does not have to be differentiable. We show that the presented method is able to efficiently adapt the posterior over the domain parameters to closer match the observed dynamics. Moreover, we demonstrate that NPDR can learn transferable policies using fewer real-world rollouts than comparable algorithms.",,,,,"neural posterior domain randomization sim-to-real, domain randomization, likelihood-free inference combining domain randomization and reinforcement learning is a widely used approach to obtain control policies that can bridge the gap between simulation and reality. however, existing methods make limiting assumptions on the form of the domain parameter distribution which prevents them from utilizing the full power of domain randomization. typically, a restricted family of probability distributions (e.g., normal or uniform) is chosen a priori for every parameter. furthermore, straightforward approaches based on deep learning require differentiable simulators, which are either not available or can only simulate a limited class of systems. such rigid assumptions diminish the applicability of domain randomization in robotics. building upon recently proposed neural likelihood-free inference methods, we introduce neural posterior domain randomization (npdr), an algorithm that alternates between learning a policy from a randomized simulator and adapting the posterior distribution over the simulator’s parameters in a bayesian fashion. our approach only requires a parameterized simulator, coarse prior ranges, a policy (optionally with optimization routine), and a small set of real-world observations. most importantly, the domain parameter distribution is not restricted to a specific family, parameters can be correlated, and the simulator does not have to be differentiable. we show that the presented method is able to efficiently adapt the posterior over the domain parameters to closer match the observed dynamics. moreover, we demonstrate that npdr can learn transferable policies using fewer real-world rollouts than comparable algorithms.",-1.8867561,4.485059,domain,reinforce,70,"id:70 (domain)<br><b>Neural Posterior Domain Randomization</b><br>kw:sim-to-real, domain randomization, likelihood-free inference"
71,71,192,https://openreview.net/forum?id=TEQWRlncJVm,Self-supervised Reinforcement Learning with Independently Controllable Subgoals,"Andrii Zadaianchuk, Georg Martius, Fanny Yang",4.0,,,,,,,False,,"object-centric representations, relations, self-supervised reinforcement learning","To successfully tackle challenging manipulation tasks, autonomous agents must learn a diverse set of skills and how to combine them.        Recently, self-supervised agents that set their own abstract goals by exploiting the discovered structure in the environment were shown to perform well on many different tasks.        In particular, some of them were applied to learn basic manipulation skills in compositional multi-object environments.         However, these methods learn skills without taking the dependencies between objects into account. Thus, the learned skills are difficult to combine in realistic environments.        We propose a novel self-supervised agent that estimates relations between environment components and uses them to independently control different parts of the environment state. In addition, the estimated relations between objects can be used to decompose a complex goal into a compatible sequence of subgoals.        We show that, by using this framework, an agent can efficiently and automatically learn manipulation tasks in multi-object environments with different relations between objects.",,,,,"self-supervised reinforcement learning with independently controllable subgoals object-centric representations, relations, self-supervised reinforcement learning to successfully tackle challenging manipulation tasks, autonomous agents must learn a diverse set of skills and how to combine them.        recently, self-supervised agents that set their own abstract goals by exploiting the discovered structure in the environment were shown to perform well on many different tasks.        in particular, some of them were applied to learn basic manipulation skills in compositional multi-object environments.         however, these methods learn skills without taking the dependencies between objects into account. thus, the learned skills are difficult to combine in realistic environments.        we propose a novel self-supervised agent that estimates relations between environment components and uses them to independently control different parts of the environment state. in addition, the estimated relations between objects can be used to decompose a complex goal into a compatible sequence of subgoals.        we show that, by using this framework, an agent can efficiently and automatically learn manipulation tasks in multi-object environments with different relations between objects.",-1.3254862,3.7140539,relations,reinforce,71,"id:71 (relations)<br><b>Self-supervised Reinforcement Learning with Independently Controllable Subgoals</b><br>kw:object-centric representations, relations, self-supervised reinforcement learning"
72,72,203,https://openreview.net/forum?id=xwEaXgFa0MR,AW-Opt: Learning Robotic Skills with Imitation andReinforcement at Scale,"Yao Lu, Karol Hausman, Yevgen Chebotar, Mengyuan Yan, Eric Jang, Alexander Herzog, Ted Xiao, Alex Irpan, Mohi Khansari, Dmitry Kalashnikov, Sergey Levine",4.0,,,,,,,False,,"reinforcement learning, imitation learning","Robotic skills can be learned via imitation learning (IL) using user-provided demonstrations, or via reinforcement learning (RL) using large amounts of autonomously collected experience. Both methods have complementary strengths and weaknesses: RL can reach a high level of performance, but requires exploration, which can be very time consuming and unsafe; IL does not require exploration, but only learns skills that are as good as the provided demonstrations. Can a single method combine the strengths of both approaches? A number of prior methods have aimed to address this question, proposing a variety of techniques that integrate elements of IL and RL. However, scaling up such methods to complex robotic skills that integrate diverse offline data and generalize meaningfully to real-world scenarios still presents a major challenge. In this paper, our aim is to test the scalability of prior IL + RL algorithms and devise a system based on detailed empirical experimentation that combines existing components in the most effective and scalable way.         To that end, we present a series of experiments aimed at understanding the implications of each design decision, so as to develop a combined approach that can utilize demonstrations and heterogeneous prior data to attain the best performance on a range of real-world and realistic simulated robotic problems. Our complete method, which we call AW-Opt, combines elements of advantage-weighted regression and QT-Opt, providing a unified approach for integrating demonstrations and offline data for robotic manipulation.",,,,,"aw-opt: learning robotic skills with imitation andreinforcement at scale reinforcement learning, imitation learning robotic skills can be learned via imitation learning (il) using user-provided demonstrations, or via reinforcement learning (rl) using large amounts of autonomously collected experience. both methods have complementary strengths and weaknesses: rl can reach a high level of performance, but requires exploration, which can be very time consuming and unsafe; il does not require exploration, but only learns skills that are as good as the provided demonstrations. can a single method combine the strengths of both approaches? a number of prior methods have aimed to address this question, proposing a variety of techniques that integrate elements of il and rl. however, scaling up such methods to complex robotic skills that integrate diverse offline data and generalize meaningfully to real-world scenarios still presents a major challenge. in this paper, our aim is to test the scalability of prior il + rl algorithms and devise a system based on detailed empirical experimentation that combines existing components in the most effective and scalable way.         to that end, we present a series of experiments aimed at understanding the implications of each design decision, so as to develop a combined approach that can utilize demonstrations and heterogeneous prior data to attain the best performance on a range of real-world and realistic simulated robotic problems. our complete method, which we call aw-opt, combines elements of advantage-weighted regression and qt-opt, providing a unified approach for integrating demonstrations and offline data for robotic manipulation.",-2.7880578,3.7246222,il,reinforce,72,"id:72 (il)<br><b>AW-Opt: Learning Robotic Skills with Imitation andReinforcement at Scale</b><br>kw:reinforcement learning, imitation learning"
73,73,251,https://openreview.net/forum?id=n6xYib0irVR,Influencing Towards Stable Multi-Agent Interactions,"Woodrow Zhouyuan Wang, Andy Shih, Annie Xie, Dorsa Sadigh",4.0,,,,,,,False,,"multi-agent interactions, human-robot interaction, non-stationarity","Learning in multi-agent environments is difficult due to the non-stationarity introduced by an opponent's or partner's changing behaviors. Instead of reactively adapting to the other agent's (opponent or partner) behavior, we propose an algorithm to proactively influence the other agent's strategy to stabilize -- which can restrain the non-stationarity caused by the other agent. We learn a low-dimensional latent representation of the other agent's strategy and the dynamics of how the latent strategy evolves with respect to our robot's behavior. With this learned dynamics model, we can define an unsupervised stability reward to train our robot to deliberately influence the other agent to stabilize towards a single strategy. We demonstrate the effectiveness of stabilizing in improving efficiency of maximizing the task reward in a variety of simulated environments, including autonomous driving, emergent communication, and robotic manipulation.",,,,,"influencing towards stable multi-agent interactions multi-agent interactions, human-robot interaction, non-stationarity learning in multi-agent environments is difficult due to the non-stationarity introduced by an opponent's or partner's changing behaviors. instead of reactively adapting to the other agent's (opponent or partner) behavior, we propose an algorithm to proactively influence the other agent's strategy to stabilize -- which can restrain the non-stationarity caused by the other agent. we learn a low-dimensional latent representation of the other agent's strategy and the dynamics of how the latent strategy evolves with respect to our robot's behavior. with this learned dynamics model, we can define an unsupervised stability reward to train our robot to deliberately influence the other agent to stabilize towards a single strategy. we demonstrate the effectiveness of stabilizing in improving efficiency of maximizing the task reward in a variety of simulated environments, including autonomous driving, emergent communication, and robotic manipulation.",-1.6136312,3.1434653,non-stationarity,no reinforce,73,"id:73 (non-stationarity)<br><b>Influencing Towards Stable Multi-Agent Interactions</b><br>kw:multi-agent interactions, human-robot interaction, non-stationarity"
74,74,314,https://openreview.net/forum?id=-QJ__aPUTN2,Guiding Multi-Step Rearrangement Tasks with Natural Language Instructions,"Elias Stengel-Eskin, Andrew Hundt, Zhuohong He, Aditya Murali, Nakul Gopalan, Matthew Gombolay, Gregory D. Hager",4.0,,,,,,,False,,"more often than a UNet-based baseline, and learns to localize actions correctly while creating a mapping of symbols to perceptual input that supports compositional reasoning. We provide a valuable resource for 3D manipulation instruction following research by porting an existing 3D block dataset with crowdsourced language to a simulated environment. Our method's",absolute improvement in identifying the correct block on the ported dataset demonstrates its ability to handle syntactic and lexical variation.,,,,,"guiding multi-step rearrangement tasks with natural language instructions more often than a unet-based baseline, and learns to localize actions correctly while creating a mapping of symbols to perceptual input that supports compositional reasoning. we provide a valuable resource for 3d manipulation instruction following research by porting an existing 3d block dataset with crowdsourced language to a simulated environment. our method's absolute improvement in identifying the correct block on the ported dataset demonstrates its ability to handle syntactic and lexical variation.",-1.709753,3.465036,block,no reinforce,74,"id:74 (block)<br><b>Guiding Multi-Step Rearrangement Tasks with Natural Language Instructions</b><br>kw:more often than a UNet-based baseline, and learns to localize actions correctly while creating a mapping of symbols to perceptual input that supports compositional reasoning. We provide a valuable resource for 3D manipulation instruction following research by porting an existing 3D block dataset with crowdsourced language to a simulated environment. Our method's"
75,75,328,https://openreview.net/forum?id=RO4DM85Z4P7,XIRL: Cross-embodiment Inverse Reinforcement Learning,"Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, Debidatta Dwibedi",4.0,,,,,,,False,,--,--,,,,,xirl: cross-embodiment inverse reinforcement learning -- --,-0.41617012,0.8995689,xirl:,reinforce,75,id:75 (xirl:)<br><b>XIRL: Cross-embodiment Inverse Reinforcement Learning</b><br>kw:--
76,76,357,https://openreview.net/forum?id=udFuJTvlhsJ,Learning Reward Functions from Scale Feedback,"Nils Wilde, Erdem Biyik, Dorsa Sadigh, Stephen L. Smith",4.0,,,,,,,False,,"HRI, reward learning, learning from choice, active learning","Today's robots are increasingly interacting with people and need to efficiently learn inexperienced user's preferences. A common framework is to iteratively query the user about which of two presented robot trajectories they prefer. While this minimizes the users effort, a strict choice does not yield any information on how much one trajectory is preferred. We propose scale feedback, where the user utilizes a slider to give more nuanced information. We introduce a probabilistic model on how users would provide feedback and derive a learning framework for the robot. We demonstrate the performance benefit of slider feedback in simulations, and validate our approach in two user studies suggesting that scale feedback enables more effective learning in practice.",,,,,"learning reward functions from scale feedback hri, reward learning, learning from choice, active learning today's robots are increasingly interacting with people and need to efficiently learn inexperienced user's preferences. a common framework is to iteratively query the user about which of two presented robot trajectories they prefer. while this minimizes the users effort, a strict choice does not yield any information on how much one trajectory is preferred. we propose scale feedback, where the user utilizes a slider to give more nuanced information. we introduce a probabilistic model on how users would provide feedback and derive a learning framework for the robot. we demonstrate the performance benefit of slider feedback in simulations, and validate our approach in two user studies suggesting that scale feedback enables more effective learning in practice.",-1.8554218,3.674877,feedback,no reinforce,76,"id:76 (feedback)<br><b>Learning Reward Functions from Scale Feedback</b><br>kw:HRI, reward learning, learning from choice, active learning"
77,77,359,https://openreview.net/forum?id=zOjU2vZzhCk,NeRF-GTO: Using a Neural Radiance Field to Grasp Transparent Objects,"Jeffrey Ichnowski, Yahav Avigal, Justin Kerr, Ken Goldberg",4.0,,,,,,,False,,"neural radiance fields, grasp planning, transparent objects","The ability to grasp and manipulate transparent objects is a major challenge for robots. Existing depth cameras have difficulty detecting, localizing, and inferring the geometry of such objects. We propose using neural radiance fields (NeRF) to detect, localize, and infer the geometry of transparent objects with sufficient accuracy to find and grasp them securely. We leverage NeRF's view-independent learned density, place lights to increase specular reflections, and perform a transparency-aware depth-rendering that we feed into the Dex-Net grasp planner. We show how additional lights create specular reflections that improve the quality of the depth map, and test a setup for a robot workcell equipped with an array of cameras to perform transparent object manipulation. We also create synthetic and real datasets of transparent objects in real-world settings, including singulated objects, cluttered tables, and the top rack of a dishwasher. In each setting we show that NeRF and Dex-Net are able to reliably compute robust grasps on transparent objects, achieving 90% and 100% grasp success rates in physical experiments on an ABB YuMi, on objects where baseline methods fail.        See supplementary material and \url{https://sites.google.com/view/nerf-gto/home} for code, video, and datasets.",,,,,"nerf-gto: using a neural radiance field to grasp transparent objects neural radiance fields, grasp planning, transparent objects the ability to grasp and manipulate transparent objects is a major challenge for robots. existing depth cameras have difficulty detecting, localizing, and inferring the geometry of such objects. we propose using neural radiance fields (nerf) to detect, localize, and infer the geometry of transparent objects with sufficient accuracy to find and grasp them securely. we leverage nerf's view-independent learned density, place lights to increase specular reflections, and perform a transparency-aware depth-rendering that we feed into the dex-net grasp planner. we show how additional lights create specular reflections that improve the quality of the depth map, and test a setup for a robot workcell equipped with an array of cameras to perform transparent object manipulation. we also create synthetic and real datasets of transparent objects in real-world settings, including singulated objects, cluttered tables, and the top rack of a dishwasher. in each setting we show that nerf and dex-net are able to reliably compute robust grasps on transparent objects, achieving 90% and 100% grasp success rates in physical experiments on an abb yumi, on objects where baseline methods fail.        see supplementary material and \url{https://sites.google.com/view/nerf-gto/home} for code, video, and datasets.",-1.623673,4.6524544,transparent,no reinforce,77,"id:77 (transparent)<br><b>NeRF-GTO: Using a Neural Radiance Field to Grasp Transparent Objects</b><br>kw:neural radiance fields, grasp planning, transparent objects"
78,78,402,https://openreview.net/forum?id=0WDtVJVwBcf,Learning Backchanneling Behaviors for a Social Robot via Data Augmentation from Human-Human Conversations,"Michael Murray, Nick Walker, Amal Nanavati, Patricia Alves-Oliveira, Nikita Filippov, Allison Sauppe, Bilge Mutlu, Maya Cakmak",4.0,,,,,,,False,,"Visual Learning, Soft Robot, Force Control","This paper explores the feasibility of learning robot force control and interaction using soft metamaterial and machine vision. We start by investigating the differential stiffness of a hollow, cone-shaped, 3D metamaterial made from soft rubber, achieving a large stiffness ratio between the axial and radial directions that leads to an adaptive form response in omni-directions during physical interaction. Then, using image data collected from its internal deformation during various interactions, we explored two similar designs but different learning strategies to estimate force control and interactions on the end-effector of a UR10 e-series robot arm. One is to directly learn the force and torque response from raw images of the metamaterial's internal deformation. The other is to indirectly estimate the 6D force and torque using a neural network by visually tracking the 6D pose of a marker fixed inside the 3D metamaterial. Finally, we integrated the two proposed systems and achieved similar force feedback and control interactions in simple tasks such as circle following and text writing. Our results show that the learning method holds the potential to support the concept of soft robot force control, providing an intuitive interface at a low cost for robotic systems, generating comparable and capable performances against classical force and torque sensors.",,,,,"learning backchanneling behaviors for a social robot via data augmentation from human-human conversations visual learning, soft robot, force control this paper explores the feasibility of learning robot force control and interaction using soft metamaterial and machine vision. we start by investigating the differential stiffness of a hollow, cone-shaped, 3d metamaterial made from soft rubber, achieving a large stiffness ratio between the axial and radial directions that leads to an adaptive form response in omni-directions during physical interaction. then, using image data collected from its internal deformation during various interactions, we explored two similar designs but different learning strategies to estimate force control and interactions on the end-effector of a ur10 e-series robot arm. one is to directly learn the force and torque response from raw images of the metamaterial's internal deformation. the other is to indirectly estimate the 6d force and torque using a neural network by visually tracking the 6d pose of a marker fixed inside the 3d metamaterial. finally, we integrated the two proposed systems and achieved similar force feedback and control interactions in simple tasks such as circle following and text writing. our results show that the learning method holds the potential to support the concept of soft robot force control, providing an intuitive interface at a low cost for robotic systems, generating comparable and capable performances against classical force and torque sensors.",-2.274434,4.4891715,force,no reinforce,78,"id:78 (force)<br><b>Learning Backchanneling Behaviors for a Social Robot via Data Augmentation from Human-Human Conversations</b><br>kw:Visual Learning, Soft Robot, Force Control"
79,79,24,https://openreview.net/forum?id=MWtinPDqfZg,Learning Feasibility to Imitate Demonstrators with Different Dynamics,"Zhangjie Cao, Yilun Hao, Mengxi Li, Dorsa Sadigh",5.0,,,,,,,False,,"Imitation Learning, Learning from Agents with Different Dynamics","The goal of learning from demonstration is to learn a policy for an agent (imitator) by mimicking the behavior in the demonstrations. Prior works on learning from demonstration assume that the demonstrations are collected by a demonstrator that has the same dynamics as the imitator. However, in many real-world applications, this assumption is limiting --- to improve the problem of lack of data in robotics, we would like to be able to leverage demonstrations collected from agents with different dynamics. However, this can be challenging as the demonstrations might not even be feasible for the imitator.        Our insight is that we can learn a feasibility metric that captures the likelihood of a demonstration being feasible by the imitator. We develop a feasibility MDP (f-MDP) and derive the feasibility score by learning an optimal policy in the f-MDP. Our proposed feasibility measure enables the imitator to learn from more informative demonstrations, and disregard the far from feasible demonstrations. Our experiments on four environments in simulation and on a real robot show that the final policy of our approach achieves a higher expected return than prior works.",,,,,"learning feasibility to imitate demonstrators with different dynamics imitation learning, learning from agents with different dynamics the goal of learning from demonstration is to learn a policy for an agent (imitator) by mimicking the behavior in the demonstrations. prior works on learning from demonstration assume that the demonstrations are collected by a demonstrator that has the same dynamics as the imitator. however, in many real-world applications, this assumption is limiting --- to improve the problem of lack of data in robotics, we would like to be able to leverage demonstrations collected from agents with different dynamics. however, this can be challenging as the demonstrations might not even be feasible for the imitator.        our insight is that we can learn a feasibility metric that captures the likelihood of a demonstration being feasible by the imitator. we develop a feasibility mdp (f-mdp) and derive the feasibility score by learning an optimal policy in the f-mdp. our proposed feasibility measure enables the imitator to learn from more informative demonstrations, and disregard the far from feasible demonstrations. our experiments on four environments in simulation and on a real robot show that the final policy of our approach achieves a higher expected return than prior works.",-2.255645,3.4009087,feasibility,no reinforce,79,"id:79 (feasibility)<br><b>Learning Feasibility to Imitate Demonstrators with Different Dynamics</b><br>kw:Imitation Learning, Learning from Agents with Different Dynamics"
80,80,27,https://openreview.net/forum?id=EougVeukEH9,O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance Learning,"Kaichun Mo, Yuzhe Qin, Fanbo Xiang, Hao Su, Leonidas Guibas",5.0,,,,,,,False,,"Object-object Affordance, Vision for Robotics, Large-scale Learning","Contrary to the vast literature in modeling, perceiving, and understanding agent-object (e.g., human-object, hand-object, robot-object) interaction in computer vision and robotics, very few past works have studied the task of object-object interaction, which also plays an important role in robotic manipulation and planning tasks. There is a rich space of object-object interaction scenarios in our daily life, such as placing an object on a messy tabletop, fitting an object inside a drawer, pushing an object using a tool, etc. In this paper, we propose a unified affordance learning framework to learn object-object interaction for various tasks. By constructing four object-object interaction task environments using physical simulation (SAPIEN) and thousands of ShapeNet models with rich geometric diversity, we are able to conduct large-scale object-object affordance learning without the need for human annotations or demonstrations. At the core of technical contribution, we propose an object-kernel point convolution network to reason about detailed interaction between two objects. Experiments on large-scale synthetic data and real-world data prove the effectiveness of the proposed approach.",,,,,"o2o-afford: annotation-free large-scale object-object affordance learning object-object affordance, vision for robotics, large-scale learning contrary to the vast literature in modeling, perceiving, and understanding agent-object (e.g., human-object, hand-object, robot-object) interaction in computer vision and robotics, very few past works have studied the task of object-object interaction, which also plays an important role in robotic manipulation and planning tasks. there is a rich space of object-object interaction scenarios in our daily life, such as placing an object on a messy tabletop, fitting an object inside a drawer, pushing an object using a tool, etc. in this paper, we propose a unified affordance learning framework to learn object-object interaction for various tasks. by constructing four object-object interaction task environments using physical simulation (sapien) and thousands of shapenet models with rich geometric diversity, we are able to conduct large-scale object-object affordance learning without the need for human annotations or demonstrations. at the core of technical contribution, we propose an object-kernel point convolution network to reason about detailed interaction between two objects. experiments on large-scale synthetic data and real-world data prove the effectiveness of the proposed approach.",-2.0276582,4.284249,object-object,no reinforce,80,"id:80 (object-object)<br><b>O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance Learning</b><br>kw:Object-object Affordance, Vision for Robotics, Large-scale Learning"
81,81,96,https://openreview.net/forum?id=WIE9t_UwOpM,Enhancing Consistent Ground Maneuverability by Robot Adaptation to Complex Off-Road Terrains,"Sriram Siva, Maggie Wigness, John Rogers, Hao Zhang",5.0,,,,,,,False,,"Robot Learning, Off-road Navigation, Terrain Adaptation","Terrain adaptation is a critical ability for a ground robot to effectively traverse unstructured off-road terrain in real-world field environments such as forests. However, the expected or planned maneuvering behaviors cannot always be accurately executed due to setbacks such as reduced tire pressure. This inconsistency negatively affects the robot's ground maneuverability and can cause slower traversal time or errors in localization.  To address this shortcoming, we propose a novel method for consistent behavior generation that enables a ground robot's actual behaviors to more accurately match expected behaviors while adapting to a variety of complex off-road terrains. Our method learns offset behaviors in a self-supervised fashion to compensate for the inconsistency between the actual and expected behaviors without requiring the explicit modeling of various setbacks. To evaluate the method, we perform extensive experiments using a physical ground robot over diverse complex off-road terrain in real-world field environments.  Experimental results show that our method enables a robot to improve its ground maneuverability on complex unstructured off-road terrain with more navigational behavior consistency, and outperforms previous and baseline methods, particularly so on challenging terrain such as that which is seen in forests.",,,,,"enhancing consistent ground maneuverability by robot adaptation to complex off-road terrains robot learning, off-road navigation, terrain adaptation terrain adaptation is a critical ability for a ground robot to effectively traverse unstructured off-road terrain in real-world field environments such as forests. however, the expected or planned maneuvering behaviors cannot always be accurately executed due to setbacks such as reduced tire pressure. this inconsistency negatively affects the robot's ground maneuverability and can cause slower traversal time or errors in localization.  to address this shortcoming, we propose a novel method for consistent behavior generation that enables a ground robot's actual behaviors to more accurately match expected behaviors while adapting to a variety of complex off-road terrains. our method learns offset behaviors in a self-supervised fashion to compensate for the inconsistency between the actual and expected behaviors without requiring the explicit modeling of various setbacks. to evaluate the method, we perform extensive experiments using a physical ground robot over diverse complex off-road terrain in real-world field environments.  experimental results show that our method enables a robot to improve its ground maneuverability on complex unstructured off-road terrain with more navigational behavior consistency, and outperforms previous and baseline methods, particularly so on challenging terrain such as that which is seen in forests.",-0.75803214,4.5576797,off-road,no reinforce,81,"id:81 (off-road)<br><b>Enhancing Consistent Ground Maneuverability by Robot Adaptation to Complex Off-Road Terrains</b><br>kw:Robot Learning, Off-road Navigation, Terrain Adaptation"
82,82,109,https://openreview.net/forum?id=LGaHnyg81sQ,Co-GAIL: Learning Diverse Strategies for Human-Robot Collaboration,"Chen Wang, Claudia P?rez-D'Arpino, Danfei Xu, L. Fei-Fei, Karen Liu, Silvio Savarese",5.0,,,,,,,False,,"Learning for Human-Robot Collaboration, Imitation Learning","We present a method for learning human-robot collaboration policy from human-human collaboration demonstrations. An effective robot assistant must learn to handle diverse human behaviors shown in the demonstrations and be robust when the humans adjust their strategies during online task execution. Our method co-optimizes a human policy and a robot policy in an interactive learning process: the human policy learns to generate diverse and plausible collaborative behaviors from demonstrations while the robot policy learns to assist by estimating the unobserved latent strategy of its human collaborator. Across a 2D strategy game, a human-robot handover task, and a multi-step collaborative manipulation task, our method outperforms the alternatives in both simulated evaluations and when executing the tasks with a real human operator in-the-loop.",,,,,"co-gail: learning diverse strategies for human-robot collaboration learning for human-robot collaboration, imitation learning we present a method for learning human-robot collaboration policy from human-human collaboration demonstrations. an effective robot assistant must learn to handle diverse human behaviors shown in the demonstrations and be robust when the humans adjust their strategies during online task execution. our method co-optimizes a human policy and a robot policy in an interactive learning process: the human policy learns to generate diverse and plausible collaborative behaviors from demonstrations while the robot policy learns to assist by estimating the unobserved latent strategy of its human collaborator. across a 2d strategy game, a human-robot handover task, and a multi-step collaborative manipulation task, our method outperforms the alternatives in both simulated evaluations and when executing the tasks with a real human operator in-the-loop.",-2.5223768,2.803262,collaboration,no reinforce,82,"id:82 (collaboration)<br><b>Co-GAIL: Learning Diverse Strategies for Human-Robot Collaboration</b><br>kw:Learning for Human-Robot Collaboration, Imitation Learning"
83,83,117,https://openreview.net/forum?id=oqZrUx-PRqb,Look Before You Leap: Safe Model-Based Reinforcement Learning with Human Intervention,"Yunkun Xu, Zhenyu Liu, Guifang Duan, Jiangcheng Zhu, Xiaolong Bai, Jianrong Tan",5.0,,,,,,,False,,"Safety RL, Model-based RL, Model Predict Control","Safety has become one of the main challenges of applying deep reinforcement learning to real world systems. Currently, the incorporation of external knowledge such as human oversight is the only means to prevent the agent from visiting the catastrophic state. In this paper, we propose MBHI, a novel framework for safe model-based reinforcement learning, which ensures safety in the state-level and can effectively avoid both local and non-local catastrophes. An ensemble of supervised learners are trained in MBHI to imitate human blocking decisions. Similar to human decision-making process, MBHI will roll out an imagined trajectory in the dynamics model before executing actions to the environment, and estimate its safety. When the imagination encounters a catastrophe, MBHI will block the current action and use an efficient MPC method to output a safety policy. We evaluate our method on several safety tasks, and the results show that MBHI achieved better performance in terms of sample efficiency and number of catastrophes compared to the baselines.",,,,,"look before you leap: safe model-based reinforcement learning with human intervention safety rl, model-based rl, model predict control safety has become one of the main challenges of applying deep reinforcement learning to real world systems. currently, the incorporation of external knowledge such as human oversight is the only means to prevent the agent from visiting the catastrophic state. in this paper, we propose mbhi, a novel framework for safe model-based reinforcement learning, which ensures safety in the state-level and can effectively avoid both local and non-local catastrophes. an ensemble of supervised learners are trained in mbhi to imitate human blocking decisions. similar to human decision-making process, mbhi will roll out an imagined trajectory in the dynamics model before executing actions to the environment, and estimate its safety. when the imagination encounters a catastrophe, mbhi will block the current action and use an efficient mpc method to output a safety policy. we evaluate our method on several safety tasks, and the results show that mbhi achieved better performance in terms of sample efficiency and number of catastrophes compared to the baselines.",-2.9755921,3.456288,mbhi,reinforce,83,"id:83 (mbhi)<br><b>Look Before You Leap: Safe Model-Based Reinforcement Learning with Human Intervention</b><br>kw:Safety RL, Model-based RL, Model Predict Control"
84,84,137,https://openreview.net/forum?id=AlJXhEI6J5W,Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble,"Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, Jinwoo Shin",5.0,,,,,,,False,,"Deep Reinforcement Learning, Offline RL, Fine-tuning","Recent advance in deep offline reinforcement learning (RL) has made it possible to train strong robotic agents from offline datasets. However, depending on the quality of the trained agents and the application being considered, it is often desirable to fine-tune such agents via further online interactions. In this paper, we observe that state-action distribution shift may lead to severe bootstrap error during fine-tuning, which destroys the good initial policy obtained via offline RL. To address this issue, we first propose a balanced replay scheme that prioritizes samples encountered online while also encouraging the use of near-on-policy samples from the offline dataset. Furthermore, we leverage multiple Q-functions trained pessimistically offline, thereby preventing overoptimism concerning unfamiliar actions at novel states during the initial training phase. We show that the proposed method improves sample-efficiency and final performance of the fine-tuned robotic agents on various locomotion and manipulation tasks.",,,,,"offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble deep reinforcement learning, offline rl, fine-tuning recent advance in deep offline reinforcement learning (rl) has made it possible to train strong robotic agents from offline datasets. however, depending on the quality of the trained agents and the application being considered, it is often desirable to fine-tune such agents via further online interactions. in this paper, we observe that state-action distribution shift may lead to severe bootstrap error during fine-tuning, which destroys the good initial policy obtained via offline rl. to address this issue, we first propose a balanced replay scheme that prioritizes samples encountered online while also encouraging the use of near-on-policy samples from the offline dataset. furthermore, we leverage multiple q-functions trained pessimistically offline, thereby preventing overoptimism concerning unfamiliar actions at novel states during the initial training phase. we show that the proposed method improves sample-efficiency and final performance of the fine-tuned robotic agents on various locomotion and manipulation tasks.",-3.2424536,3.6832979,offline,reinforce,84,"id:84 (offline)<br><b>Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble</b><br>kw:Deep Reinforcement Learning, Offline RL, Fine-tuning"
85,85,157,https://openreview.net/forum?id=xQ8rr3-zpiH,Just Label What You Need: Fine-Grained Active Selection for P&P through Partially Labeled Scenes,"Sean Segal, Nishanth Kumar, Sergio Casas, Wenyuan Zeng, Mengye Ren, Jingkang Wang, Raquel Urtasun",5.0,,,,,,,False,,"Self-Driving, Active Learning","Self-driving vehicles must perceive and predict the future positions of nearby actors to avoid collisions and drive safely. A deep learning module is often responsible for this task, requiring large-scale, high-quality training datasets. Due to high labeling costs, active learning approaches are an appealing solution to maximizing model performance for a given labeling budget. However, despite its appeal, there has been little scientific analysis of active learning approaches for the perception and prediction (P&P) problem. In this work, we study active learning techniques for P&P and find that the traditional active learning formulation is ill-suited. We thus introduce generalizations that ensure that our approach is both cost-aware and allows for fine-grained selection of examples through partially labeled scenes. Extensive experiments on a real-world dataset suggest significant improvements across perception, prediction, and downstream planning tasks.",,,,,"just label what you need: fine-grained active selection for p&p through partially labeled scenes self-driving, active learning self-driving vehicles must perceive and predict the future positions of nearby actors to avoid collisions and drive safely. a deep learning module is often responsible for this task, requiring large-scale, high-quality training datasets. due to high labeling costs, active learning approaches are an appealing solution to maximizing model performance for a given labeling budget. however, despite its appeal, there has been little scientific analysis of active learning approaches for the perception and prediction (p&p) problem. in this work, we study active learning techniques for p&p and find that the traditional active learning formulation is ill-suited. we thus introduce generalizations that ensure that our approach is both cost-aware and allows for fine-grained selection of examples through partially labeled scenes. extensive experiments on a real-world dataset suggest significant improvements across perception, prediction, and downstream planning tasks.",-2.342533,4.0850325,active,no reinforce,85,"id:85 (active)<br><b>Just Label What You Need: Fine-Grained Active Selection for P&P through Partially Labeled Scenes</b><br>kw:Self-Driving, Active Learning"
86,86,158,https://openreview.net/forum?id=1GNV9SW95eJ,Learning Off-Policy with Online Planning,"Harshit Sikchi, Wenxuan Zhou, David Held",5.0,,,,,,,False,,"Reinforcement Learning, Trajectory Optimization, Safety","Reinforcement learning (RL) in low-data and risk-sensitive domains requires performant and flexible deployment policies that can readily incorporate constraints during deployment. One such class of policies are the semi-parametric H-step lookahead policies, which select actions using trajectory optimization over a dynamics model for a fixed horizon with a terminal value function. In this work, we investigate a novel instantiation of H-step lookahead with a learned model and a terminal value function learned by a model-free off-policy algorithm, named Learning Off-Policy with Online Planning (LOOP). We provide a theoretical analysis of this method, suggesting a tradeoff between model errors and value function errors, and empirically demonstrate this tradeoff to be beneficial in deep reinforcement learning. Furthermore, we identify the ""Actor Divergence"" issue in this framework and propose Actor Regularized Control (ARC), a modified trajectory optimization procedure. We evaluate our method on a set of robotic tasks for Offline and Online RL and demonstrate improved performance. We also show the flexibility of LOOP to incorporate safety constraints during deployment with a set of navigation environments. We demonstrate that LOOP is a desirable framework for robotics applications based on its strong performance in various important RL settings.",,,,,"learning off-policy with online planning reinforcement learning, trajectory optimization, safety reinforcement learning (rl) in low-data and risk-sensitive domains requires performant and flexible deployment policies that can readily incorporate constraints during deployment. one such class of policies are the semi-parametric h-step lookahead policies, which select actions using trajectory optimization over a dynamics model for a fixed horizon with a terminal value function. in this work, we investigate a novel instantiation of h-step lookahead with a learned model and a terminal value function learned by a model-free off-policy algorithm, named learning off-policy with online planning (loop). we provide a theoretical analysis of this method, suggesting a tradeoff between model errors and value function errors, and empirically demonstrate this tradeoff to be beneficial in deep reinforcement learning. furthermore, we identify the ""actor divergence"" issue in this framework and propose actor regularized control (arc), a modified trajectory optimization procedure. we evaluate our method on a set of robotic tasks for offline and online rl and demonstrate improved performance. we also show the flexibility of loop to incorporate safety constraints during deployment with a set of navigation environments. we demonstrate that loop is a desirable framework for robotics applications based on its strong performance in various important rl settings.",-2.814808,3.2286525,off-policy,reinforce,86,"id:86 (off-policy)<br><b>Learning Off-Policy with Online Planning</b><br>kw:Reinforcement Learning, Trajectory Optimization, Safety"
87,87,168,https://openreview.net/forum?id=vrU4d2yjjUn,Assisted Robust Reward Design,"Jerry Zhi-Yang He, Anca Dragan",5.0,,,,,,,False,,"Reward Design, Safety, Human-in-the-loop, Active Learning","Real-world robotic tasks require complex reward functions. When we define the problem the robot needs to solve, we pretend that a designer specifies this complex reward exactly, and it is set in stone from then on. In practice, however, reward design is an iterative process: the designer chooses a reward, eventually encounters an ``````''edge-case'' environment where the reward incentivizes the wrong behavior, revises the reward, and repeats. What would it mean to rethink robotics problems to formally account for this iterative nature of reward design? We propose that the robot not take the specified reward for granted, but rather have uncertainty about it, and account for the future design iterations as future evidence. We contribute an Assisted Reward Design method that speeds up the design process by anticipating and influencing this future evidence: rather than letting the designer eventually encounter failure cases and revise the reward then, the method actively exposes the designer to such environments during the development phase. We test this method in an autonomous driving task and find that it more quickly improves the car's behavior in held-out environments by proposing environments that are ``''edge cases'' for the current reward.",,,,,"assisted robust reward design reward design, safety, human-in-the-loop, active learning real-world robotic tasks require complex reward functions. when we define the problem the robot needs to solve, we pretend that a designer specifies this complex reward exactly, and it is set in stone from then on. in practice, however, reward design is an iterative process: the designer chooses a reward, eventually encounters an ``````''edge-case'' environment where the reward incentivizes the wrong behavior, revises the reward, and repeats. what would it mean to rethink robotics problems to formally account for this iterative nature of reward design? we propose that the robot not take the specified reward for granted, but rather have uncertainty about it, and account for the future design iterations as future evidence. we contribute an assisted reward design method that speeds up the design process by anticipating and influencing this future evidence: rather than letting the designer eventually encounter failure cases and revise the reward then, the method actively exposes the designer to such environments during the development phase. we test this method in an autonomous driving task and find that it more quickly improves the car's behavior in held-out environments by proposing environments that are ``''edge cases'' for the current reward.",-2.0308373,4.6430235,reward,no reinforce,87,"id:87 (reward)<br><b>Assisted Robust Reward Design</b><br>kw:Reward Design, Safety, Human-in-the-loop, Active Learning"
88,88,172,https://openreview.net/forum?id=UD4TAsdE-JM,LEO: Learning Energy-based Models in Factor Graph Optimization,"Paloma Sodhi, Eric Dexheimer, Mustafa Mukadam, Stuart Anderson, Michael Kaess",5.0,,,,,,,False,,"factor graphs, energy-based models, state estimation","We address the problem of learning observation models end-to-end for estimation. Robots operating in partially observable environments must infer latent states from multiple sensory inputs using observation models that capture the joint distribution between latent states and observations. This inference problem can be formulated as an objective over a graph that optimizes for the most likely sequence of states using all previous measurements. Prior work uses observation models that are either known a-priori or trained on surrogate losses independent of the graph optimizer. In this paper, we propose a method to directly optimize end-to-end tracking performance by learning observation models with the graph optimizer in the loop. This direct approach may appear, however, to require the inference algorithm to be fully differentiable, which many state-of-the-art graph optimizers are not. Our key insight is to instead formulate the problem as that of energy-based learning. We propose a novel approach, LEO, for learning observation models end-to-end with non-differentiable graph optimizers. LEO alternates between sampling trajectories from the graph posterior and updating the model to match these samples to ground truth trajectories. We propose a way to generate such samples efficiently using incremental Gauss-Newton solvers. We compare LEO against baselines on datasets drawn from two distinct tasks: navigation and real-world planar pushing. We show that LEO is able to learn complex observation models with lower errors and fewer samples.",,,,,"leo: learning energy-based models in factor graph optimization factor graphs, energy-based models, state estimation we address the problem of learning observation models end-to-end for estimation. robots operating in partially observable environments must infer latent states from multiple sensory inputs using observation models that capture the joint distribution between latent states and observations. this inference problem can be formulated as an objective over a graph that optimizes for the most likely sequence of states using all previous measurements. prior work uses observation models that are either known a-priori or trained on surrogate losses independent of the graph optimizer. in this paper, we propose a method to directly optimize end-to-end tracking performance by learning observation models with the graph optimizer in the loop. this direct approach may appear, however, to require the inference algorithm to be fully differentiable, which many state-of-the-art graph optimizers are not. our key insight is to instead formulate the problem as that of energy-based learning. we propose a novel approach, leo, for learning observation models end-to-end with non-differentiable graph optimizers. leo alternates between sampling trajectories from the graph posterior and updating the model to match these samples to ground truth trajectories. we propose a way to generate such samples efficiently using incremental gauss-newton solvers. we compare leo against baselines on datasets drawn from two distinct tasks: navigation and real-world planar pushing. we show that leo is able to learn complex observation models with lower errors and fewer samples.",-0.98779565,4.9367127,observation,no reinforce,88,"id:88 (observation)<br><b>LEO: Learning Energy-based Models in Factor Graph Optimization</b><br>kw:factor graphs, energy-based models, state estimation"
89,89,213,https://openreview.net/forum?id=JT4orC5QaLQ,Strength Through Diversity: Robust Behavior Learning via Mixture Policies,"Tim Seyde, Wilko Schwarting, Igor Gilitschenski, Markus Wulfmeier, Daniela Rus",5.0,,,,,,,False,,"Learning Control, Hierarchical Optimization, Sample Efficiency","Efficiency in robot learning is highly dependent on hyperparameters. Robot morphology and task structure differ widely and finding the optimal setting typically requires sequential or parallel repetition of experiments, strongly increasing the interaction count. We propose a training method that only relies on a single trial by enabling agents to select and combine controller designs conditioned on the task. Our Hyperparameter Mixture Policies (HMPs) feature diverse sub-policies that vary in distribution types and parameterization, reducing the impact of design choices and unlocking synergies between low-level components. We demonstrate strong performance on the DeepMind Control Suite and a simulated ANYmal robot, showing that HMPs yield robust, data-efficient learning.",,,,,"strength through diversity: robust behavior learning via mixture policies learning control, hierarchical optimization, sample efficiency efficiency in robot learning is highly dependent on hyperparameters. robot morphology and task structure differ widely and finding the optimal setting typically requires sequential or parallel repetition of experiments, strongly increasing the interaction count. we propose a training method that only relies on a single trial by enabling agents to select and combine controller designs conditioned on the task. our hyperparameter mixture policies (hmps) feature diverse sub-policies that vary in distribution types and parameterization, reducing the impact of design choices and unlocking synergies between low-level components. we demonstrate strong performance on the deepmind control suite and a simulated anymal robot, showing that hmps yield robust, data-efficient learning.",-2.833896,3.6906805,mixture,no reinforce,89,"id:89 (mixture)<br><b>Strength Through Diversity: Robust Behavior Learning via Mixture Policies</b><br>kw:Learning Control, Hierarchical Optimization, Sample Efficiency"
90,90,218,https://openreview.net/forum?id=tL24gvaTwhb,Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback,"Xiaofei Wang, Kimin Lee, Kourosh Hakhamaneshi, Pieter Abbeel, Michael Laskin",5.0,,,,,,,False,,"Reinforcement Learning, Skill Extraction, Human Preferences","A promising approach to solving challenging long-horizon tasks has been to extract behavior priors (skills) by fitting generative models to large offline datasets of demonstrations. However, such generative models inherit the biases of the underlying data and result in poor and unusable skills when trained on imperfect demonstration data. To better align skill extraction with human intent we present Skill Preferences (SkiP), an algorithm that learns a model over human preferences and uses it to extract human-aligned skills from offline data. After extracting human-preferred skills, SkiP also utilizes human feedback to solve downstream tasks with RL. We show that SkiP enables a simulated kitchen robot to solve complex multi-step manipulation tasks and substantially outperforms prior leading RL algorithms with human preferences as well as leading skill extraction algorithms without human preferences.",,,,,"skill preferences: learning to extract and execute robotic skills from human feedback reinforcement learning, skill extraction, human preferences a promising approach to solving challenging long-horizon tasks has been to extract behavior priors (skills) by fitting generative models to large offline datasets of demonstrations. however, such generative models inherit the biases of the underlying data and result in poor and unusable skills when trained on imperfect demonstration data. to better align skill extraction with human intent we present skill preferences (skip), an algorithm that learns a model over human preferences and uses it to extract human-aligned skills from offline data. after extracting human-preferred skills, skip also utilizes human feedback to solve downstream tasks with rl. we show that skip enables a simulated kitchen robot to solve complex multi-step manipulation tasks and substantially outperforms prior leading rl algorithms with human preferences as well as leading skill extraction algorithms without human preferences.",-2.9431555,3.2302384,preferences,reinforce,90,"id:90 (preferences)<br><b>Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback</b><br>kw:Reinforcement Learning, Skill Extraction, Human Preferences"
91,91,271,https://openreview.net/forum?id=Pxs5XwId51n,"""Good Robot! Now Watch This!'': Repurposing Reinforcement Learning for Task-to-Task Transfer","Andrew Hundt, Aditya Murali, Priyanka Hubli, Ran Liu, Nakul Gopalan, Matthew Gombolay, Gregory D. Hager",5.0,,,,,,,False,,--,--,,,,,"""good robot! now watch this!'': repurposing reinforcement learning for task-to-task transfer -- --",-1.2777346,1.2138205,watch,reinforce,91,"id:91 (watch)<br><b>""Good Robot! Now Watch This!'': Repurposing Reinforcement Learning for Task-to-Task Transfer</b><br>kw:--"
92,92,324,https://openreview.net/forum?id=s3tu6Ma1jga,Motivating Physical Activity via Competitive Human-Robot Interaction,"Boling Yang, Golnaz Habibi, Patrick Lancaster, Byron Boots, Joshua Smith",5.0,,,,,,,False,,"Competitive Human-robot Interaction, Reinforcement Learning, HRI, Adversarial Learning, Multi-agent System","This project aims to motivate research in competitive human-robot interaction by creating a robot competitor that can challenge human users in certain scenarios such as physical exercise and games. With this goal in mind, we introduce the Fencing Game, a human-robot competition used to evaluate both the capabilities of the robot competitor and user experience. We develop the robot competitor through multi-agent reinforcement learning and show that it can perform well against human competitors. Our user study additionally found that our system was able to continuously create challenging and enjoyable interactions for humans and the majority of human subjects considered the system to be entertaining and useful for improving the quality of their exercise.",,,,,"motivating physical activity via competitive human-robot interaction competitive human-robot interaction, reinforcement learning, hri, adversarial learning, multi-agent system this project aims to motivate research in competitive human-robot interaction by creating a robot competitor that can challenge human users in certain scenarios such as physical exercise and games. with this goal in mind, we introduce the fencing game, a human-robot competition used to evaluate both the capabilities of the robot competitor and user experience. we develop the robot competitor through multi-agent reinforcement learning and show that it can perform well against human competitors. our user study additionally found that our system was able to continuously create challenging and enjoyable interactions for humans and the majority of human subjects considered the system to be entertaining and useful for improving the quality of their exercise.",-2.3162227,3.4802601,competitor,reinforce,92,"id:92 (competitor)<br><b>Motivating Physical Activity via Competitive Human-Robot Interaction</b><br>kw:Competitive Human-robot Interaction, Reinforcement Learning, HRI, Adversarial Learning, Multi-agent System"
93,93,335,https://openreview.net/forum?id=U1GhcnR4jNI,Language Grounding with 3D Objects,"Jesse Thomason, Mohit Shridhar, Yonatan Bisk, Chris Paxton, Luke Zettlemoyer",5.0,,,,,,,False,,"Benchmark, Language Grounding, Vision, 3D","Seemingly simple natural language requests to a robot are generally underspecified, for example ""Can you bring me the wireless mouse?"" Flat images of candidate mice may not provide the discriminative information needed for ""wireless."" The world, and objects in it, are not flat images but complex 3D shapes. If a human requests an object based on any of its basic properties, such as color, shape, or texture, robots should perform the necessary exploration to accomplish the task. In particular, while substantial effort and progress has been made on understanding explicitly visual attributes like color and category, comparatively little progress has been made on understanding language about shapes and contours. In this work, we introduce a novel reasoning task that targets both visual and non-visual language about 3D objects. Our new benchmark ShapeNet Annotated with Referring Expressions (SNARE) requires a model to choose which of two objects is being referenced by a natural language description. We introduce several CLIP-based models for distinguishing objects and demonstrate that while recent advances in jointly modeling vision and language are useful for robotic language understanding, it is still the case that these image-based models are weaker at understanding the 3D nature of objects -- properties which play a key role in manipulation. We find that adding view estimation to language grounding models improves accuracy on both SNARE and when identifying objects referred to in language on a robot platform, but note that a large gap remains between these models and human performance.",,,,,"language grounding with 3d objects benchmark, language grounding, vision, 3d seemingly simple natural language requests to a robot are generally underspecified, for example ""can you bring me the wireless mouse?"" flat images of candidate mice may not provide the discriminative information needed for ""wireless."" the world, and objects in it, are not flat images but complex 3d shapes. if a human requests an object based on any of its basic properties, such as color, shape, or texture, robots should perform the necessary exploration to accomplish the task. in particular, while substantial effort and progress has been made on understanding explicitly visual attributes like color and category, comparatively little progress has been made on understanding language about shapes and contours. in this work, we introduce a novel reasoning task that targets both visual and non-visual language about 3d objects. our new benchmark shapenet annotated with referring expressions (snare) requires a model to choose which of two objects is being referenced by a natural language description. we introduce several clip-based models for distinguishing objects and demonstrate that while recent advances in jointly modeling vision and language are useful for robotic language understanding, it is still the case that these image-based models are weaker at understanding the 3d nature of objects -- properties which play a key role in manipulation. we find that adding view estimation to language grounding models improves accuracy on both snare and when identifying objects referred to in language on a robot platform, but note that a large gap remains between these models and human performance.",-1.198058,4.9154,language,no reinforce,93,"id:93 (language)<br><b>Language Grounding with 3D Objects</b><br>kw:Benchmark, Language Grounding, Vision, 3D"
94,94,343,https://openreview.net/forum?id=NeGDZeyjcKa,A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution,"Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, Yoav Artzi",5.0,,,,,,,False,,"vision and language, spatial representations, semantic mapping, representation learning, instruction following","Natural language provides an accessible and expressive interface to specify long-term tasks for robotic agents. However, non-experts are likely to specify such tasks with high-level instructions, which abstract over specific robot actions through several layers of abstraction. We propose that key to bridging this gap between language and robot actions over long execution horizons are persistent representations. We propose a persistent spatial semantic representation method, and show how it enables building an agent that performs hierarchical reasoning to effectively execute long-term tasks. We evaluate our approach on the ALFRED benchmark and achieve state-of-the-art results, despite completely avoiding the commonly used step-by-step instructions. https://hlsm-alfred.github.io/",,,,,"a persistent spatial semantic representation for high-level natural language instruction execution vision and language, spatial representations, semantic mapping, representation learning, instruction following natural language provides an accessible and expressive interface to specify long-term tasks for robotic agents. however, non-experts are likely to specify such tasks with high-level instructions, which abstract over specific robot actions through several layers of abstraction. we propose that key to bridging this gap between language and robot actions over long execution horizons are persistent representations. we propose a persistent spatial semantic representation method, and show how it enables building an agent that performs hierarchical reasoning to effectively execute long-term tasks. we evaluate our approach on the alfred benchmark and achieve state-of-the-art results, despite completely avoiding the commonly used step-by-step instructions. https://hlsm-alfred.github.io/",-1.2211089,4.439742,persistent,no reinforce,94,"id:94 (persistent)<br><b>A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution</b><br>kw:vision and language, spatial representations, semantic mapping, representation learning, instruction following"
95,95,354,https://openreview.net/forum?id=rif3a5NAxU6,Implicit Behavioral Cloning,"Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, Jonathan Tompson",5.0,,,,,,,False,,"Implicit Models, Energy-Based Models, Imitation Learning","We find that across a wide range of robot policy learning scenarios, treating supervised policy learning with an implicit model generally performs better, on average, than commonly used explicit models.  We present extensive experiments on this finding, and we provide both intuitive insight and theoretical arguments distinguishing the properties of implicit models compared to their explicit counterparts, particularly with respect to approximating complex, potentially discontinuous and multi-valued (set-valued) functions. On robotic policy learning tasks we show that implicit behavior-cloning policies with energy-based models (EBM) often outperform common explicit (Mean Square Error, or Mixture Density) behavior-cloning policies, including on tasks with high-dimensional action spaces and visual image inputs. We find these policies provide competitive results or outperform state-of-the-art offline reinforcement learning methods on the challenging human-expert tasks from the D4RL benchmark suite, despite using no reward information. In the real world, robots with implicit policies can learn complex and remarkably subtle behaviors on contact-rich tasks from human demonstrations, including tasks with high combinatorial complexity and tasks requiring 1mm precision.",,,,,"implicit behavioral cloning implicit models, energy-based models, imitation learning we find that across a wide range of robot policy learning scenarios, treating supervised policy learning with an implicit model generally performs better, on average, than commonly used explicit models.  we present extensive experiments on this finding, and we provide both intuitive insight and theoretical arguments distinguishing the properties of implicit models compared to their explicit counterparts, particularly with respect to approximating complex, potentially discontinuous and multi-valued (set-valued) functions. on robotic policy learning tasks we show that implicit behavior-cloning policies with energy-based models (ebm) often outperform common explicit (mean square error, or mixture density) behavior-cloning policies, including on tasks with high-dimensional action spaces and visual image inputs. we find these policies provide competitive results or outperform state-of-the-art offline reinforcement learning methods on the challenging human-expert tasks from the d4rl benchmark suite, despite using no reward information. in the real world, robots with implicit policies can learn complex and remarkably subtle behaviors on contact-rich tasks from human demonstrations, including tasks with high combinatorial complexity and tasks requiring 1mm precision.",-2.668663,3.6984782,implicit,reinforce,95,"id:95 (implicit)<br><b>Implicit Behavioral Cloning</b><br>kw:Implicit Models, Energy-Based Models, Imitation Learning"
96,96,371,https://openreview.net/forum?id=dgQdvPZnH-t,LanguageRefer: Spatial-Language Model for 3D Visual Grounding,"Junha Roh, Karthik Desingh, Ali Farhadi, Dieter Fox",5.0,,,,,,,False,,"Referring task, Language model, 3D visual grounding, 3D Navigation","To realize robots that can understand human instructions and perform meaningful tasks in the near future, it is important to develop learned models that can understand referential language to identify common objects in real-world 3D scenes. In this paper, we develop a spatial-language model for a 3D visual grounding problem. Specifically, given a reconstructed 3D scene in the form of a point cloud with 3D bounding boxes of potential object candidates, and a language utterance referring to a target object in the scene, our model identifies the target object from a set of potential candidates. Our spatial-language model uses a transformer-based architecture that combines spatial embedding from bounding-box with a finetuned language embedding from DistilBert and reasons among the objects in the 3D scene to find the target object. We show that our model performs competitively on visio-linguistic datasets proposed by ReferIt3D. We provide additional analysis of performance in spatial reasoning tasks decoupled from perception noise, the effect of view-dependent utterances in terms of accuracy, and view-point annotations for potential robotics applications.",,,,,"languagerefer: spatial-language model for 3d visual grounding referring task, language model, 3d visual grounding, 3d navigation to realize robots that can understand human instructions and perform meaningful tasks in the near future, it is important to develop learned models that can understand referential language to identify common objects in real-world 3d scenes. in this paper, we develop a spatial-language model for a 3d visual grounding problem. specifically, given a reconstructed 3d scene in the form of a point cloud with 3d bounding boxes of potential object candidates, and a language utterance referring to a target object in the scene, our model identifies the target object from a set of potential candidates. our spatial-language model uses a transformer-based architecture that combines spatial embedding from bounding-box with a finetuned language embedding from distilbert and reasons among the objects in the 3d scene to find the target object. we show that our model performs competitively on visio-linguistic datasets proposed by referit3d. we provide additional analysis of performance in spatial reasoning tasks decoupled from perception noise, the effect of view-dependent utterances in terms of accuracy, and view-point annotations for potential robotics applications.",-0.87991554,4.82484,3d,no reinforce,96,"id:96 (3d)<br><b>LanguageRefer: Spatial-Language Model for 3D Visual Grounding</b><br>kw:Referring task, Language model, 3D visual grounding, 3D Navigation"
97,97,372,https://openreview.net/forum?id=s8xjoLghedM,Learning Multimodal Rewards from Rankings,"Vivek Myers, Erdem Biyik, Nima Anari, Dorsa Sadigh",5.0,,,,,,,False,,"HRI, reward learning, multi-modality, rankings, active learning","Learning from human feedback has shown to be a useful approach in acquiring robot reward functions. However, expert feedback is often assumed to be drawn from an underlying unimodal reward function. This assumption does not always hold including in settings where multiple experts provide data or when a single expert provides data for different tasks---we thus go beyond learning a unimodal reward and focus on learning a multimodal reward function. We formulate the multimodal reward learning as a mixture learning problem and develop a novel ranking-based learning approach, where the experts are only required to rank a given set of trajectories. Furthermore, as access to interaction data is often expensive in robotics, we develop an active querying approach to accelerate the learning process. We conduct experiments and user studies using a multi-task variant of OpenAI's LunarLander and a real Fetch robot, where we collect data from multiple users with different preferences. The results suggest that our approach can efficiently learn multimodal reward functions, and improve data-efficiency over benchmark methods that we adapt to our learning problem.",,,,,"learning multimodal rewards from rankings hri, reward learning, multi-modality, rankings, active learning learning from human feedback has shown to be a useful approach in acquiring robot reward functions. however, expert feedback is often assumed to be drawn from an underlying unimodal reward function. this assumption does not always hold including in settings where multiple experts provide data or when a single expert provides data for different tasks---we thus go beyond learning a unimodal reward and focus on learning a multimodal reward function. we formulate the multimodal reward learning as a mixture learning problem and develop a novel ranking-based learning approach, where the experts are only required to rank a given set of trajectories. furthermore, as access to interaction data is often expensive in robotics, we develop an active querying approach to accelerate the learning process. we conduct experiments and user studies using a multi-task variant of openai's lunarlander and a real fetch robot, where we collect data from multiple users with different preferences. the results suggest that our approach can efficiently learn multimodal reward functions, and improve data-efficiency over benchmark methods that we adapt to our learning problem.",-2.0753276,3.7642088,reward,no reinforce,97,"id:97 (reward)<br><b>Learning Multimodal Rewards from Rankings</b><br>kw:HRI, reward learning, multi-modality, rankings, active learning"
98,98,384,https://openreview.net/forum?id=fy4ZBWxYbIo,A Workflow for Offline Model-Free Robotic Reinforcement Learning,"Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, Sergey Levine",5.0,,,,,,,False,,"workflow, offline RL, no online tuning","Offline reinforcement learning (RL) enables learning control policies by utilizing only prior experience, without any online interaction. This can allow robots to acquire generalizable skills from large and diverse datasets, without costly online data collection. Despite recent algorithmic advances in offline RL, applying these methods to real-world problems has proven challenging. Although offline RL methods can learn from prior data, there is no clear and well-understood process for making various design choices, from model architecture to algorithm hyperparameters, without actually evaluating the learned policies online. In this paper, our aim is to develop a practical workflow for applying offline RL analogous to the relatively well-understood workflows for supervised learning problems. To this end, we devise a set of metrics and conditions that can be tracked over the course of offline training, and can inform the practitioner about how the algorithm and model architecture should be adjusted to improve final performance. Our workflow is derived from a conceptual understanding of the behavior of conservative offline RL algorithms and cross-validation in supervised learning. We demonstrate the efficacy of this workflow in producing effective policies without any online tuning, both in several simulated robotic learning scenarios and for three tasks on two distinct real robots, focusing on learning manipulation skills with raw image observations with sparse binary rewards.",,,,,"a workflow for offline model-free robotic reinforcement learning workflow, offline rl, no online tuning offline reinforcement learning (rl) enables learning control policies by utilizing only prior experience, without any online interaction. this can allow robots to acquire generalizable skills from large and diverse datasets, without costly online data collection. despite recent algorithmic advances in offline rl, applying these methods to real-world problems has proven challenging. although offline rl methods can learn from prior data, there is no clear and well-understood process for making various design choices, from model architecture to algorithm hyperparameters, without actually evaluating the learned policies online. in this paper, our aim is to develop a practical workflow for applying offline rl analogous to the relatively well-understood workflows for supervised learning problems. to this end, we devise a set of metrics and conditions that can be tracked over the course of offline training, and can inform the practitioner about how the algorithm and model architecture should be adjusted to improve final performance. our workflow is derived from a conceptual understanding of the behavior of conservative offline rl algorithms and cross-validation in supervised learning. we demonstrate the efficacy of this workflow in producing effective policies without any online tuning, both in several simulated robotic learning scenarios and for three tasks on two distinct real robots, focusing on learning manipulation skills with raw image observations with sparse binary rewards.",-3.3830843,3.443419,offline,reinforce,98,"id:98 (offline)<br><b>A Workflow for Offline Model-Free Robotic Reinforcement Learning</b><br>kw:workflow, offline RL, no online tuning"
99,99,4,https://openreview.net/forum?id=ar8FMzaZbcq,Fully Self-Supervised Class Awareness in Dense Object Descriptors,"Denis Hadjivelichkov, Dimitrios Kanoulas",6.0,,,,,,,False,,"Self-Supervision, Descriptor Learning, Object Correspondence",We address the problem of inferring self-supervised dense semantic correspondences between objects        in multi-object scenes. The method introduces learning of class-aware dense object descriptors by providing either unsupervised discrete labels or confidence in object similarities. We quantitatively and qualitatively show that the introduced method outperforms previous techniques with more robust pixel-to-pixel matches. An example robotic application is also shown~- grasping of objects in clutter based on corresponding points.,,,,,"fully self-supervised class awareness in dense object descriptors self-supervision, descriptor learning, object correspondence we address the problem of inferring self-supervised dense semantic correspondences between objects        in multi-object scenes. the method introduces learning of class-aware dense object descriptors by providing either unsupervised discrete labels or confidence in object similarities. we quantitatively and qualitatively show that the introduced method outperforms previous techniques with more robust pixel-to-pixel matches. an example robotic application is also shown~- grasping of objects in clutter based on corresponding points.",-1.4684956,4.1327367,dense,no reinforce,99,"id:99 (dense)<br><b>Fully Self-Supervised Class Awareness in Dense Object Descriptors</b><br>kw:Self-Supervision, Descriptor Learning, Object Correspondence"
100,100,64,https://openreview.net/forum?id=GhMZNcr54zt,Learning to Predict Vehicle Trajectories with Model-based Planning,"Haoran Song, Di Luan, Wenchao Ding, Michael Y Wang, Qifeng Chen",6.0,,,,,,,False,,"Trajectory Prediction, Autonomous Driving","Predicting the future trajectories of on-road vehicles is critical for autonomous driving. In this paper, we introduce a novel prediction framework called PRIME, which stands for Prediction with Model-based Planning. Unlike recent prediction works that utilize neural networks to model scene context and produce unconstrained trajectories, PRIME is designed to generate accurate and feasibility-guaranteed future trajectory predictions. PRIME guarantees the trajectory feasibility by exploiting a model-based generator to produce future trajectories under explicit constraints and enables accurate multimodal prediction by utilizing a learning-based evaluator to select future trajectories. We conduct experiments on the large-scale Argoverse Motion Forecasting Benchmark, where PRIME outperforms the state-of-the-art methods in prediction accuracy, feasibility, and robustness under imperfect tracking.",,,,,"learning to predict vehicle trajectories with model-based planning trajectory prediction, autonomous driving predicting the future trajectories of on-road vehicles is critical for autonomous driving. in this paper, we introduce a novel prediction framework called prime, which stands for prediction with model-based planning. unlike recent prediction works that utilize neural networks to model scene context and produce unconstrained trajectories, prime is designed to generate accurate and feasibility-guaranteed future trajectory predictions. prime guarantees the trajectory feasibility by exploiting a model-based generator to produce future trajectories under explicit constraints and enables accurate multimodal prediction by utilizing a learning-based evaluator to select future trajectories. we conduct experiments on the large-scale argoverse motion forecasting benchmark, where prime outperforms the state-of-the-art methods in prediction accuracy, feasibility, and robustness under imperfect tracking.",-3.1564786,3.2220612,prime,no reinforce,100,"id:100 (prime)<br><b>Learning to Predict Vehicle Trajectories with Model-based Planning</b><br>kw:Trajectory Prediction, Autonomous Driving"
101,101,86,https://openreview.net/forum?id=n1hDe9iK6ms,Learning Visible Connectivity Dynamics for Cloth Smoothing,"Xingyu Lin, Yufei Wang, Zixuan Huang, David Held",6.0,,,,,,,False,,"Deformable object manipulation, dynamics model learning","Robotic manipulation of cloth remains challenging due to the complex dynamics of cloth, lack of a low-dimensional state representation, and self-occlusions. In contrast to previous model-based approaches that learn a pixel-based dynamics model or a compressed latent vector dynamics, we propose to learn a particle-based dynamics model from a partial point cloud observation. To overcome the challenges of partial observability, we infer which visible points are connected on the underlying cloth mesh.  We then learn a dynamics model over this visible connectivity graph. Compared to previous learning-based approaches, our model poses strong inductive bias with its particle based representation for learning the underlying cloth physics; it can generalize to cloths with novel shapes; it is invariant to visual features; and the predictions can be more easily visualized. We show that our method greatly outperforms previous state-of-the-art model-based and model-free reinforcement learning methods in simulation. Furthermore, we demonstrate zero-shot sim-to-real transfer where we deploy the model trained in simulation on a Franka arm and show that the model can successfully smooth cloths of different materials, geometries and colors from crumpled configurations. Videos can be found in the supplement and on our anonymous project website: https://sites.google.com/view/vcd-cloth.",,,,,"learning visible connectivity dynamics for cloth smoothing deformable object manipulation, dynamics model learning robotic manipulation of cloth remains challenging due to the complex dynamics of cloth, lack of a low-dimensional state representation, and self-occlusions. in contrast to previous model-based approaches that learn a pixel-based dynamics model or a compressed latent vector dynamics, we propose to learn a particle-based dynamics model from a partial point cloud observation. to overcome the challenges of partial observability, we infer which visible points are connected on the underlying cloth mesh.  we then learn a dynamics model over this visible connectivity graph. compared to previous learning-based approaches, our model poses strong inductive bias with its particle based representation for learning the underlying cloth physics; it can generalize to cloths with novel shapes; it is invariant to visual features; and the predictions can be more easily visualized. we show that our method greatly outperforms previous state-of-the-art model-based and model-free reinforcement learning methods in simulation. furthermore, we demonstrate zero-shot sim-to-real transfer where we deploy the model trained in simulation on a franka arm and show that the model can successfully smooth cloths of different materials, geometries and colors from crumpled configurations. videos can be found in the supplement and on our anonymous project website: https://sites.google.com/view/vcd-cloth.",-1.2672188,4.7571516,cloth,reinforce,101,"id:101 (cloth)<br><b>Learning Visible Connectivity Dynamics for Cloth Smoothing</b><br>kw:Deformable object manipulation, dynamics model learning"
102,102,111,https://openreview.net/forum?id=sIVC-oZN1PQ,Visually-Grounded Library of Behaviors for Manipulating Diverse Objects across Diverse Configurations and Views,"Jingyun Yang, Hsiao-Yu Tung, Yunchu Zhang, Gaurav Pathak, Ashwini Pokle, Christopher G Atkeson, Katerina Fragkiadaki",6.0,,,,,,,False,,"robot learning, visual representation, interactive perception","We propose a visually-grounded library of behaviors approach for learning to manipulate diverse objects across varying initial and goal configurations and camera placements. Our key innovation is to disentangle the standard image-to-action mapping into two separate modules that use different types of perceptual input: (1) a behavior selector which conditions on intrinsic and semantically-rich object appearance features to select the behaviors that can successfully perform the desired tasks on the object in hand, and (2) a library of behaviors each of which conditions on extrinsic and abstract object properties, such as object location and pose, to predict actions to execute over time. The selector uses a semantically-rich 3D object feature representation extracted from images in a differential end-to-end manner. This representation is trained to be view-invariant and affordance-aware using self-supervision, by predicting varying views and successful object manipulations. We test our framework on pushing and grasping diverse objects in simulation as well as transporting rigid, granular, and liquid food ingredients in a real robot setup. Our model outperforms image-to-action mappings that do not factorize static and dynamic object properties. We further ablate the contribution of the selector's input and show the benefits of the proposed view-predictive, affordance-aware 3D visual object representations.",,,,,"visually-grounded library of behaviors for manipulating diverse objects across diverse configurations and views robot learning, visual representation, interactive perception we propose a visually-grounded library of behaviors approach for learning to manipulate diverse objects across varying initial and goal configurations and camera placements. our key innovation is to disentangle the standard image-to-action mapping into two separate modules that use different types of perceptual input: (1) a behavior selector which conditions on intrinsic and semantically-rich object appearance features to select the behaviors that can successfully perform the desired tasks on the object in hand, and (2) a library of behaviors each of which conditions on extrinsic and abstract object properties, such as object location and pose, to predict actions to execute over time. the selector uses a semantically-rich 3d object feature representation extracted from images in a differential end-to-end manner. this representation is trained to be view-invariant and affordance-aware using self-supervision, by predicting varying views and successful object manipulations. we test our framework on pushing and grasping diverse objects in simulation as well as transporting rigid, granular, and liquid food ingredients in a real robot setup. our model outperforms image-to-action mappings that do not factorize static and dynamic object properties. we further ablate the contribution of the selector's input and show the benefits of the proposed view-predictive, affordance-aware 3d visual object representations.",-1.5141528,4.6689544,library,no reinforce,102,"id:102 (library)<br><b>Visually-Grounded Library of Behaviors for Manipulating Diverse Objects across Diverse Configurations and Views</b><br>kw:robot learning, visual representation, interactive perception"
103,103,127,https://openreview.net/forum?id=DCfV0wDvtoV,Generalised Task Planning with First-Order Function Approximation,"Jun Hao Alvin Ng, Ron Petrick",6.0,,,,,,,False,,"task planning, relational reinforcement learning, transfer learning","Real world robotics often operates in uncertain and dynamic environments. Therefore, generalisation over different scenarios is of practical interest. In the absence of a model, value-based reinforcement learning can be used to learn a goal-directed policy. Typically, the interaction between robots and the objects in the environment exhibit a first-order structure. We introduce first-order, or relational, features to represent an approximation of the Q-function so that it can induce a generalised policy. Empirical results for a service robot domain show that our online relational reinforcement learning method is scalable to large-scale problems and enables transfer learning between different problems and simulation environments with dissimilar transition dynamics.",,,,,"generalised task planning with first-order function approximation task planning, relational reinforcement learning, transfer learning real world robotics often operates in uncertain and dynamic environments. therefore, generalisation over different scenarios is of practical interest. in the absence of a model, value-based reinforcement learning can be used to learn a goal-directed policy. typically, the interaction between robots and the objects in the environment exhibit a first-order structure. we introduce first-order, or relational, features to represent an approximation of the q-function so that it can induce a generalised policy. empirical results for a service robot domain show that our online relational reinforcement learning method is scalable to large-scale problems and enables transfer learning between different problems and simulation environments with dissimilar transition dynamics.",-1.7473997,3.2928886,generalised,reinforce,103,"id:103 (generalised)<br><b>Generalised Task Planning with First-Order Function Approximation</b><br>kw:task planning, relational reinforcement learning, transfer learning"
104,104,174,https://openreview.net/forum?id=tCfLLiP7vje,Seeing Glass: Joint Point-Cloud and Depth Completion for Transparent Objects,"Haoping Xu, Yi Ru Wang, Sagi Eppel, Alan Aspuru-Guzik, Florian Shkurti, Animesh Garg",6.0,,,,☆,,,False,,"Transparent Objects, Depth Completion, 3D Perception, Data Collection","The basis of many object manipulation algorithms is RGB-D input. Yet, commodity RGB-D sensors can only provide distorted depth maps for a wide range of transparent objects due light refraction and absorption. To tackle the perception challenges posed by transparent objects, we propose TranspareNet, a joint point cloud and depth completion method, with the ability to complete the depth of transparent objects in cluttered and complex scenes, even with partially filled fluid contents within the vessels. To address the shortcomings of existing transparent object data collection schemes in literature, we also propose an automated dataset creation workflow that consists of robot-controlled image collection and vision-based automatic annotation. Through this automated workflow, we created Transparent Object Depth Dataset (TODD), which consists of nearly 15000 RGB-D images. Our experimental evaluation demonstrates that TranspareNet outperforms existing state-of-the-art depth completion methods on multiple datasets, including ClearGrasp, and that it also handles cluttered scenes when trained on TODD. Code and dataset will be released at https://anonymous.4open.science/r/TranspareNet-567C/",,,,,"seeing glass: joint point-cloud and depth completion for transparent objects transparent objects, depth completion, 3d perception, data collection the basis of many object manipulation algorithms is rgb-d input. yet, commodity rgb-d sensors can only provide distorted depth maps for a wide range of transparent objects due light refraction and absorption. to tackle the perception challenges posed by transparent objects, we propose transparenet, a joint point cloud and depth completion method, with the ability to complete the depth of transparent objects in cluttered and complex scenes, even with partially filled fluid contents within the vessels. to address the shortcomings of existing transparent object data collection schemes in literature, we also propose an automated dataset creation workflow that consists of robot-controlled image collection and vision-based automatic annotation. through this automated workflow, we created transparent object depth dataset (todd), which consists of nearly 15000 rgb-d images. our experimental evaluation demonstrates that transparenet outperforms existing state-of-the-art depth completion methods on multiple datasets, including cleargrasp, and that it also handles cluttered scenes when trained on todd. code and dataset will be released at https://anonymous.4open.science/r/transparenet-567c/",-1.2276036,4.6082535,transparent,no reinforce,104,"id:104 (transparent)<br><b>Seeing Glass: Joint Point-Cloud and Depth Completion for Transparent Objects</b><br>kw:Transparent Objects, Depth Completion, 3D Perception, Data Collection"
105,105,186,https://openreview.net/forum?id=Esw0Wh6Stt5,ReLMM: Practical RL for Learning MobileManipulation Skills Using Only Onboard Sensors,"Charles Sun, J?drzej Orbik, Coline Manon Devin, Brian H Yang, Abhishek Gupta, Glen Berseth, Sergey Levine",6.0,,,,☆,,,False,,"mobile manipulation, reinforcement learning, Reset-Free","In this paper, we study how robots can autonomously learn skills that require a combination of navigation and grasping. Learning robotic skills in the real world remains challenging without large scale data collection and supervision. Our aim is to devise a robotic reinforcement learning system for learning navigation and manipulation together, in an autonomous way without human intervention, enabling continual learning under realistic assumptions. Specifically, our system, ReLMM, can learn continuously on a real-world platform without any environment instrumentation, without human intervention, and without access to privileged information, such as maps, objects positions, or a global view of the environment. Our method employs a modularized policy with components for manipulation and navigation, where uncertainty over the manipulation success drives exploration for the navigation controller, and the manipulation module provides rewards for navigation. We evaluate our method on a room cleanup task, where the robot must navigate to and pick up items of scattered on the floor. After a grasp curriculum training phase, ReLMM can learn navigation and grasping together fully automatically, in around 40 hours of real-world training.",,,,,"relmm: practical rl for learning mobilemanipulation skills using only onboard sensors mobile manipulation, reinforcement learning, reset-free in this paper, we study how robots can autonomously learn skills that require a combination of navigation and grasping. learning robotic skills in the real world remains challenging without large scale data collection and supervision. our aim is to devise a robotic reinforcement learning system for learning navigation and manipulation together, in an autonomous way without human intervention, enabling continual learning under realistic assumptions. specifically, our system, relmm, can learn continuously on a real-world platform without any environment instrumentation, without human intervention, and without access to privileged information, such as maps, objects positions, or a global view of the environment. our method employs a modularized policy with components for manipulation and navigation, where uncertainty over the manipulation success drives exploration for the navigation controller, and the manipulation module provides rewards for navigation. we evaluate our method on a room cleanup task, where the robot must navigate to and pick up items of scattered on the floor. after a grasp curriculum training phase, relmm can learn navigation and grasping together fully automatically, in around 40 hours of real-world training.",-2.7929463,3.544274,"intervention,",reinforce,105,"id:105 (intervention,)<br><b>ReLMM: Practical RL for Learning MobileManipulation Skills Using Only Onboard Sensors</b><br>kw:mobile manipulation, reinforcement learning, Reset-Free"
106,106,207,https://openreview.net/forum?id=wBT0lZJAJ0V,Learn2Assemble with Structured Representations and Search for Robotic Architectural Construction,"Niklas Funk, Georgia Chalvatzaki, Boris Belousov, Jan Peters",6.0,,,,,,,False,,"Structured representations, Autonomous assembly, Manipulation","Autonomous robotic assembly requires a well-orchestrated sequence of high-level actions and smooth manipulation executions. Learning to assemble complex 3D structures remains a challenging problem that requires drawing connections between target designs and building blocks, and creating valid assembly sequences considering structural stability and feasibility. To address the combinatorial complexity of the assembly tasks, we propose a multi-head attention graph representation that can be trained with reinforcement learning (RL) to encode the spatial relations and provide meaningful assembly actions. Combining structured representations with model-free RL and Monte-Carlo planning allows agents to operate with various target shapes and building block types. We design a hierarchical control framework that learns to sequence the building blocks to construct arbitrary 3D designs and ensures their feasibility, as we plan the geometric execution with the robot-in-the-loop. We demonstrate the flexibility of the proposed structured representation and our algorithmic solution in a series of simulated 3D assembly tasks with robotic evaluation, which showcases our method's ability to learn to construct stable structures with a large number of building blocks. Code and videos are available at: https://sites.google.com/view/learn2assemble",,,,,"learn2assemble with structured representations and search for robotic architectural construction structured representations, autonomous assembly, manipulation autonomous robotic assembly requires a well-orchestrated sequence of high-level actions and smooth manipulation executions. learning to assemble complex 3d structures remains a challenging problem that requires drawing connections between target designs and building blocks, and creating valid assembly sequences considering structural stability and feasibility. to address the combinatorial complexity of the assembly tasks, we propose a multi-head attention graph representation that can be trained with reinforcement learning (rl) to encode the spatial relations and provide meaningful assembly actions. combining structured representations with model-free rl and monte-carlo planning allows agents to operate with various target shapes and building block types. we design a hierarchical control framework that learns to sequence the building blocks to construct arbitrary 3d designs and ensures their feasibility, as we plan the geometric execution with the robot-in-the-loop. we demonstrate the flexibility of the proposed structured representation and our algorithmic solution in a series of simulated 3d assembly tasks with robotic evaluation, which showcases our method's ability to learn to construct stable structures with a large number of building blocks. code and videos are available at: https://sites.google.com/view/learn2assemble",-2.7232425,3.7906659,assembly,reinforce,106,"id:106 (assembly)<br><b>Learn2Assemble with Structured Representations and Search for Robotic Architectural Construction</b><br>kw:Structured representations, Autonomous assembly, Manipulation"
107,107,212,https://openreview.net/forum?id=pXpytHo_GC1,Learning to Plan Optimistically: Uncertainty-Guided Deep Exploration via Latent Model Ensembles,"Tim Seyde, Wilko Schwarting, Sertac Karaman, Daniela Rus",6.0,,,,,,,False,,"Learning Control, Sample Efficiency, Exploration","Learning complex robot behaviors through interaction requires structured exploration. Planning should target interactions with the potential to optimize long-term performance, while only reducing uncertainty where conducive to this objective. This paper presents Latent Optimistic Value Exploration (LOVE), a strategy that enables deep exploration through optimism in the face of uncertain long-term rewards. We combine latent world models with value function estimation to predict infinite-horizon returns and recover associated uncertainty via ensembling. The policy is then trained on an upper confidence bound (UCB) objective to identify and select the interactions most promising to improve long-term performance. We apply LOVE to visual robot control tasks in continuous action spaces and demonstrate on average more than 15% improved sample efficiency in comparison to state-of-the-art and other exploration objectives. In sparse and hard to explore environments we achieve an average improvement of over 30%.",,,,,"learning to plan optimistically: uncertainty-guided deep exploration via latent model ensembles learning control, sample efficiency, exploration learning complex robot behaviors through interaction requires structured exploration. planning should target interactions with the potential to optimize long-term performance, while only reducing uncertainty where conducive to this objective. this paper presents latent optimistic value exploration (love), a strategy that enables deep exploration through optimism in the face of uncertain long-term rewards. we combine latent world models with value function estimation to predict infinite-horizon returns and recover associated uncertainty via ensembling. the policy is then trained on an upper confidence bound (ucb) objective to identify and select the interactions most promising to improve long-term performance. we apply love to visual robot control tasks in continuous action spaces and demonstrate on average more than 15% improved sample efficiency in comparison to state-of-the-art and other exploration objectives. in sparse and hard to explore environments we achieve an average improvement of over 30%.",-2.304985,3.6495605,exploration,no reinforce,107,"id:107 (exploration)<br><b>Learning to Plan Optimistically: Uncertainty-Guided Deep Exploration via Latent Model Ensembles</b><br>kw:Learning Control, Sample Efficiency, Exploration"
108,108,247,https://openreview.net/forum?id=uJi8OvaanP6,Redundancy Resolution as Action Bias in Policy Search for Robotic Manipulation,"Firas Al-Hafez, Jochen J. Steil",6.0,,,,,,,False,,"Deep Reinforcement Learning, Evolution Strategies, Redundancy Resolution, Action Bias, Velocity Control","We propose a novel approach that biases actions during policy search by lifting the concept of redundancy resolution from multi-DoF robot kinematics to the level of the reward in deep reinforcement learning and evolution strategies. The key idea is to bias the distribution of executed actions in the sense that the immediate reward remains unchanged. The resulting biased actions favor secondary objectives yielding policies that are safer to apply on the real robot. We demonstrate the feasibility of our method, considered as policy search with redundant action bias (PSRAB), in a reaching and a pick-and-lift task with a 7-DoF Franka robot arm trained in RLBench -- a recently introduced benchmark for robotic manipulation -- using state-of-the-art TD3 deep reinforcement learning and OpenAI's evolutionary strategy. We show that it is a flexible approach without the need of significant fine-tuning and interference with the main objective even across different policy search methods and tasks of different complexity. We evaluate our approach in simulation and on the real robot.",,,,,"redundancy resolution as action bias in policy search for robotic manipulation deep reinforcement learning, evolution strategies, redundancy resolution, action bias, velocity control we propose a novel approach that biases actions during policy search by lifting the concept of redundancy resolution from multi-dof robot kinematics to the level of the reward in deep reinforcement learning and evolution strategies. the key idea is to bias the distribution of executed actions in the sense that the immediate reward remains unchanged. the resulting biased actions favor secondary objectives yielding policies that are safer to apply on the real robot. we demonstrate the feasibility of our method, considered as policy search with redundant action bias (psrab), in a reaching and a pick-and-lift task with a 7-dof franka robot arm trained in rlbench -- a recently introduced benchmark for robotic manipulation -- using state-of-the-art td3 deep reinforcement learning and openai's evolutionary strategy. we show that it is a flexible approach without the need of significant fine-tuning and interference with the main objective even across different policy search methods and tasks of different complexity. we evaluate our approach in simulation and on the real robot.",-2.4212697,3.5648952,redundancy,reinforce,108,"id:108 (redundancy)<br><b>Redundancy Resolution as Action Bias in Policy Search for Robotic Manipulation</b><br>kw:Deep Reinforcement Learning, Evolution Strategies, Redundancy Resolution, Action Bias, Velocity Control"
109,109,294,https://openreview.net/forum?id=wVIqlSqKu2D,Differentiable Rendering and Identification of Impact Sounds,"Samuel Clarke, Negin Heravi, Mark Rau, Ruohan Gao, Jiajun Wu, Doug James, Jeannette Bohg",6.0,,,,,,,False,,"Differentiable Sound Rendering, Auditory Scene Analysis","Rigid objects make distinctive sounds during manipulation. These sounds are a function of object features, such as shape and material, and of contact forces during manipulation. Being able to infer from sound an object's acoustic properties, how it is being manipulated, and what events it is participating in could augment and complement what robots can perceive from vision, especially in case of occlusion, low visual resolution, poor lighting, or blurred focus. Annotations on sound data are rare. Therefore, existing inference systems mostly include a sound renderer in the loop, and use analysis-by-synthesis to optimize for object acoustic properties. Optimizing parameters with respect to a non-differentiable renderer is slow and hard to scale to complex scenes. We present DiffImpact, a fully differentiable model for sounds rigid objects make during impacts, based on physical principles of impact forces, rigid object vibration, and other acoustic effects. Its differentiability enables gradient-based, efficient joint inference of acoustic properties of the objects and characteristics and timings of each individual impact. DiffImpact can also be plugged in as the decoder of an autoencoder, and trained end-to-end on real audio data, so that the encoder can learn to solve the inverse problem in a self-supervised way. Experiments demonstrate that our model's physics-based inductive biases make it more resource efficient and expressive than state-of-the-art pure learning-based alternatives, on both forward rendering of impact sounds and inverse tasks such as acoustic property inference and blind source separation of impact sounds.",,,,,"differentiable rendering and identification of impact sounds differentiable sound rendering, auditory scene analysis rigid objects make distinctive sounds during manipulation. these sounds are a function of object features, such as shape and material, and of contact forces during manipulation. being able to infer from sound an object's acoustic properties, how it is being manipulated, and what events it is participating in could augment and complement what robots can perceive from vision, especially in case of occlusion, low visual resolution, poor lighting, or blurred focus. annotations on sound data are rare. therefore, existing inference systems mostly include a sound renderer in the loop, and use analysis-by-synthesis to optimize for object acoustic properties. optimizing parameters with respect to a non-differentiable renderer is slow and hard to scale to complex scenes. we present diffimpact, a fully differentiable model for sounds rigid objects make during impacts, based on physical principles of impact forces, rigid object vibration, and other acoustic effects. its differentiability enables gradient-based, efficient joint inference of acoustic properties of the objects and characteristics and timings of each individual impact. diffimpact can also be plugged in as the decoder of an autoencoder, and trained end-to-end on real audio data, so that the encoder can learn to solve the inverse problem in a self-supervised way. experiments demonstrate that our model's physics-based inductive biases make it more resource efficient and expressive than state-of-the-art pure learning-based alternatives, on both forward rendering of impact sounds and inverse tasks such as acoustic property inference and blind source separation of impact sounds.",-0.8750096,5.0660143,acoustic,no reinforce,109,"id:109 (acoustic)<br><b>Differentiable Rendering and Identification of Impact Sounds</b><br>kw:Differentiable Sound Rendering, Auditory Scene Analysis"
110,110,318,https://openreview.net/forum?id=ovRdr3FOIIm,Geometry-aware Bayesian Optimization in Robotics using Riemannian Mat?rn Kernels,"No?mie Jaquier, Viacheslav Borovitskiy, Andrei Smolensky, Alexander Terenin, Tamim Asfour, Leonel Rozo",6.0,,,,,,,False,,"Bayesian optimization, Matérn kernels, Riemannian manifolds","Bayesian optimization is a data-efficient technique which can be used for control parameter tuning, parametric policy adaptation, and structure design in robotics. Many of these problems require optimization of functions defined on non-Euclidean domains like spheres, rotation groups, or spaces of positive-definite matrices. To do so, one must place a Gaussian process prior, or equivalently define a kernel, on the space of interest. Effective kernels typically reflect the geometry of the spaces they are defined on, but designing them is generally non-trivial. Recent work on the Riemannian Matérn kernels, based on stochastic partial differential equations and spectral theory of the Laplace--Beltrami operator, offers promising avenues towards constructing such geometry-aware kernels. In this paper, we study techniques for implementing these kernels on manifolds of interest in robotics, demonstrate their performance on a set of artificial benchmark functions, and illustrate geometry-aware Bayesian optimization for a variety of robotic applications, covering orientation control, manipulability optimization, and  motion planning, while showing its improved performance.",,,,,"geometry-aware bayesian optimization in robotics using riemannian mat?rn kernels bayesian optimization, matérn kernels, riemannian manifolds bayesian optimization is a data-efficient technique which can be used for control parameter tuning, parametric policy adaptation, and structure design in robotics. many of these problems require optimization of functions defined on non-euclidean domains like spheres, rotation groups, or spaces of positive-definite matrices. to do so, one must place a gaussian process prior, or equivalently define a kernel, on the space of interest. effective kernels typically reflect the geometry of the spaces they are defined on, but designing them is generally non-trivial. recent work on the riemannian matérn kernels, based on stochastic partial differential equations and spectral theory of the laplace--beltrami operator, offers promising avenues towards constructing such geometry-aware kernels. in this paper, we study techniques for implementing these kernels on manifolds of interest in robotics, demonstrate their performance on a set of artificial benchmark functions, and illustrate geometry-aware bayesian optimization for a variety of robotic applications, covering orientation control, manipulability optimization, and  motion planning, while showing its improved performance.",-3.3482246,3.7431047,kernels,no reinforce,110,"id:110 (kernels)<br><b>Geometry-aware Bayesian Optimization in Robotics using Riemannian Mat?rn Kernels</b><br>kw:Bayesian optimization, Matérn kernels, Riemannian manifolds"
111,111,331,https://openreview.net/forum?id=ceOmpjMhlyS,STORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation,"Mohak Bhardwaj, Balakumar Sundaralingam, Arsalan Mousavian, Nathan D. Ratliff, Dieter Fox, Fabio Ramos, Byron Boots",6.0,,,,,,,False,,"model-predictive control, manipulation","Sampling-based model-predictive control (MPC) is a promising tool for feedback control of robots with complex, non-smooth dynamics, and cost functions. However, the computationally demanding nature of sampling-based MPC algorithms has been a key bottleneck in their application to high-dimensional robotic manipulation problems in the real world. Previous methods have addressed this issue by running MPC in the task space while relying on a low-level operational space controller for joint control. However, by not using the joint space of the robot in the MPC formulation, existing methods cannot directly account for non-task space related constraints such as avoiding joint limits, singular configurations, and link collisions. In this paper, we develop a system for fast, joint space sampling-based MPC for manipulators that is efficiently parallelized using GPUs. Our approach can handle task and joint space constraints while taking less than 8ms~(125Hz) to compute the next control command. Further, our method can tightly integrate perception into the control problem by utilizing learned cost functions from raw sensor data. We validate our approach by deploying it on a Franka Panda robot for a variety of dynamic manipulation tasks. We study the effect of different cost formulations and MPC parameters on the synthesized behavior and provide key insights that pave the way for the application of sampling-based MPC for manipulators in a principled manner. We also provide highly optimized, open-source code to be used by the wider robot learning and control community. Videos of experiments can be found at: https://sites.google.com/view/manipulation-mpc",,,,,"storm: an integrated framework for fast joint-space model-predictive control for reactive manipulation model-predictive control, manipulation sampling-based model-predictive control (mpc) is a promising tool for feedback control of robots with complex, non-smooth dynamics, and cost functions. however, the computationally demanding nature of sampling-based mpc algorithms has been a key bottleneck in their application to high-dimensional robotic manipulation problems in the real world. previous methods have addressed this issue by running mpc in the task space while relying on a low-level operational space controller for joint control. however, by not using the joint space of the robot in the mpc formulation, existing methods cannot directly account for non-task space related constraints such as avoiding joint limits, singular configurations, and link collisions. in this paper, we develop a system for fast, joint space sampling-based mpc for manipulators that is efficiently parallelized using gpus. our approach can handle task and joint space constraints while taking less than 8ms~(125hz) to compute the next control command. further, our method can tightly integrate perception into the control problem by utilizing learned cost functions from raw sensor data. we validate our approach by deploying it on a franka panda robot for a variety of dynamic manipulation tasks. we study the effect of different cost formulations and mpc parameters on the synthesized behavior and provide key insights that pave the way for the application of sampling-based mpc for manipulators in a principled manner. we also provide highly optimized, open-source code to be used by the wider robot learning and control community. videos of experiments can be found at: https://sites.google.com/view/manipulation-mpc",-1.8586816,4.0391846,mpc,no reinforce,111,"id:111 (mpc)<br><b>STORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation</b><br>kw:model-predictive control, manipulation"
112,112,337,https://openreview.net/forum?id=xHnJS2GYFDz,DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries,"Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, Justin Solomon",6.0,,,,,,,False,,"multi-camera, 3D object detection, autonomous driving","We introduce a framework for multi-camera 3D object detection. In contrast to existing works, which estimate 3D bounding boxes directly from monocular images or use depth prediction networks to generate input for 3D object detection from 2D information, our method manipulates predictions directly in 3D space. Our architecture extracts 2D  features from multiple camera images and then uses a sparse set of 3D object queries to index into these 2D features, linking 3D positions to multi-view images using camera transformation matrices. Finally, our model makes a bounding box prediction per object query, using a set-to-set loss to measure the discrepancy between the ground-truth and the prediction. This top-down approach outperforms its bottom-up counterpart in which object bounding box prediction follows per-pixel depth estimation, since it does not suffer from the compounding error introduced by a depth prediction model. Moreover, our method does not require  post-processing such as non-maximum suppression, dramatically improving inference speed. We achieve state-of-the-art performance on the nuScenes autonomous driving benchmark.",,,,,"detr3d: 3d object detection from multi-view images via 3d-to-2d queries multi-camera, 3d object detection, autonomous driving we introduce a framework for multi-camera 3d object detection. in contrast to existing works, which estimate 3d bounding boxes directly from monocular images or use depth prediction networks to generate input for 3d object detection from 2d information, our method manipulates predictions directly in 3d space. our architecture extracts 2d  features from multiple camera images and then uses a sparse set of 3d object queries to index into these 2d features, linking 3d positions to multi-view images using camera transformation matrices. finally, our model makes a bounding box prediction per object query, using a set-to-set loss to measure the discrepancy between the ground-truth and the prediction. this top-down approach outperforms its bottom-up counterpart in which object bounding box prediction follows per-pixel depth estimation, since it does not suffer from the compounding error introduced by a depth prediction model. moreover, our method does not require  post-processing such as non-maximum suppression, dramatically improving inference speed. we achieve state-of-the-art performance on the nuscenes autonomous driving benchmark.",-1.9307669,4.1817365,3d,no reinforce,112,"id:112 (3d)<br><b>DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries</b><br>kw:multi-camera, 3D object detection, autonomous driving"
113,113,347,https://openreview.net/forum?id=CGn3XKSf7vf,Single-Shot Scene Reconstruction,"Sergey Zakharov, Rares Andrei Ambrus, Dennis Park, Vitor Campagnolo Guizilini, Wadim Kehl, Fredo Durand, Joshua B. Tenenbaum, Vincent Sitzmann, Jiajun Wu, Adrien Gaidon",6.0,,,,,,,False,,"scene reconstruction, differentiable rendering","We introduce a novel scene reconstruction method to infer a fully editable and re-renderable model of a 3D road scene from a single image. We represent movable objects separately from the immovable background, and recover a full 3D model of each distinct object as well as their spatial relations in the scene. Based on transformer-based detectors and neural implicit 3D representations, we build a Scene Decomposition Network (SDN) that reconstructs the scene, and the reconstruction can further be used in analysis-by-synthesis via differentiable rendering. Trained only on simulated road scenes, our method generalizes well to real data in the same class without any adaptation thanks to its strong inductive priors. Experiments on two synthetic-real dataset pairs (PD-DDAD and VKITTI-KITTI) show that our method can robustly recover scene geometry and appearance, as well as reconstruct and re-render the scene from novel viewpoints.",,,,,"single-shot scene reconstruction scene reconstruction, differentiable rendering we introduce a novel scene reconstruction method to infer a fully editable and re-renderable model of a 3d road scene from a single image. we represent movable objects separately from the immovable background, and recover a full 3d model of each distinct object as well as their spatial relations in the scene. based on transformer-based detectors and neural implicit 3d representations, we build a scene decomposition network (sdn) that reconstructs the scene, and the reconstruction can further be used in analysis-by-synthesis via differentiable rendering. trained only on simulated road scenes, our method generalizes well to real data in the same class without any adaptation thanks to its strong inductive priors. experiments on two synthetic-real dataset pairs (pd-ddad and vkitti-kitti) show that our method can robustly recover scene geometry and appearance, as well as reconstruct and re-render the scene from novel viewpoints.",-1.6490464,3.968891,scene,no reinforce,113,"id:113 (scene)<br><b>Single-Shot Scene Reconstruction</b><br>kw:scene reconstruction, differentiable rendering"
114,114,377,https://openreview.net/forum?id=CPbn4N3a2zC,Structured deep generative models for sampling on constraint manifolds in sequential manipulation,"Joaquim Ortiz-Haro, Jung-Su Ha, Danny Driess, Marc Toussaint",6.0,,,,,,,False,,"Generative Models, Nonlinear Optimization, Constraint Graph, Robotic Sequential Manipulation","Sampling efficiently on constraint manifolds is a core problem in robotics. We propose Deep Generative Constraint Sampling (DGCS), which combines a deep generative model for sampling close to a constraint manifold with a nonlinear constrained optimization to project onto the constraint manifold. The generative model is conditioned on the problem instance, taking a scene image as input, and it is trained with a dataset of solutions and a novel analytic constraint term. To further improve the precision and diversity of samples, we extend the approach to exploit a factorization of the constrained problem. We evaluate our approach in two problems of robotic sequential manipulation in cluttered environments. Experimental results demonstrate that our deep generative model produces diverse and precise samples and outperforms heuristic warmstart initialization.",,,,,"structured deep generative models for sampling on constraint manifolds in sequential manipulation generative models, nonlinear optimization, constraint graph, robotic sequential manipulation sampling efficiently on constraint manifolds is a core problem in robotics. we propose deep generative constraint sampling (dgcs), which combines a deep generative model for sampling close to a constraint manifold with a nonlinear constrained optimization to project onto the constraint manifold. the generative model is conditioned on the problem instance, taking a scene image as input, and it is trained with a dataset of solutions and a novel analytic constraint term. to further improve the precision and diversity of samples, we extend the approach to exploit a factorization of the constrained problem. we evaluate our approach in two problems of robotic sequential manipulation in cluttered environments. experimental results demonstrate that our deep generative model produces diverse and precise samples and outperforms heuristic warmstart initialization.",-2.1110945,3.3884363,generative,no reinforce,114,"id:114 (generative)<br><b>Structured deep generative models for sampling on constraint manifolds in sequential manipulation</b><br>kw:Generative Models, Nonlinear Optimization, Constraint Graph, Robotic Sequential Manipulation"
115,115,386,https://openreview.net/forum?id=KOq9qDgn-Ta,Correspondence-Free Point Cloud Registration with SO(3)-Equivariant Implicit Shape Representations,"Minghan Zhu, Maani Ghaffari, Huei Peng",6.0,,,,,,,False,,"Deep Learning in Grasping and Manipulation, Computer Vision for Robotic Applications, Imitation Learning, Reinforcement Learning, Learning from Demonstration","Modern Reinforcement Learning (RL) algorithms are not sample efficient to train on multi-step tasks in complex domains, impeding their wider deployment in the real world. We address this problem by leveraging the insight that RL models trained to complete one set of tasks can be repurposed to complete related tasks when given just a handful of demonstrations. Based upon this insight, we propose See-SPOT-Run (SSR), a new computational approach to robot learning that enables a robot to complete a variety of real robot tasks in novel problem domains without task-specific training. SSR uses pretrained RL models to create vectors that represent model, task, and action relevance in demonstration and test scenes. SSR then compares these vectors via our Cycle Consistency Distance (CCD) metric to determine the next action to take. SSR completes 58% more task steps and 20% more trials than a baseline few-shot learning method that requires task-specific training. SSR also achieves a four order of magnitude improvement in compute efficiency and a 20% to three order of magnitude improvement in sample efficiency compared to the baseline and to training RL models from scratch. To our knowledge, we are the first to address multi-step tasks from demonstration on a real robot without task-specific training, where both the visual input and action space output are high dimensional. Code will be made available.",,,,,"correspondence-free point cloud registration with so(3)-equivariant implicit shape representations deep learning in grasping and manipulation, computer vision for robotic applications, imitation learning, reinforcement learning, learning from demonstration modern reinforcement learning (rl) algorithms are not sample efficient to train on multi-step tasks in complex domains, impeding their wider deployment in the real world. we address this problem by leveraging the insight that rl models trained to complete one set of tasks can be repurposed to complete related tasks when given just a handful of demonstrations. based upon this insight, we propose see-spot-run (ssr), a new computational approach to robot learning that enables a robot to complete a variety of real robot tasks in novel problem domains without task-specific training. ssr uses pretrained rl models to create vectors that represent model, task, and action relevance in demonstration and test scenes. ssr then compares these vectors via our cycle consistency distance (ccd) metric to determine the next action to take. ssr completes 58% more task steps and 20% more trials than a baseline few-shot learning method that requires task-specific training. ssr also achieves a four order of magnitude improvement in compute efficiency and a 20% to three order of magnitude improvement in sample efficiency compared to the baseline and to training rl models from scratch. to our knowledge, we are the first to address multi-step tasks from demonstration on a real robot without task-specific training, where both the visual input and action space output are high dimensional. code will be made available.",-3.0080035,3.4609616,ssr,reinforce,115,"id:115 (ssr)<br><b>Correspondence-Free Point Cloud Registration with SO(3)-Equivariant Implicit Shape Representations</b><br>kw:Deep Learning in Grasping and Manipulation, Computer Vision for Robotic Applications, Imitation Learning, Reinforcement Learning, Learning from Demonstration"
116,116,389,https://openreview.net/forum?id=-JwmfQC6IRt,Guided Imitation of Task and Motion Planning,"Michael James McDonald, Dylan Hadfield-Menell",6.0,,,,,,,False,,"task and motion planning, mobile manipulation, imitation learning","While modern policy optimization methods can do complex manipulation from sensor data, they struggle on problems with extended time horizons and multiple sub-goals. On the other hand, task and motion planning (TAMP) methods scale to long horizons but they are computationally expensive and need to precisely track world state. We propose a method that tries to get the best of both worlds: we train a policy to imitate a TAMP solver's output. This produces a feed-forward policy that can accomplish multi-step tasks from sensory data. First, we build an asynchronous distributed TAMP solver that can produce supervision data fast enough for imitation learning. Then, we propose a hierarchical policy architecture that lets us use partially trained control policies to speed up the TAMP solver. In robotic manipulation tasks, the partially trained policies reduce the time needed for planning by a factor of up to 2.5. We show results in a range of pick-place tasks, solving the 4 object pick-place task from RoboSuite 88% of the time from object pose observations and solving the RoboDesk 9-goal multitask benchmark 67% of the time from RGB images.",,,,,"guided imitation of task and motion planning task and motion planning, mobile manipulation, imitation learning while modern policy optimization methods can do complex manipulation from sensor data, they struggle on problems with extended time horizons and multiple sub-goals. on the other hand, task and motion planning (tamp) methods scale to long horizons but they are computationally expensive and need to precisely track world state. we propose a method that tries to get the best of both worlds: we train a policy to imitate a tamp solver's output. this produces a feed-forward policy that can accomplish multi-step tasks from sensory data. first, we build an asynchronous distributed tamp solver that can produce supervision data fast enough for imitation learning. then, we propose a hierarchical policy architecture that lets us use partially trained control policies to speed up the tamp solver. in robotic manipulation tasks, the partially trained policies reduce the time needed for planning by a factor of up to 2.5. we show results in a range of pick-place tasks, solving the 4 object pick-place task from robosuite 88% of the time from object pose observations and solving the robodesk 9-goal multitask benchmark 67% of the time from rgb images.",-2.3539238,3.5813766,tamp,no reinforce,116,"id:116 (tamp)<br><b>Guided Imitation of Task and Motion Planning</b><br>kw:task and motion planning, mobile manipulation, imitation learning"
117,117,411,https://openreview.net/forum?id=lAtePxetBNb,Multi-Agent Trajectory Prediction by Combining Egocentric and Allocentric Views,"Xiaosong Jia, Liting Sun, Hang Zhao, Masayoshi Tomizuka, Wei Zhan",6.0,,,,,,,False,,"Autonomous Driving, Joint Trajectory Prediction, Multi-Agent Interaction","Trajectory prediction of road participants such as vehicles and pedestrians is crucial for autonomous driving. Recently, graph neural network (GNN) is widely adopted to capture the social interactions among the agents. Many GNN-based models formulate the prediction task as a single-agent prediction problem where multiple inference is needed for multi-agent prediction (which is common in practice), which leads to fundamental inconsistency in terms of homotopy as well as inefficiency for the memory and time. Moreover, even for models that do perform joint prediction, typically one centric agent is selected and all other agents’ information is normalized based on that. Such centric-only normalization leads to asymmetric encoding of different agents in GNN, which might harm its performance.  In this work, we propose a efficient multi-agent prediction framework that can predict all agents' trajectories jointly by normalizing and processing all agents' information symmetrically and homogeneously with combined egocentirc and allocentric views. Experiments are conducted on two interaction-rich behavior datasets: INTERACTION (vehicles) and TrajNet++ (pedestrian). The results show that the proposed framework can significantly boost the inference speed of the GNN-based model for multi-agent prediction and achieve better performance. In the INTERACTION dataset's challenge, the proposed model achieved the 1st place in the regular track and generalization track.",,,,,"multi-agent trajectory prediction by combining egocentric and allocentric views autonomous driving, joint trajectory prediction, multi-agent interaction trajectory prediction of road participants such as vehicles and pedestrians is crucial for autonomous driving. recently, graph neural network (gnn) is widely adopted to capture the social interactions among the agents. many gnn-based models formulate the prediction task as a single-agent prediction problem where multiple inference is needed for multi-agent prediction (which is common in practice), which leads to fundamental inconsistency in terms of homotopy as well as inefficiency for the memory and time. moreover, even for models that do perform joint prediction, typically one centric agent is selected and all other agents’ information is normalized based on that. such centric-only normalization leads to asymmetric encoding of different agents in gnn, which might harm its performance.  in this work, we propose a efficient multi-agent prediction framework that can predict all agents' trajectories jointly by normalizing and processing all agents' information symmetrically and homogeneously with combined egocentirc and allocentric views. experiments are conducted on two interaction-rich behavior datasets: interaction (vehicles) and trajnet++ (pedestrian). the results show that the proposed framework can significantly boost the inference speed of the gnn-based model for multi-agent prediction and achieve better performance. in the interaction dataset's challenge, the proposed model achieved the 1st place in the regular track and generalization track.",-3.1456845,3.541419,prediction,no reinforce,117,"id:117 (prediction)<br><b>Multi-Agent Trajectory Prediction by Combining Egocentric and Allocentric Views</b><br>kw:Autonomous Driving, Joint Trajectory Prediction, Multi-Agent Interaction"
118,118,30,https://openreview.net/forum?id=9aVCUv3nKBg,Adversarially Robust Imitation Learning,"Jianren Wang, Ziwen Zhuang, Yuyang Wang, Hang Zhao",7.0,,,,,,,False,,"Imitation Learning, Adversarial Learning","Modern imitation learning (IL) utilizes deep neural networks (DNNs) as function approximators to mimic the policy of the expert demonstrations. However, DNNs can be easily fooled by subtle noise added to the input, which is even non-detectable by humans. This makes the learned agent vulnerable to attacks, especially in IL where agents can struggle to recover from the errors. In such light, we propose a sound Adversarially Robust Imitation Learning (ARIL) method. In our setting, an agent and an adversary are trained alternatively. The former with adversarially attacked input at each timestep mimics the behavior of an online expert and the latter learns to add perturbations on the states by forcing the learned agent to fail on choosing the right decisions. We theoretically prove that ARIL can achieve adversarial robustness and evaluate ARIL on multiple benchmarks from DM Control Suite. The result reveals that our method (ARIL) achieves better robustness compare with other imitation learning methods under both sensory attack and physical attack.",,,,,"adversarially robust imitation learning imitation learning, adversarial learning modern imitation learning (il) utilizes deep neural networks (dnns) as function approximators to mimic the policy of the expert demonstrations. however, dnns can be easily fooled by subtle noise added to the input, which is even non-detectable by humans. this makes the learned agent vulnerable to attacks, especially in il where agents can struggle to recover from the errors. in such light, we propose a sound adversarially robust imitation learning (aril) method. in our setting, an agent and an adversary are trained alternatively. the former with adversarially attacked input at each timestep mimics the behavior of an online expert and the latter learns to add perturbations on the states by forcing the learned agent to fail on choosing the right decisions. we theoretically prove that aril can achieve adversarial robustness and evaluate aril on multiple benchmarks from dm control suite. the result reveals that our method (aril) achieves better robustness compare with other imitation learning methods under both sensory attack and physical attack.",-2.930077,3.5166209,adversarially,no reinforce,118,"id:118 (adversarially)<br><b>Adversarially Robust Imitation Learning</b><br>kw:Imitation Learning, Adversarial Learning"
119,119,71,https://openreview.net/forum?id=bEito8UUUmf,Probabilistic and Geometric Depth: Detecting Objects in Perspective,"Tai Wang, Xinge ZHU, Jiangmiao Pang, Dahua Lin",7.0,,,,,,,False,,"Probabilistic and Geometric Depth, Monocular 3D Detection","3D object detection is an important capability needed in various practical applications such as driver assistance systems. Monocular 3D detection, as an economical solution compared to conventional settings relying on binocular vision or LiDAR, has drawn increasing attention recently but still yields unsatisfactory results. This paper first presents a systematic study on this problem. We observe that the current monocular 3D detection can be simplified as an instance depth estimation problem: The inaccurate instance depth blocks all the other 3D attribute predictions from improving the overall detection performance. However, recent methods directly estimate the depth based on isolated instances or pixels while ignoring the geometric relations across different objects. These geometric relations can be valuable constraints as the key information about depth is not directly manifest in the monocular image. Therefore, we construct geometric relation graphs across predicted objects and use the graph to facilitate depth estimation. As the preliminary depth estimation of each instance is usually inaccurate in this ill-posed setting, we incorporate a probabilistic representation to capture the uncertainty. It provides an important indicator to identify confident predictions and further guide the depth propagation. Despite the simplicity of the basic idea, our method obtains significant improvements on KITTI and nuScenes benchmarks, achieving 1st place out of all monocular vision-only methods while still maintaining real-time efficiency.",,,,,"probabilistic and geometric depth: detecting objects in perspective probabilistic and geometric depth, monocular 3d detection 3d object detection is an important capability needed in various practical applications such as driver assistance systems. monocular 3d detection, as an economical solution compared to conventional settings relying on binocular vision or lidar, has drawn increasing attention recently but still yields unsatisfactory results. this paper first presents a systematic study on this problem. we observe that the current monocular 3d detection can be simplified as an instance depth estimation problem: the inaccurate instance depth blocks all the other 3d attribute predictions from improving the overall detection performance. however, recent methods directly estimate the depth based on isolated instances or pixels while ignoring the geometric relations across different objects. these geometric relations can be valuable constraints as the key information about depth is not directly manifest in the monocular image. therefore, we construct geometric relation graphs across predicted objects and use the graph to facilitate depth estimation. as the preliminary depth estimation of each instance is usually inaccurate in this ill-posed setting, we incorporate a probabilistic representation to capture the uncertainty. it provides an important indicator to identify confident predictions and further guide the depth propagation. despite the simplicity of the basic idea, our method obtains significant improvements on kitti and nuscenes benchmarks, achieving 1st place out of all monocular vision-only methods while still maintaining real-time efficiency.",-2.091547,4.5031815,geometric,no reinforce,119,"id:119 (geometric)<br><b>Probabilistic and Geometric Depth: Detecting Objects in Perspective</b><br>kw:Probabilistic and Geometric Depth, Monocular 3D Detection"
120,120,106,https://openreview.net/forum?id=5P_3bRWiRsF,RoCUS: Robot Controller Understanding via Sampling,"Yilun Zhou, Serena Booth, Nadia Figueroa, Julie Shah",7.0,,,,,,,False,,"debugging and evaluation, algorithmic transparency","As robots are deployed in complex situations, engineers and end users must develop a holistic understanding of their behaviors, capabilities, and limitations. Some behaviors are directly optimized by the objective function. They often include success rate, completion time or energy consumption. Other behaviors -- e.g., collision avoidance, trajectory smoothness or motion legibility -- are typically emergent but equally important for safe and trustworthy deployment. Designing an objective which optimizes every aspect of robot behavior is hard. In this paper, we advocate for systematic analysis of a wide array of behaviors for holistic understanding of robot controllers and, to this end, propose a framework, RoCUS, which uses Bayesian posterior sampling to find situations where the robot controller exhibits user-specified behaviors, such as highly jerky motions. We use RoCUS to analyze three controller classes (deep learning models, rapidly exploring random trees and dynamical system formulations) on two domains (2D navigation and a 7 degree-of-freedom arm reaching), and uncover insights to further our understanding of these controllers and ultimately improve their designs.",,,,,"rocus: robot controller understanding via sampling debugging and evaluation, algorithmic transparency as robots are deployed in complex situations, engineers and end users must develop a holistic understanding of their behaviors, capabilities, and limitations. some behaviors are directly optimized by the objective function. they often include success rate, completion time or energy consumption. other behaviors -- e.g., collision avoidance, trajectory smoothness or motion legibility -- are typically emergent but equally important for safe and trustworthy deployment. designing an objective which optimizes every aspect of robot behavior is hard. in this paper, we advocate for systematic analysis of a wide array of behaviors for holistic understanding of robot controllers and, to this end, propose a framework, rocus, which uses bayesian posterior sampling to find situations where the robot controller exhibits user-specified behaviors, such as highly jerky motions. we use rocus to analyze three controller classes (deep learning models, rapidly exploring random trees and dynamical system formulations) on two domains (2d navigation and a 7 degree-of-freedom arm reaching), and uncover insights to further our understanding of these controllers and ultimately improve their designs.",-1.9375551,4.331913,understanding,no reinforce,120,"id:120 (understanding)<br><b>RoCUS: Robot Controller Understanding via Sampling</b><br>kw:debugging and evaluation, algorithmic transparency"
121,121,114,https://openreview.net/forum?id=50523z0PALg,Using Model Knowledge for Learning Rigid-Body Forward Dynamics with Gaussian processes,"Lucas Rath, Andreas Ren? Geist, Sebastian Trimpe",7.0,,,,,,,False,,"Machine Learning, Robotics, Analytical mechanics","If a robot’s dynamics are difficult to model solely through analytical mechanics, it is an attractive option to directly learn it from data. Yet, solely data-driven approaches require considerable amounts of data for training and do not extrapolate well to unseen regions of the system’s state space. In this work, we emphasize that when a robot’s links are sufficiently rigid, many analytical functions such as kinematics, inertia functions, and surface constraints encode informative prior knowledge on its dynamics. To this effect, we propose a framework for learning probabilistic forward dynamics that combines analytical knowledge with Gaussian processes utilizing automatic differentiation with GPU acceleration. Compared to solely data-driven modeling, the model’s data efficiency improves while the model also respects physical constraints. We illustrate the proposed structured model on a seven joint robot arm in PyBullet. An efficient  implementation of the framework is enclosed to the supplementary material.",,,,,"using model knowledge for learning rigid-body forward dynamics with gaussian processes machine learning, robotics, analytical mechanics if a robot’s dynamics are difficult to model solely through analytical mechanics, it is an attractive option to directly learn it from data. yet, solely data-driven approaches require considerable amounts of data for training and do not extrapolate well to unseen regions of the system’s state space. in this work, we emphasize that when a robot’s links are sufficiently rigid, many analytical functions such as kinematics, inertia functions, and surface constraints encode informative prior knowledge on its dynamics. to this effect, we propose a framework for learning probabilistic forward dynamics that combines analytical knowledge with gaussian processes utilizing automatic differentiation with gpu acceleration. compared to solely data-driven modeling, the model’s data efficiency improves while the model also respects physical constraints. we illustrate the proposed structured model on a seven joint robot arm in pybullet. an efficient  implementation of the framework is enclosed to the supplementary material.",-1.173825,4.6587024,analytical,no reinforce,121,"id:121 (analytical)<br><b>Using Model Knowledge for Learning Rigid-Body Forward Dynamics with Gaussian processes</b><br>kw:Machine Learning, Robotics, Analytical mechanics"
122,122,128,https://openreview.net/forum?id=8xC5NNej-l_,S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning in Robotics,"Samarth Sinha, Ajay Mandlekar, Animesh Garg",7.0,,,,,,,False,,"Offline Reinforcement Learning, Data Augmentation, Self-Supervised Learning","Offline reinforcement learning proposes to learn policies from large collected datasets without interacting with the physical environment.  These algorithms have made it possible to learn useful skills from data that can then be deployed in the environment in real-world settings where interactions may be costly or dangerous, such as autonomous driving or factories.  However, offline agents are unable to access the environment to collect new data, and therefore are trained on a static dataset.  In this paper, we study the effectiveness of performing data augmentations on the state space, and study 7 different augmentation schemes and how they behave with existing offline RL algorithms. We then combine the best data performing augmentation scheme with a state-of-the-art Q-learning technique, and improve the function approximation of the Q-networks by smoothening out the learned state-action space. We experimentally show that using this Surprisingly Simple Self-Supervision technique in RL (S4RL), we significantly improve over the current state-of-the-art algorithms on offline robot learning environments such as MetaWorld [1] and RoboSuite [2,3], and benchmark datasets such as D4RL [4].",,,,,"s4rl: surprisingly simple self-supervision for offline reinforcement learning in robotics offline reinforcement learning, data augmentation, self-supervised learning offline reinforcement learning proposes to learn policies from large collected datasets without interacting with the physical environment.  these algorithms have made it possible to learn useful skills from data that can then be deployed in the environment in real-world settings where interactions may be costly or dangerous, such as autonomous driving or factories.  however, offline agents are unable to access the environment to collect new data, and therefore are trained on a static dataset.  in this paper, we study the effectiveness of performing data augmentations on the state space, and study 7 different augmentation schemes and how they behave with existing offline rl algorithms. we then combine the best data performing augmentation scheme with a state-of-the-art q-learning technique, and improve the function approximation of the q-networks by smoothening out the learned state-action space. we experimentally show that using this surprisingly simple self-supervision technique in rl (s4rl), we significantly improve over the current state-of-the-art algorithms on offline robot learning environments such as metaworld [1] and robosuite [2,3], and benchmark datasets such as d4rl [4].",-2.6462688,3.395272,offline,reinforce,122,"id:122 (offline)<br><b>S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning in Robotics</b><br>kw:Offline Reinforcement Learning, Data Augmentation, Self-Supervised Learning"
123,123,136,https://openreview.net/forum?id=FCoh4OLZ1Gg,Embodied Semantic Scene Graph Generation,"Xinghang Li, Di Guo, Huaping Liu, Fuchun Sun",7.0,,,,,,,False,,"Semantic Scene Graph, Embodied Exploration, Learning for Visual Navigation","Semantic scene graph provides an effective way for intelligent agents to better understand the environment and it has been extensively used in many robotic applications. Existing work mainly focuses on generating the scene graph from the sensory information collected from a pre-defined path, while the environment should be exhaustively explored with a carefully designed path in order to obtain a comprehensive semantic scene graph efficiently. In this paper, we propose a new task of Embodied Semantic Scene Graph Generation, which exploits the embodiment of the intelligent agent to autonomously generate an appropriate path to explore the environment for scene graph generation. To this end, a learning framework with the paradigms of imitation learning and reinforcement learning is proposed to help the agent generate proper actions to explore the environment and the scene graph is incrementally constructed. The proposed method is evaluated on the AI2Thor environment using both the quantitative and qualitative performance indexes. Additionally, we implement the proposed method on a streaming video captioning task and promising experimental results are achieved.",,,,,"embodied semantic scene graph generation semantic scene graph, embodied exploration, learning for visual navigation semantic scene graph provides an effective way for intelligent agents to better understand the environment and it has been extensively used in many robotic applications. existing work mainly focuses on generating the scene graph from the sensory information collected from a pre-defined path, while the environment should be exhaustively explored with a carefully designed path in order to obtain a comprehensive semantic scene graph efficiently. in this paper, we propose a new task of embodied semantic scene graph generation, which exploits the embodiment of the intelligent agent to autonomously generate an appropriate path to explore the environment for scene graph generation. to this end, a learning framework with the paradigms of imitation learning and reinforcement learning is proposed to help the agent generate proper actions to explore the environment and the scene graph is incrementally constructed. the proposed method is evaluated on the ai2thor environment using both the quantitative and qualitative performance indexes. additionally, we implement the proposed method on a streaming video captioning task and promising experimental results are achieved.",-1.8824153,3.80851,scene,reinforce,123,"id:123 (scene)<br><b>Embodied Semantic Scene Graph Generation</b><br>kw:Semantic Scene Graph, Embodied Exploration, Learning for Visual Navigation"
124,124,165,https://openreview.net/forum?id=p9Pe-l9MMEq,Scaling Up Multi-Task Robotic Reinforcement Learning,"Dmitry Kalashnikov, Jake Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski, Chelsea Finn, Sergey Levine, Karol Hausman",7.0,,,,,,,False,,--,--,,,,,scaling up multi-task robotic reinforcement learning -- --,-0.6320718,0.15684012,scaling,reinforce,124,id:124 (scaling)<br><b>Scaling Up Multi-Task Robotic Reinforcement Learning</b><br>kw:--
125,125,169,https://openreview.net/forum?id=GIgsuWifgIi,You Only Evaluate Once: a Simple Baseline Algorithm for Offline RL,"Wonjoon Goo, Scott Niekum",7.0,,,,,,,False,,"offline reinforcement learning, conservative policy evaluation","The goal of offline reinforcement learning (RL) is to find an optimal policy given prerecorded trajectories. Many current approaches customize existing off-policy RL algorithms, especially actor-critic algorithms in which policy evaluation and improvement are iterated. However, the convergence of such approaches is not guaranteed due to the use of complex non-linear function approximation and an intertwined optimization process.        By contrast, we propose a simple baseline algorithm for offline RL that only performs the policy evaluation step once so that the algorithm does not require complex stabilization schemes. Since the proposed algorithm is not likely to converge to an optimal policy, it is an appropriate baseline for actor-critic algorithms that ought to be outperformed if there is indeed value in iterative optimization in the offline setting. Surprisingly, we empirically find that the proposed algorithm exhibits competitive and sometimes even state-of-the-art performance in a subset of the D4RL offline RL benchmark. This result suggests that future work is needed to fully exploit the potential advantages of iterative optimization in order to justify the reduced stability of such methods.",,,,,"you only evaluate once: a simple baseline algorithm for offline rl offline reinforcement learning, conservative policy evaluation the goal of offline reinforcement learning (rl) is to find an optimal policy given prerecorded trajectories. many current approaches customize existing off-policy rl algorithms, especially actor-critic algorithms in which policy evaluation and improvement are iterated. however, the convergence of such approaches is not guaranteed due to the use of complex non-linear function approximation and an intertwined optimization process.        by contrast, we propose a simple baseline algorithm for offline rl that only performs the policy evaluation step once so that the algorithm does not require complex stabilization schemes. since the proposed algorithm is not likely to converge to an optimal policy, it is an appropriate baseline for actor-critic algorithms that ought to be outperformed if there is indeed value in iterative optimization in the offline setting. surprisingly, we empirically find that the proposed algorithm exhibits competitive and sometimes even state-of-the-art performance in a subset of the d4rl offline rl benchmark. this result suggests that future work is needed to fully exploit the potential advantages of iterative optimization in order to justify the reduced stability of such methods.",-3.4943638,3.3961062,offline,reinforce,125,"id:125 (offline)<br><b>You Only Evaluate Once: a Simple Baseline Algorithm for Offline RL</b><br>kw:offline reinforcement learning, conservative policy evaluation"
126,126,178,https://openreview.net/forum?id=tjdXRqKaz5Y,Aligning an optical interferometer with beam divergence control and continuous action space,"Stepan Makarenko, Dmitry Igorevich Sorokin, Alexander Ulanov, Alexander Lvovsky",7.0,,,,,,,False,,"sim-to-real, robotics, optical interferometer","Reinforcement learning is finding its way to real-world problem application, transferring from simulated environments to physical setups. In this work, we implement vision-based alignment of an optical Mach-Zehnder interferometer with a confocal telescope in one arm, which controls the diameter and divergence of the corresponding beam.  We use a continuous action space; exponential scaling enables us to handle actions within a range of over two orders of magnitude. Our agent trains only in a simulated environment with domain randomizations. In an experimental evaluation, the agent significantly outperforms an existing solution and a human expert.",,,,,"aligning an optical interferometer with beam divergence control and continuous action space sim-to-real, robotics, optical interferometer reinforcement learning is finding its way to real-world problem application, transferring from simulated environments to physical setups. in this work, we implement vision-based alignment of an optical mach-zehnder interferometer with a confocal telescope in one arm, which controls the diameter and divergence of the corresponding beam.  we use a continuous action space; exponential scaling enables us to handle actions within a range of over two orders of magnitude. our agent trains only in a simulated environment with domain randomizations. in an experimental evaluation, the agent significantly outperforms an existing solution and a human expert.",-2.1301332,3.167691,interferometer,reinforce,126,"id:126 (interferometer)<br><b>Aligning an optical interferometer with beam divergence control and continuous action space</b><br>kw:sim-to-real, robotics, optical interferometer"
127,127,190,https://openreview.net/forum?id=j3Rguo81Yi_,Specializing Versatile Skill Libraries using Local Mixture of Experts,"Onur Celik, Dongzhuoran Zhou, Ge Li, Philipp Becker, Gerhard Neumann",7.0,,,,,,,False,,"Episodic Policy Search, Versatile Skill Learning, Hierarchical RL, Curriculum Learning","A long-cherished vision in robotics is to equip robots with skills that match the versatility and precision of humans.        For example, when playing table tennis, a robot should be capable of returning the ball in various ways while precisely placing it at a desired location.         A common approach to model such versatile behavior is to use a Mixture of Experts (MoE) model, where each expert is a contextual motion primitive.        However, learning such MoEs is challenging as most objectives force the model to cover the entire context spaces, which prevents specialization of the primitives resulting in rather low quality components.         Starting from maximum entropy reinforcement learning (RL), we decompose the objective into optimizing an individual lower bound per mixture component.        Further, we introduce a curriculum by allowing the components to focus        on a local context region, enabling the model to learn highly accurate skill representations.        To this end, we use local context distributions that are adapted jointly with the expert primitives. Our lower bound advocates an iterative addition of new components, where new components will concentrate on local context regions not covered by the current MoE.        This local and incremental learning results in a modular MoE model of high accuracy and versatility, where both properties can be scaled by adding more components on the fly.         We demonstrate this by an extensive ablation and on two challenging simulated robot skill learning tasks. We compare our achieved performance to HiREPS, a known hierarchical policy search method for learning diverse skills and LaDiPS.",,,,,"specializing versatile skill libraries using local mixture of experts episodic policy search, versatile skill learning, hierarchical rl, curriculum learning a long-cherished vision in robotics is to equip robots with skills that match the versatility and precision of humans.        for example, when playing table tennis, a robot should be capable of returning the ball in various ways while precisely placing it at a desired location.         a common approach to model such versatile behavior is to use a mixture of experts (moe) model, where each expert is a contextual motion primitive.        however, learning such moes is challenging as most objectives force the model to cover the entire context spaces, which prevents specialization of the primitives resulting in rather low quality components.         starting from maximum entropy reinforcement learning (rl), we decompose the objective into optimizing an individual lower bound per mixture component.        further, we introduce a curriculum by allowing the components to focus        on a local context region, enabling the model to learn highly accurate skill representations.        to this end, we use local context distributions that are adapted jointly with the expert primitives. our lower bound advocates an iterative addition of new components, where new components will concentrate on local context regions not covered by the current moe.        this local and incremental learning results in a modular moe model of high accuracy and versatility, where both properties can be scaled by adding more components on the fly.         we demonstrate this by an extensive ablation and on two challenging simulated robot skill learning tasks. we compare our achieved performance to hireps, a known hierarchical policy search method for learning diverse skills and ladips.",-2.2856085,4.2739124,local,reinforce,127,"id:127 (local)<br><b>Specializing Versatile Skill Libraries using Local Mixture of Experts</b><br>kw:Episodic Policy Search, Versatile Skill Learning, Hierarchical RL, Curriculum Learning"
128,128,236,https://openreview.net/forum?id=IScz42A3iCI,Equivariant $Q$ Learning in Spatial Action Spaces,"Dian Wang, Robin Walters, Xupeng Zhu, Robert Platt",7.0,,,,,,,False,,"Reinforcement Learning, Equivariance, Manipulation, function is equivariant and proposes","Recently, a variety of new equivariant neural network model architectures have been proposed that generalize better over rotational and reflectional symmetries than standard models. These models are relevant to robotics because many robotics problems can be expressed in a rotationally symmetric way. This paper focuses on equivariance over a visual state space and a spatial action space -- the setting where the robot action space includes a subset of  . In this situation, we know a priori that rotations and translations in the state image should result in the same rotations and translations in the spatial action dimensions of the optimal policy. Therefore, we can use equivariant model architectures to make  learning more sample efficient. This paper identifies when the optimal network architectures for this setting. We show experimentally that this approach outperforms standard methods in a set of challenging manipulation problems.",,,,,"equivariant $q$ learning in spatial action spaces reinforcement learning, equivariance, manipulation, function is equivariant and proposes recently, a variety of new equivariant neural network model architectures have been proposed that generalize better over rotational and reflectional symmetries than standard models. these models are relevant to robotics because many robotics problems can be expressed in a rotationally symmetric way. this paper focuses on equivariance over a visual state space and a spatial action space -- the setting where the robot action space includes a subset of  . in this situation, we know a priori that rotations and translations in the state image should result in the same rotations and translations in the spatial action dimensions of the optimal policy. therefore, we can use equivariant model architectures to make  learning more sample efficient. this paper identifies when the optimal network architectures for this setting. we show experimentally that this approach outperforms standard methods in a set of challenging manipulation problems.",-1.4053328,4.211267,equivariant,reinforce,128,"id:128 (equivariant)<br><b>Equivariant $Q$ Learning in Spatial Action Spaces</b><br>kw:Reinforcement Learning, Equivariance, Manipulation, function is equivariant and proposes"
129,129,274,https://openreview.net/forum?id=ftOqDUeLPn3,Dealing with the Unknown: Pessimistic Offline Reinforcement Learning,"Jinning Li, Chen Tang, Masayoshi Tomizuka, Wei Zhan",7.0,,,,,,,False,,"Offline Reinforcement Learning, Out-of-Distribution States","Reinforcement Learning (RL) has been shown effective in domains where the agent can learn policies by actively interacting with its operating environment. However, if we change the RL scheme to offline setting where the agent can only update its policy via static datasets, one of the major issues in offline reinforcement learning emerges, i.e. distributional shift. We propose a Pessimistic Offline Reinforcement Learning (PessORL) algorithm to actively lead the agent back to the area where it is familiar by manipulating the value function. We focus on problems caused by out-of-distribution (OOD) states, and deliberately penalize high values at states that are absent in the training dataset, so that the learned pessimistic value function lower bounds the true value anywhere within the state space. We evaluate the PessORL algorithm on various benchmark tasks, where we show that our method gains better performance by explicitly handling OOD states, when compared to those methods merely considering OOD actions.",,,,,"dealing with the unknown: pessimistic offline reinforcement learning offline reinforcement learning, out-of-distribution states reinforcement learning (rl) has been shown effective in domains where the agent can learn policies by actively interacting with its operating environment. however, if we change the rl scheme to offline setting where the agent can only update its policy via static datasets, one of the major issues in offline reinforcement learning emerges, i.e. distributional shift. we propose a pessimistic offline reinforcement learning (pessorl) algorithm to actively lead the agent back to the area where it is familiar by manipulating the value function. we focus on problems caused by out-of-distribution (ood) states, and deliberately penalize high values at states that are absent in the training dataset, so that the learned pessimistic value function lower bounds the true value anywhere within the state space. we evaluate the pessorl algorithm on various benchmark tasks, where we show that our method gains better performance by explicitly handling ood states, when compared to those methods merely considering ood actions.",-2.7654858,3.6697352,pessimistic,reinforce,129,"id:129 (pessimistic)<br><b>Dealing with the Unknown: Pessimistic Offline Reinforcement Learning</b><br>kw:Offline Reinforcement Learning, Out-of-Distribution States"
130,130,334,https://openreview.net/forum?id=8kbp23tSGYv,BC-0: Zero-Shot Task Generalization with Robotic Imitation Learning,"Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, Chelsea Finn",7.0,,,,,,,False,,"Zero-Shot Imitation Learning, One-Shot Imitation Learning, Multi-Task Imitation, Deep Learning","In this paper, we study the problem of enabling a vision-based robotic manipulation system to generalize to novel tasks, a long-standing challenge in robot learning. We approach the challenge from an imitation learning perspective, aiming to study how scaling and broadening the data collected can facilitate such generalization. To that end, we develop an interactive and flexible imitation learning system that can learn from both demonstrations and interventions and can be conditioned on different forms of information that convey the task, including pre-trained embeddings of natural language or videos of humans performing the task. When scaling data collection on a real robot to more than 100 distinct tasks, we find that this system can perform 21 unseen manipulation tasks with an average success rate of 44%, without any robot demonstrations for those tasks.",,,,,"bc-0: zero-shot task generalization with robotic imitation learning zero-shot imitation learning, one-shot imitation learning, multi-task imitation, deep learning in this paper, we study the problem of enabling a vision-based robotic manipulation system to generalize to novel tasks, a long-standing challenge in robot learning. we approach the challenge from an imitation learning perspective, aiming to study how scaling and broadening the data collected can facilitate such generalization. to that end, we develop an interactive and flexible imitation learning system that can learn from both demonstrations and interventions and can be conditioned on different forms of information that convey the task, including pre-trained embeddings of natural language or videos of humans performing the task. when scaling data collection on a real robot to more than 100 distinct tasks, we find that this system can perform 21 unseen manipulation tasks with an average success rate of 44%, without any robot demonstrations for those tasks.",-1.410674,3.3751562,imitation,no reinforce,130,"id:130 (imitation)<br><b>BC-0: Zero-Shot Task Generalization with Robotic Imitation Learning</b><br>kw:Zero-Shot Imitation Learning, One-Shot Imitation Learning, Multi-Task Imitation, Deep Learning"
131,131,391,https://openreview.net/forum?id=3HZLte8gMYS,Social Interactions as Recursive MDPs,"Ravi Tejwani, Yen-Ling Kuo, Tianmin Shu, Boris Katz, Andrei Barbu",7.0,,,,,,,False,,,"While machines and robots must interact with humans, providing them with social skills has been a largely overlooked topic. This is mostly a consequence of the fact that tasks such as navigation, command following, and even game playing are well-defined, while social reasoning still mostly remains a pre-theoretic problem. We demonstrate how social interactions can be effectively incorporated into MDPs by reasoning recursively about the goals of other agents. In essence, our method extends the reward function to include a combination of physical goals (something agents want to accomplish in the configuration space, a traditional MDP) and social goals (something agents want to accomplish relative to the goals of other agents). Our Social MDPs allow specifying reward functions in terms of the estimated reward functions of other agents, modeling interactions such as helping or hindering another agent (by maximizing or minimizing the other agent's reward) while balancing this with the actual physical goals of each agent. Our formulation allows for an arbitrary function of another agent's estimated reward structure and physical goals, enabling more complex behaviors such as politely hindering another agent or aggressively helping them. Extending Social MDPs in the same manner as I-POMDPs extension would enable interactions such as convincing another agent that something is true. To what extent the Social MDPs presented here and their potential Social POMDPs variant account for all possible social interactions is unknown, but having a precise mathematical model to guide questions about social interactions has both practical value (we demonstrate how to make zero-shot social inferences and one could imagine chatbots and robots guided by Social MDPs) and theoretical value by bringing the tools of MDP that have so successfully organized research around navigation to hopefully shed light on what social interactions really are given their extreme importance to human well-being and human civilization.",,,,,"social interactions as recursive mdps nan while machines and robots must interact with humans, providing them with social skills has been a largely overlooked topic. this is mostly a consequence of the fact that tasks such as navigation, command following, and even game playing are well-defined, while social reasoning still mostly remains a pre-theoretic problem. we demonstrate how social interactions can be effectively incorporated into mdps by reasoning recursively about the goals of other agents. in essence, our method extends the reward function to include a combination of physical goals (something agents want to accomplish in the configuration space, a traditional mdp) and social goals (something agents want to accomplish relative to the goals of other agents). our social mdps allow specifying reward functions in terms of the estimated reward functions of other agents, modeling interactions such as helping or hindering another agent (by maximizing or minimizing the other agent's reward) while balancing this with the actual physical goals of each agent. our formulation allows for an arbitrary function of another agent's estimated reward structure and physical goals, enabling more complex behaviors such as politely hindering another agent or aggressively helping them. extending social mdps in the same manner as i-pomdps extension would enable interactions such as convincing another agent that something is true. to what extent the social mdps presented here and their potential social pomdps variant account for all possible social interactions is unknown, but having a precise mathematical model to guide questions about social interactions has both practical value (we demonstrate how to make zero-shot social inferences and one could imagine chatbots and robots guided by social mdps) and theoretical value by bringing the tools of mdp that have so successfully organized research around navigation to hopefully shed light on what social interactions really are given their extreme importance to human well-being and human civilization.",-3.0603447,4.310833,social,no reinforce,131,id:131 (social)<br><b>Social Interactions as Recursive MDPs</b><br>kw:nan
132,132,398,https://openreview.net/forum?id=_lkBGOctkip,LILA: Language-Informed Latent Actions,"Siddharth Karamcheti, Megha Srivastava, Percy Liang, Dorsa Sadigh",7.0,,,,,,,False,,"Language for Shared Autonomy, Language & Robotics, Learned Latent Actions, Human-Robot Interaction","We introduce Language-Informed Latent Actions (LILA), a framework for learning natural language interfaces in the context of human-robot collaboration. LILA falls under the shared autonomy paradigm: in addition to providing discrete language inputs, humans are given a low-dimensional controller -- e.g., a 2 degree-of-freedom (DoF) joystick that moves left/right or up/down -- for operating the robot. LILA learns to use language to modulate this controller, providing users with a control space that adapts based on the provided language: given an instruction like “place the cereal bowl on the tray,” LILA might learn a 2-DoF control space where one dimension controls the distance from the robot's end-effector to the bowl, and the other dimension controls the robot's end-effector pose relative to the grasp point on the bowl. We evaluate LILA with real-world user studies pairing users with a 7-DoF Franka Emika Panda Arm, with the goal of completing a series of complex manipulation tasks with a lower dimensional control space conditioned on language. LILA models are not only more sample efficient and performant than imitation learning and end-effector control baselines, but also are qualitatively preferred by users.",,,,,"lila: language-informed latent actions language for shared autonomy, language & robotics, learned latent actions, human-robot interaction we introduce language-informed latent actions (lila), a framework for learning natural language interfaces in the context of human-robot collaboration. lila falls under the shared autonomy paradigm: in addition to providing discrete language inputs, humans are given a low-dimensional controller -- e.g., a 2 degree-of-freedom (dof) joystick that moves left/right or up/down -- for operating the robot. lila learns to use language to modulate this controller, providing users with a control space that adapts based on the provided language: given an instruction like “place the cereal bowl on the tray,” lila might learn a 2-dof control space where one dimension controls the distance from the robot's end-effector to the bowl, and the other dimension controls the robot's end-effector pose relative to the grasp point on the bowl. we evaluate lila with real-world user studies pairing users with a 7-dof franka emika panda arm, with the goal of completing a series of complex manipulation tasks with a lower dimensional control space conditioned on language. lila models are not only more sample efficient and performant than imitation learning and end-effector control baselines, but also are qualitatively preferred by users.",-2.4308884,3.9354613,lila,no reinforce,132,"id:132 (lila)<br><b>LILA: Language-Informed Latent Actions</b><br>kw:Language for Shared Autonomy, Language & Robotics, Learned Latent Actions, Human-Robot Interaction"
133,133,409,https://openreview.net/forum?id=WFqkGvBy78,Parallelised Diffeomorphic Sampling-based Motion Planning,"Tin Lai, Weiming Zhi, Tucker Hermans, Fabio Ramos",7.0,,,,,,,False,,"Sampling-based motion planning, diffeomorphism, Normalising flows, Sampling distribution, RRT, PRM","We propose Parallelised Diffeomorphic Sampling-based Motion Planning (PDMP). PDMP is a novel parallelised framework that uses bijective and differentiable mappings, or diffeomorphisms, to transform sampling distributions of sampling-based motion planners, in a manner akin to normalising flows. Unlike normalising flow models which use invertible neural network structures to represent these diffeomorphisms, we develop them from gradient information of desired costs, and encode desirable behaviour, such as obstacle avoidance. These transformed sampling distributions can then be used for sampling-based motion planning. A particular example is when we wish to imbue the sampling distribution with knowledge of the environment geometry, such that drawn samples are less prone to be in collision. To this end, we propose to learn a continuous occupancy representation from environment occupancy data, such that gradients of the representation defines a valid diffeomorphism and is amenable to fast parallelise evaluation. We use this to ``morph'' the sampling distribution to draw far less collision-prone samples. PDMP is able to leverage gradient information of costs, to inject specifications, in a manner similar to optimisation-based motion planning methods, but relies on drawing from a sampling distribution, retaining the tendency to find more global solutions, thereby bridging the gap between trajectory optimisation and sampling-based planning methods.",,,,,"parallelised diffeomorphic sampling-based motion planning sampling-based motion planning, diffeomorphism, normalising flows, sampling distribution, rrt, prm we propose parallelised diffeomorphic sampling-based motion planning (pdmp). pdmp is a novel parallelised framework that uses bijective and differentiable mappings, or diffeomorphisms, to transform sampling distributions of sampling-based motion planners, in a manner akin to normalising flows. unlike normalising flow models which use invertible neural network structures to represent these diffeomorphisms, we develop them from gradient information of desired costs, and encode desirable behaviour, such as obstacle avoidance. these transformed sampling distributions can then be used for sampling-based motion planning. a particular example is when we wish to imbue the sampling distribution with knowledge of the environment geometry, such that drawn samples are less prone to be in collision. to this end, we propose to learn a continuous occupancy representation from environment occupancy data, such that gradients of the representation defines a valid diffeomorphism and is amenable to fast parallelise evaluation. we use this to ``morph'' the sampling distribution to draw far less collision-prone samples. pdmp is able to leverage gradient information of costs, to inject specifications, in a manner similar to optimisation-based motion planning methods, but relies on drawing from a sampling distribution, retaining the tendency to find more global solutions, thereby bridging the gap between trajectory optimisation and sampling-based planning methods.",-1.8518621,4.7697315,sampling-based,no reinforce,133,"id:133 (sampling-based)<br><b>Parallelised Diffeomorphic Sampling-based Motion Planning</b><br>kw:Sampling-based motion planning, diffeomorphism, Normalising flows, Sampling distribution, RRT, PRM"
134,134,380,https://openreview.net/forum?id=HTfApPeT4DZ,Behavior Varied Adversarial Pedestrian Generation for Autonomous Vehicle Testing,"Maria Priisalu, Aleksis Pirinen, Ciprian Paduraru, Cristian Sminchisescu",7.0,,,,,,,False,,"Autonomous Vehicles, AV Testing, Reinforcement Learning","There exist several datasets for developing self-driving car methodologies. However, manually collected datasets impose inherent limitations on the variability of test cases and it is particularly difficult to acquire challenging scenarios, e.g. ones which involve collisions with pedestrians. Automatically synthesizing safety-critical scenarios for autonomous vehicles (AVs) can vastly increase the amount of relevant data for training and evaluating such AVs. However, existing approaches for scenario generation rely on handcrafted pedestrian behavior models, which limits the realism of the corresponding scenarios. In this paper we reformulate the problem as learning \emph{where to place} pedestrians such that the induced scenarios are collision prone for an AV. Our pedestrian placement model can be used in conjunction with \emph{any} goal driven pedestrian model -- be it hand-designed or learnable -- which makes it possible to challenge an AV with a wide range of pedestrian behaviors. Furthermore the proposed pedestrian placement model can be utilized with any goal based AV model given sufficient problem constraints.             The pedestrian placement model is further conditioned on both scene semantics and occlusions to ensure semantic and visual plausibility, which increases the realism of the resulting scenarios. Moreover, to our best knowledge we are the first to show that it is possible to learn a collision seeking scenario generation model when both the pedestrian and the AV are collision avoiding. We also provide a discussion on how to extend the results to modern AV models.",,,,,"behavior varied adversarial pedestrian generation for autonomous vehicle testing autonomous vehicles, av testing, reinforcement learning there exist several datasets for developing self-driving car methodologies. however, manually collected datasets impose inherent limitations on the variability of test cases and it is particularly difficult to acquire challenging scenarios, e.g. ones which involve collisions with pedestrians. automatically synthesizing safety-critical scenarios for autonomous vehicles (avs) can vastly increase the amount of relevant data for training and evaluating such avs. however, existing approaches for scenario generation rely on handcrafted pedestrian behavior models, which limits the realism of the corresponding scenarios. in this paper we reformulate the problem as learning \emph{where to place} pedestrians such that the induced scenarios are collision prone for an av. our pedestrian placement model can be used in conjunction with \emph{any} goal driven pedestrian model -- be it hand-designed or learnable -- which makes it possible to challenge an av with a wide range of pedestrian behaviors. furthermore the proposed pedestrian placement model can be utilized with any goal based av model given sufficient problem constraints.             the pedestrian placement model is further conditioned on both scene semantics and occlusions to ensure semantic and visual plausibility, which increases the realism of the resulting scenarios. moreover, to our best knowledge we are the first to show that it is possible to learn a collision seeking scenario generation model when both the pedestrian and the av are collision avoiding. we also provide a discussion on how to extend the results to modern av models.",-1.4339424,5.2303615,pedestrian,reinforce,134,"id:134 (pedestrian)<br><b>Behavior Varied Adversarial Pedestrian Generation for Autonomous Vehicle Testing</b><br>kw:Autonomous Vehicles, AV Testing, Reinforcement Learning"
135,135,2,https://openreview.net/forum?id=_daq0uh6yXr,Example-Driven Model-Based Reinforcement Learning for Solving Long-Horizon Visuomotor Tasks,"Bohan Wu, Suraj Nair, Li Fei-Fei, Chelsea Finn",8.0,,,,,,,False,,"model-based reinforcement learning, long-horizon planning","In this paper, we study the problem of learning a repertoire of low-level skills from raw images that can be sequenced to complete long-horizon visuomotor tasks. Reinforcement learning (RL) is a promising approach for acquiring short-horizon skills autonomously. However, the focus of RL algorithms has largely been on the success of those individual skills, more so than learning and grounding a large repertoire of skills that can be sequenced to complete extended multi-stage tasks. The latter demands robustness and persistence, as errors in skills can compound over time, and may require the robot to have a number of primitive skills in its repertoire, rather than just one. To this end, we introduce EMBR, a model-based RL method for learning primitive skills that are suitable for completing long-horizon visuomotor tasks. EMBR learns and plans using a learned model, critic, and success classifier, where the success classifier serves both as a reward function for RL and as a grounding mechanism to continuously detect if the robot should retry a skill when unsuccessful or under perturbations. Further, the learned model is task-agnostic and trained using data from all skills, enabling the robot to efficiently learn a number of distinct primitives. These visuomotor primitive skills and their associated pre- and post-conditions can then be directly combined with off-the-shelf symbolic planners to complete long-horizon tasks. On a Franka Emika robot arm, we find that EMBR enables the robot to complete three long-horizon visuomotor tasks at 85% success rate, such as organizing an office desk, a file cabinet, and drawers, which require sequencing up to 12 skills, involve 14 unique learned primitives, and demand generalization to novel objects.",,,,,"example-driven model-based reinforcement learning for solving long-horizon visuomotor tasks model-based reinforcement learning, long-horizon planning in this paper, we study the problem of learning a repertoire of low-level skills from raw images that can be sequenced to complete long-horizon visuomotor tasks. reinforcement learning (rl) is a promising approach for acquiring short-horizon skills autonomously. however, the focus of rl algorithms has largely been on the success of those individual skills, more so than learning and grounding a large repertoire of skills that can be sequenced to complete extended multi-stage tasks. the latter demands robustness and persistence, as errors in skills can compound over time, and may require the robot to have a number of primitive skills in its repertoire, rather than just one. to this end, we introduce embr, a model-based rl method for learning primitive skills that are suitable for completing long-horizon visuomotor tasks. embr learns and plans using a learned model, critic, and success classifier, where the success classifier serves both as a reward function for rl and as a grounding mechanism to continuously detect if the robot should retry a skill when unsuccessful or under perturbations. further, the learned model is task-agnostic and trained using data from all skills, enabling the robot to efficiently learn a number of distinct primitives. these visuomotor primitive skills and their associated pre- and post-conditions can then be directly combined with off-the-shelf symbolic planners to complete long-horizon tasks. on a franka emika robot arm, we find that embr enables the robot to complete three long-horizon visuomotor tasks at 85% success rate, such as organizing an office desk, a file cabinet, and drawers, which require sequencing up to 12 skills, involve 14 unique learned primitives, and demand generalization to novel objects.",-2.6145496,4.0286922,long-horizon,reinforce,135,"id:135 (long-horizon)<br><b>Example-Driven Model-Based Reinforcement Learning for Solving Long-Horizon Visuomotor Tasks</b><br>kw:model-based reinforcement learning, long-horizon planning"
136,136,39,https://openreview.net/forum?id=Pp0Co2vU28N,IMU-Assisted Learning of Single-View Rolling Shutter Correction,"Jiawei Mo, Md Jahidul Islam, Junaed Sattar",8.0,,,,,,,False,,"Rolling Shutter Correction, IMU, Learning","Rolling shutter distortion is highly undesirable for photography and computer vision algorithms (e.g., visual SLAM) because pixels can be potentially captured at different times and poses. In this paper, we propose a deep neural network to predict depth and row-wise pose from a single image for rolling shutter correction. Our contribution in this work is to incorporate inertial measurement unit (IMU) data into the pose refinement process, which, compared to the state-of-the-art, greatly enhances the pose prediction. The improved accuracy and robustness make it possible for numerous vision algorithms to use imagery captured by rolling shutter cameras and produce highly accurate results. We also extend a dataset to have real rolling shutter images, IMU data, depth maps, camera poses, and corresponding global shutter images for rolling shutter correction training. We demonstrate the efficacy of the proposed method by evaluating the performance of Direct Sparse Odometry (DSO) algorithm on rolling shutter imagery corrected using the proposed approach. Results show marked improvements of the DSO algorithm over using uncorrected imagery, validating the proposed approach.",,,,,"imu-assisted learning of single-view rolling shutter correction rolling shutter correction, imu, learning rolling shutter distortion is highly undesirable for photography and computer vision algorithms (e.g., visual slam) because pixels can be potentially captured at different times and poses. in this paper, we propose a deep neural network to predict depth and row-wise pose from a single image for rolling shutter correction. our contribution in this work is to incorporate inertial measurement unit (imu) data into the pose refinement process, which, compared to the state-of-the-art, greatly enhances the pose prediction. the improved accuracy and robustness make it possible for numerous vision algorithms to use imagery captured by rolling shutter cameras and produce highly accurate results. we also extend a dataset to have real rolling shutter images, imu data, depth maps, camera poses, and corresponding global shutter images for rolling shutter correction training. we demonstrate the efficacy of the proposed method by evaluating the performance of direct sparse odometry (dso) algorithm on rolling shutter imagery corrected using the proposed approach. results show marked improvements of the dso algorithm over using uncorrected imagery, validating the proposed approach.",-2.297831,4.2356997,shutter,no reinforce,136,"id:136 (shutter)<br><b>IMU-Assisted Learning of Single-View Rolling Shutter Correction</b><br>kw:Rolling Shutter Correction, IMU, Learning"
137,137,61,https://openreview.net/forum?id=yhy25u-DrjR,"Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning","brian ichter, Pierre Sermanet, Corey Lynch",8.0,,,,,,,False,,"RRT, Task and Motion Planning, Model-based Planning, Tree-search","Long-horizon planning in realistic environments requires the ability to reason over sequential tasks in high-dimensional state spaces with complex dynamics. Classical motion planning algorithms, such as rapidly-exploring random trees, are capable of efficiently exploring large state spaces and computing long-horizon, sequential plans. However, these algorithms are generally challenged with complex, stochastic, and high-dimensional state spaces as well as in the presence of small, topologically complex goal regions, which naturally emerge in tasks that interact with the environment. Machine learning offers a promising solution for its ability to learn general policies that can handle complex interactions and high-dimensional observations. However, these policies are generally limited in horizon length. Our approach, Broadly-Exploring, Local-policy Trees (BELT), merges these two approaches to leverage the strengths of both through a task-conditioned, model-based tree search. BELT uses an RRT-inspired tree search to efficiently explore the state space. Locally, the exploration is guided by a task-conditioned, learned policy capable of performing general short-horizon tasks. This task space can be quite general and abstract; its only requirements are to be sampleable and to well-cover the space of useful tasks. This search is aided by a task-conditioned model that temporally extends dynamics propagation to allow long-horizon search and sequential reasoning over tasks. BELT is demonstrated experimentally to be able to plan long-horizon, sequential trajectories with a goal conditioned policy and generate plans that are robust.",,,,,"broadly-exploring, local-policy trees for long-horizon task planning rrt, task and motion planning, model-based planning, tree-search long-horizon planning in realistic environments requires the ability to reason over sequential tasks in high-dimensional state spaces with complex dynamics. classical motion planning algorithms, such as rapidly-exploring random trees, are capable of efficiently exploring large state spaces and computing long-horizon, sequential plans. however, these algorithms are generally challenged with complex, stochastic, and high-dimensional state spaces as well as in the presence of small, topologically complex goal regions, which naturally emerge in tasks that interact with the environment. machine learning offers a promising solution for its ability to learn general policies that can handle complex interactions and high-dimensional observations. however, these policies are generally limited in horizon length. our approach, broadly-exploring, local-policy trees (belt), merges these two approaches to leverage the strengths of both through a task-conditioned, model-based tree search. belt uses an rrt-inspired tree search to efficiently explore the state space. locally, the exploration is guided by a task-conditioned, learned policy capable of performing general short-horizon tasks. this task space can be quite general and abstract; its only requirements are to be sampleable and to well-cover the space of useful tasks. this search is aided by a task-conditioned model that temporally extends dynamics propagation to allow long-horizon search and sequential reasoning over tasks. belt is demonstrated experimentally to be able to plan long-horizon, sequential trajectories with a goal conditioned policy and generate plans that are robust.",-1.8657169,4.364693,sequential,no reinforce,137,"id:137 (sequential)<br><b>Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning</b><br>kw:RRT, Task and Motion Planning, Model-based Planning, Tree-search"
138,138,113,https://openreview.net/forum?id=m5k1XdK5nI2,Exploring Adversarial Robustness of Multi-sensor Perception Systems in Self Driving,"James Tu, Huichen Li, Xinchen Yan, Mengye Ren, Yun Chen, Ming Liang, Eilyan Bitar, Ersin Yumer, Raquel Urtasun",8.0,,,,,,,False,,"Adversarial, Self-Driving, Detection, Multimodal","Modern self-driving perception systems have been shown to improve upon processing complementary inputs such as LiDAR with images. In isolation, 2D images have been found to be extremely vulnerable to adversarial attacks. Yet, there are limited studies on the adversarial robustness of multi-modal models that fuse LiDAR and image features. Furthermore, existing works do not consider physically realizable perturbations that are consistent across the input modalities. In this paper, we showcase practical susceptibilities of multi-sensor detection by inserting an adversarial object on a host vehicle. We focus on physically realizable and input-agnostic attacks that are feasible to execute in practice, and show that a single universal adversary can hide different host vehicles from state-of-the-art multi-modal detectors. Our experiments demonstrate that successful attacks are primarily caused by easily corrupted image features. Furthermore, in modern sensor fusion methods which project image features into 3D, adversarial attacks can  exploit  the projection process to generate false positives in distant regions in 3D. Towards more robust multi-modal perception systems, we show that adversarial training with feature denoising can boost robustness to such attacks significantly.",,,,,"exploring adversarial robustness of multi-sensor perception systems in self driving adversarial, self-driving, detection, multimodal modern self-driving perception systems have been shown to improve upon processing complementary inputs such as lidar with images. in isolation, 2d images have been found to be extremely vulnerable to adversarial attacks. yet, there are limited studies on the adversarial robustness of multi-modal models that fuse lidar and image features. furthermore, existing works do not consider physically realizable perturbations that are consistent across the input modalities. in this paper, we showcase practical susceptibilities of multi-sensor detection by inserting an adversarial object on a host vehicle. we focus on physically realizable and input-agnostic attacks that are feasible to execute in practice, and show that a single universal adversary can hide different host vehicles from state-of-the-art multi-modal detectors. our experiments demonstrate that successful attacks are primarily caused by easily corrupted image features. furthermore, in modern sensor fusion methods which project image features into 3d, adversarial attacks can  exploit  the projection process to generate false positives in distant regions in 3d. towards more robust multi-modal perception systems, we show that adversarial training with feature denoising can boost robustness to such attacks significantly.",-2.2861576,4.352671,attacks,no reinforce,138,"id:138 (attacks)<br><b>Exploring Adversarial Robustness of Multi-sensor Perception Systems in Self Driving</b><br>kw:Adversarial, Self-Driving, Detection, Multimodal"
139,139,187,https://openreview.net/forum?id=nWLt35BU1z_,Taskography: Evaluating robot task planning over large 3D scene graphs,"Christopher Agia, Krishna Murthy Jatavallabhula, Mohamed Khodeir, Ondrej Miksik, Vibhav Vineet, Mustafa Mukadam, Liam Paull, Florian Shkurti",8.0,,,,,,,False,,"Robot task planning, 3D scene graphs, learning to plan, benchmarks, planning","3D scene graphs (3DSGs) are an emerging description; unifying symbolic, topological, and metric scene representations. However, typical 3DSGs contain hundreds of objects and symbols even for small environments; rendering task planning on the \emph{full} graph impractical. We construct \textbf{Taskography}, the first large-scale robotic task planning benchmark over 3DSGs. While most benchmarking efforts in this area focus on \emph{vision-based planning}, we systematically study \emph{symbolic} planning, to decouple planning performance from visual representation learning. We observe that, among existing methods, neither classical nor learning-based planners are capable of real-time planning over \emph{full} 3DSGs. Enabling real-time planning demands progress on \emph{both} (a) sparsifying 3DSGs for tractable planning and (b) designing planners that better exploit 3DSG hierarchies. Towards the former goal, we propose \textbf{Scrub}, a task-conditioned 3DSG sparsification method; enabling classical planners to match (and surpass) state-of-the-art learning-based planners. Towards the latter goal, we propose \textbf{Seek}, a procedure enabling learning-based planners to exploit 3DSG structure, reducing the number of replanning queries required by current best approaches by an order of magnitude. We will open-source all code and baselines to spur further research along the intersections of robot task planning, learning and 3DSGs.",,,,,"taskography: evaluating robot task planning over large 3d scene graphs robot task planning, 3d scene graphs, learning to plan, benchmarks, planning 3d scene graphs (3dsgs) are an emerging description; unifying symbolic, topological, and metric scene representations. however, typical 3dsgs contain hundreds of objects and symbols even for small environments; rendering task planning on the \emph{full} graph impractical. we construct \textbf{taskography}, the first large-scale robotic task planning benchmark over 3dsgs. while most benchmarking efforts in this area focus on \emph{vision-based planning}, we systematically study \emph{symbolic} planning, to decouple planning performance from visual representation learning. we observe that, among existing methods, neither classical nor learning-based planners are capable of real-time planning over \emph{full} 3dsgs. enabling real-time planning demands progress on \emph{both} (a) sparsifying 3dsgs for tractable planning and (b) designing planners that better exploit 3dsg hierarchies. towards the former goal, we propose \textbf{scrub}, a task-conditioned 3dsg sparsification method; enabling classical planners to match (and surpass) state-of-the-art learning-based planners. towards the latter goal, we propose \textbf{seek}, a procedure enabling learning-based planners to exploit 3dsg structure, reducing the number of replanning queries required by current best approaches by an order of magnitude. we will open-source all code and baselines to spur further research along the intersections of robot task planning, learning and 3dsgs.",-4.113947,4.033167,planners,no reinforce,139,"id:139 (planners)<br><b>Taskography: Evaluating robot task planning over large 3D scene graphs</b><br>kw:Robot task planning, 3D scene graphs, learning to plan, benchmarks, planning"
140,140,191,https://openreview.net/forum?id=csMg2h_LR37,Orientation Probabilistic Movement Primitives on Riemannian Manifolds,"Leonel Rozo, Vedant Dave",8.0,,,,,,,False,,"learning from demonstration, movement primitives, riemannian manifolds","Learning complex robot motions necessarily demands to have models that are able to encode and retrieve full-pose trajectories when tasks are defined in operational spaces. Probabilistic movement primitives (ProMPs) stand out as a principled approach that models trajectory distributions learned from demonstrations. ProMPs allow for trajectory modulation and blending to achieve better generalization to novel situations. However, when ProMPs are employed in operational space, their original formulation does not directly apply to full-pose movements including rotational trajectories described by quaternions. This paper proposes a Riemannian formulation of ProMPs that enables encoding and retrieving of quaternion trajectories. Our method builds on Riemannian manifold theory, and exploits multilinear geodesic regression for estimating the ProMPs parameters. This novel approach makes ProMPs a suitable model for learning complex full-pose robot motion patterns. Riemannian ProMPs are tested on toy examples to illustrate their workflow, and on real learning-from-demonstration experiments.",,,,,"orientation probabilistic movement primitives on riemannian manifolds learning from demonstration, movement primitives, riemannian manifolds learning complex robot motions necessarily demands to have models that are able to encode and retrieve full-pose trajectories when tasks are defined in operational spaces. probabilistic movement primitives (promps) stand out as a principled approach that models trajectory distributions learned from demonstrations. promps allow for trajectory modulation and blending to achieve better generalization to novel situations. however, when promps are employed in operational space, their original formulation does not directly apply to full-pose movements including rotational trajectories described by quaternions. this paper proposes a riemannian formulation of promps that enables encoding and retrieving of quaternion trajectories. our method builds on riemannian manifold theory, and exploits multilinear geodesic regression for estimating the promps parameters. this novel approach makes promps a suitable model for learning complex full-pose robot motion patterns. riemannian promps are tested on toy examples to illustrate their workflow, and on real learning-from-demonstration experiments.",-2.4530988,4.095651,promps,no reinforce,140,"id:140 (promps)<br><b>Orientation Probabilistic Movement Primitives on Riemannian Manifolds</b><br>kw:learning from demonstration, movement primitives, riemannian manifolds"
141,141,193,https://openreview.net/forum?id=XEY7ZeEZEjI,Error-Aware Imitation Learning from Teleoperation Data for Mobile Manipulation,"Josiah Wong, Albert Tung, Andrey Kurenkov, Ajay Mandlekar, Li Fei-Fei, Silvio Savarese, Roberto Mart?n-Mart?n",8.0,,,,,,,False,,"Mobile Manipulation, Imitation Learning, Error Detection","In mobile manipulation (MM), robots can both navigate within and interact with their environment and are thus able to complete many more tasks than robots only capable of navigation or manipulation. In this work, we explore how to apply imitation learning (IL) to learn continuous visuo-motor policies for MM tasks. Much prior work has shown that IL can train visuo-motor policies for either manipulation or navigation domains, but few works have applied IL to the MM domain. Doing this is challenging for two reasons: on the data side, current interfaces make collecting high-quality human demonstrations difficult, and on the learning side, policies trained on limited data can suffer from covariate shift when deployed. To address these problems, we first propose Mobile Manipulation RoboTurk (MoMaRT), a novel teleoperation framework allowing simultaneous navigation and manipulation of mobile manipulators, and collect a first-of-its-kind large scale dataset in a realistic simulated kitchen setting. We then propose a learned error detection system to address the covariate shift by detecting when an agent is in a potential failure state. We train performant IL policies and error detectors from this data, and achieve over 45% task success rate and 85% error detection success rate across multiple multi-stage tasks when trained on expert data. Additional results and video at https://sites.google.com/view/il-for-mm/home.",,,,,"error-aware imitation learning from teleoperation data for mobile manipulation mobile manipulation, imitation learning, error detection in mobile manipulation (mm), robots can both navigate within and interact with their environment and are thus able to complete many more tasks than robots only capable of navigation or manipulation. in this work, we explore how to apply imitation learning (il) to learn continuous visuo-motor policies for mm tasks. much prior work has shown that il can train visuo-motor policies for either manipulation or navigation domains, but few works have applied il to the mm domain. doing this is challenging for two reasons: on the data side, current interfaces make collecting high-quality human demonstrations difficult, and on the learning side, policies trained on limited data can suffer from covariate shift when deployed. to address these problems, we first propose mobile manipulation roboturk (momart), a novel teleoperation framework allowing simultaneous navigation and manipulation of mobile manipulators, and collect a first-of-its-kind large scale dataset in a realistic simulated kitchen setting. we then propose a learned error detection system to address the covariate shift by detecting when an agent is in a potential failure state. we train performant il policies and error detectors from this data, and achieve over 45% task success rate and 85% error detection success rate across multiple multi-stage tasks when trained on expert data. additional results and video at https://sites.google.com/view/il-for-mm/home.",-2.4291537,3.7984488,mobile,no reinforce,141,"id:141 (mobile)<br><b>Error-Aware Imitation Learning from Teleoperation Data for Mobile Manipulation</b><br>kw:Mobile Manipulation, Imitation Learning, Error Detection"
142,142,199,https://openreview.net/forum?id=JvXqtLtAtMY,Self-supervised Point Cloud Prediction Using 3D Spatio-temporal Convolutional Networks,"Benedikt Mersch, Xieyuanli Chen, Jens Behley, Cyrill Stachniss",8.0,,,,,,,False,,"Point Cloud Prediction, 3D LiDAR, Temporal Convolutional Networks","Exploiting past 3D LiDAR scans to predict future point clouds is a promising method for autonomous mobile systems to realize foresighted state estimation, collision avoidance, and planning. In this paper, we address the problem of predicting future 3D LiDAR point clouds given a sequence of past LiDAR scans. Estimating the future scene on the sensor level does not require any preceding steps as in localization or tracking systems and can be trained self-supervised. We propose an end-to-end approach that exploits a 2D range image representation of each 3D LiDAR scan and concatenates a sequence of range images to obtain a 3D tensor. Based on such tensors, we develop an encoder-decoder architecture using 3D convolutions to jointly aggregate spatial and temporal information of the scene and to predict the future 3D point clouds. We evaluate our method on multiple datasets and the experimental results suggest that our method outperforms existing point cloud prediction architectures and generalizes well to new, unseen environments without additional fine-tuning. Our method operates online and is faster than the common LiDAR frame rate of 10 Hz.",,,,,"self-supervised point cloud prediction using 3d spatio-temporal convolutional networks point cloud prediction, 3d lidar, temporal convolutional networks exploiting past 3d lidar scans to predict future point clouds is a promising method for autonomous mobile systems to realize foresighted state estimation, collision avoidance, and planning. in this paper, we address the problem of predicting future 3d lidar point clouds given a sequence of past lidar scans. estimating the future scene on the sensor level does not require any preceding steps as in localization or tracking systems and can be trained self-supervised. we propose an end-to-end approach that exploits a 2d range image representation of each 3d lidar scan and concatenates a sequence of range images to obtain a 3d tensor. based on such tensors, we develop an encoder-decoder architecture using 3d convolutions to jointly aggregate spatial and temporal information of the scene and to predict the future 3d point clouds. we evaluate our method on multiple datasets and the experimental results suggest that our method outperforms existing point cloud prediction architectures and generalizes well to new, unseen environments without additional fine-tuning. our method operates online and is faster than the common lidar frame rate of 10 hz.",-1.9218745,4.147604,lidar,no reinforce,142,"id:142 (lidar)<br><b>Self-supervised Point Cloud Prediction Using 3D Spatio-temporal Convolutional Networks</b><br>kw:Point Cloud Prediction, 3D LiDAR, Temporal Convolutional Networks"
143,143,258,https://openreview.net/forum?id=0f7gUXVAcE9,RICE: Refining Instance Masks in Cluttered Environments with Graph Neural Networks,"Chris Xie, Arsalan Mousavian, Yu Xiang, Dieter Fox",8.0,,,,,,,False,,"Unseen Object Instance Segmentation, Graph Neural Network","Segmenting unseen object instances in cluttered environments is an important capability that robots need when functioning in unstructured environments. While previous methods have exhibited promising results, they still tend to provide incorrect results in highly cluttered scenes.  We postulate that a network architecture that encodes relations between objects at a high-level can be beneficial. Thus, in this work, we propose a novel framework that refines the output of such methods by utilizing a graph-based representation of instance masks.  We train deep networks capable of sampling smart perturbations to the segmentations, and a graph neural network, which can encode relations between objects, to evaluate the perturbed segmentations.  Our proposed method is orthogonal to previous works and achieves state-of-the-art performance when combined with them. We demonstrate an application that uses uncertainty estimates generated by our method to guide a manipulator, leading to efficient understanding of cluttered scenes. Code, models, and video can be found at https://github.com/chrisdxie/rice.",,,,,"rice: refining instance masks in cluttered environments with graph neural networks unseen object instance segmentation, graph neural network segmenting unseen object instances in cluttered environments is an important capability that robots need when functioning in unstructured environments. while previous methods have exhibited promising results, they still tend to provide incorrect results in highly cluttered scenes.  we postulate that a network architecture that encodes relations between objects at a high-level can be beneficial. thus, in this work, we propose a novel framework that refines the output of such methods by utilizing a graph-based representation of instance masks.  we train deep networks capable of sampling smart perturbations to the segmentations, and a graph neural network, which can encode relations between objects, to evaluate the perturbed segmentations.  our proposed method is orthogonal to previous works and achieves state-of-the-art performance when combined with them. we demonstrate an application that uses uncertainty estimates generated by our method to guide a manipulator, leading to efficient understanding of cluttered scenes. code, models, and video can be found at https://github.com/chrisdxie/rice.",-1.1134949,4.5188513,cluttered,no reinforce,143,"id:143 (cluttered)<br><b>RICE: Refining Instance Masks in Cluttered Environments with Graph Neural Networks</b><br>kw:Unseen Object Instance Segmentation, Graph Neural Network"
144,144,280,https://openreview.net/forum?id=PDy45cdhiZ_,Stochastic Policy Optimization with Heuristic Information for Robot Learning,"SEONGHYUN KIM, Ingook Jang, Samyeul Noh, Hyunseok Kim",8.0,,,,,,,False,,"Robot manipulation, Reinforcement learning, Stochastic policy","Stochastic policy-based deep reinforcement learning (RL) approaches have remarkably succeeded to deal with continuous control tasks. However, applying these methods to manipulation tasks remains a challenge since actuators of a robot manipulator require high dimensional continuous action spaces. In this paper, we propose exploration-bounded exploration actor-critic (EBE-AC), a novel deep RL approach to combine stochastic policy optimization with interpretable human knowledge. The human knowledge is defined as heuristic information based on both physical relationships between a robot and objects and binary signals of whether the robot has achieved certain states. The proposed approach, EBE-AC, combines an off-policy actor-critic algorithm with an entropy maximization based on the heuristic information. On a robotic manipulation task, we demonstrate that EBE-AC outperforms prior state-of-the-art off-policy actor-critic deep RL algorithms in terms of sample efficiency. In addition, we found that EBE-AC can be easily combined with latent information, where EBE-AC with latent information further improved sample efficiency and robustness.",,,,,"stochastic policy optimization with heuristic information for robot learning robot manipulation, reinforcement learning, stochastic policy stochastic policy-based deep reinforcement learning (rl) approaches have remarkably succeeded to deal with continuous control tasks. however, applying these methods to manipulation tasks remains a challenge since actuators of a robot manipulator require high dimensional continuous action spaces. in this paper, we propose exploration-bounded exploration actor-critic (ebe-ac), a novel deep rl approach to combine stochastic policy optimization with interpretable human knowledge. the human knowledge is defined as heuristic information based on both physical relationships between a robot and objects and binary signals of whether the robot has achieved certain states. the proposed approach, ebe-ac, combines an off-policy actor-critic algorithm with an entropy maximization based on the heuristic information. on a robotic manipulation task, we demonstrate that ebe-ac outperforms prior state-of-the-art off-policy actor-critic deep rl algorithms in terms of sample efficiency. in addition, we found that ebe-ac can be easily combined with latent information, where ebe-ac with latent information further improved sample efficiency and robustness.",-3.0583587,3.2980156,stochastic,reinforce,144,"id:144 (stochastic)<br><b>Stochastic Policy Optimization with Heuristic Information for Robot Learning</b><br>kw:Robot manipulation, Reinforcement learning, Stochastic policy"
145,145,327,https://openreview.net/forum?id=FS30JeiGG3h,Learning Models as Functionals of Signed-Distance Fields for Manipulation Planning,"Danny Driess, Jung-Su Ha, Marc Toussaint, Russ Tedrake",8.0,,,,,,,False,,,"This work proposes an optimization-based Task and Motion Planning (TAMP) framework where the objectives are learned functionals of signed-distance fields that represent objects in the scene. Most TAMP approaches rely on analytical models and carefully chosen abstractions/state-spaces to be effective. While traditional TAMP approaches show remarkable capabilities, they usually rely on analytical (dynamic) models. A central question is how models can be obtained from data that are not primarily accurate in their predictions, but, more importantly, enable efficient reasoning within a planning framework, while at the same time being closely coupled to perception spaces. We show that signed-distance fields not only enable to learn and represent a variety of models with higher accuracy compared to point-cloud and occupancy measure representations, but also that SDF-based models are suitable for optimization-based planning. To demonstrate the versatility of our approach, we learn both kinematic and dynamic models to solve tasks that involve hanging mugs on hooks and pushing objects on a table. We can unify these quite different tasks within one framework, since SDFs are the common object representation.",,,,,"learning models as functionals of signed-distance fields for manipulation planning nan this work proposes an optimization-based task and motion planning (tamp) framework where the objectives are learned functionals of signed-distance fields that represent objects in the scene. most tamp approaches rely on analytical models and carefully chosen abstractions/state-spaces to be effective. while traditional tamp approaches show remarkable capabilities, they usually rely on analytical (dynamic) models. a central question is how models can be obtained from data that are not primarily accurate in their predictions, but, more importantly, enable efficient reasoning within a planning framework, while at the same time being closely coupled to perception spaces. we show that signed-distance fields not only enable to learn and represent a variety of models with higher accuracy compared to point-cloud and occupancy measure representations, but also that sdf-based models are suitable for optimization-based planning. to demonstrate the versatility of our approach, we learn both kinematic and dynamic models to solve tasks that involve hanging mugs on hooks and pushing objects on a table. we can unify these quite different tasks within one framework, since sdfs are the common object representation.",-2.0138507,4.1458178,signed-distance,no reinforce,145,id:145 (signed-distance)<br><b>Learning Models as Functionals of Signed-Distance Fields for Manipulation Planning</b><br>kw:nan
146,146,339,https://openreview.net/forum?id=5DjX89Wyhk-,Predicting Stable Configurations for Semantic Placement of Novel Objects,"Chris Paxton, Chris Xie, Tucker Hermans, Dieter Fox",8.0,,,,,,,False,,"Deep learning for robotic manipulation, learning for motion planning, semantic manipulation","Human environments contain numerous objects configured in a variety of arrangements. Our goal is to enable robots to repose previously unseen objects according to learned semantic relationships in novel environments. We break this problem down into two parts: (1) finding physically valid locations for the objects and (2) determining if those poses satisfy learned, high-level semantic relationships.        We build our models and training from the ground up to be tightly integrated with our proposed planning algorithm for semantic placement of unknown objects. We train our models purely in simulation, with no fine-tuning needed for use in the real world.        Our approach enables motion planning for semantic rearrangement of unknown objects in scenes with varying geometry from only RGB-D sensing. Our experiments through a set of simulated ablations demonstrate that using a relational classifier alone is not sufficient for reliable planning. We further demonstrate the ability of our planner to generate and execute diverse manipulation plans through a set of real-world experiments with a variety of objects.",,,,,"predicting stable configurations for semantic placement of novel objects deep learning for robotic manipulation, learning for motion planning, semantic manipulation human environments contain numerous objects configured in a variety of arrangements. our goal is to enable robots to repose previously unseen objects according to learned semantic relationships in novel environments. we break this problem down into two parts: (1) finding physically valid locations for the objects and (2) determining if those poses satisfy learned, high-level semantic relationships.        we build our models and training from the ground up to be tightly integrated with our proposed planning algorithm for semantic placement of unknown objects. we train our models purely in simulation, with no fine-tuning needed for use in the real world.        our approach enables motion planning for semantic rearrangement of unknown objects in scenes with varying geometry from only rgb-d sensing. our experiments through a set of simulated ablations demonstrate that using a relational classifier alone is not sufficient for reliable planning. we further demonstrate the ability of our planner to generate and execute diverse manipulation plans through a set of real-world experiments with a variety of objects.",-2.0062635,3.8244724,semantic,no reinforce,146,"id:146 (semantic)<br><b>Predicting Stable Configurations for Semantic Placement of Novel Objects</b><br>kw:Deep learning for robotic manipulation, learning for motion planning, semantic manipulation"
147,147,341,https://openreview.net/forum?id=H1-uwiTbY9z,Distributional Depth-Based Estimation of Object Articulation Models,"Ajinkya Jain, Stephen Giguere, Rudolf Lioutikov, Scott Niekum",8.0,,,,,,,False,,"Articulated Objects, Model Learning, Uncertainty Estimation, Kinematics, Deep Learning","We propose a method that efficiently learns distributions over articulation models directly from depth images without the need to know articulation model categories a priori. By contrast, existing methods that learn articulation models from raw observations require objects to be textured, and most only predict point estimates of the model parameters. Our core contributions include a novel representation for distributions over rigid body transformations and articulation model parameters based on Screw theory, von Mises-Fisher distributions and Stiefel manifolds. Combining these concepts allows for an efficient, mathematically sound representation that inherently satisfies several constraints that rigid body transformations and articulations must adhere to. In addition, we introduce a novel deep-learning based approach, DUST-net, that efficiently learns such distributions and, hence, performs category-independent articulation model estimation while also providing model uncertainties. We evaluate our approach on two benchmarking datasets and three real-world objects and compare its performance with two current state-of-the-art methods. Our results demonstrate that DUST-net can successfully learn distributions over articulation models and their parameters for novel objects across articulation model categories with better accuracy than state-of-the-art methods.",,,,,"distributional depth-based estimation of object articulation models articulated objects, model learning, uncertainty estimation, kinematics, deep learning we propose a method that efficiently learns distributions over articulation models directly from depth images without the need to know articulation model categories a priori. by contrast, existing methods that learn articulation models from raw observations require objects to be textured, and most only predict point estimates of the model parameters. our core contributions include a novel representation for distributions over rigid body transformations and articulation model parameters based on screw theory, von mises-fisher distributions and stiefel manifolds. combining these concepts allows for an efficient, mathematically sound representation that inherently satisfies several constraints that rigid body transformations and articulations must adhere to. in addition, we introduce a novel deep-learning based approach, dust-net, that efficiently learns such distributions and, hence, performs category-independent articulation model estimation while also providing model uncertainties. we evaluate our approach on two benchmarking datasets and three real-world objects and compare its performance with two current state-of-the-art methods. our results demonstrate that dust-net can successfully learn distributions over articulation models and their parameters for novel objects across articulation model categories with better accuracy than state-of-the-art methods.",-0.4229458,5.5578423,articulation,no reinforce,147,"id:147 (articulation)<br><b>Distributional Depth-Based Estimation of Object Articulation Models</b><br>kw:Articulated Objects, Model Learning, Uncertainty Estimation, Kinematics, Deep Learning"
148,148,362,https://openreview.net/forum?id=KnOYrZf17CQ,Safe Driving via Expert Guided Policy Optimization,"Zhenghao Peng, Quanyi Li, Chunxiao Liu, Bolei Zhou",8.0,,,,,,,False,,"Safe Reinforcement Learning, Human-in-the-loop, Imitation Learning","When learning common skills like driving, beginners usually have domain experts standing by to ensure the safety of the learning process. We formulate such learning scheme under the Expert-in-the-loop Reinforcement Learning where a guardian is introduced to safeguard the exploration of the learning agent. While allowing the sufficient exploration in the uncertain environment, the guardian intervenes under dangerous situations and demonstrates the correct actions to avoid potential accidents. Thus ERL enables both exploration and expert's partial demonstration as two training sources. Following such a setting, we develop a novel Expert Guided Policy Optimization (EGPO) method which integrates the guardian in the loop of reinforcement learning. The guardian is composed of an expert policy to generate demonstration and a switch function to decide when to intervene. Particularly, a constrained optimization technique is used to tackle the trivial solution that the agent deliberately behaves dangerously to deceive the expert into taking over. Offline RL technique is further used to learn from the partial demonstration generated by the expert. Safe driving experiments show that our method achieves superior training and test-time safety, outperforms baselines with a substantial margin in sample efficiency, and preserves the generalizabiliy to unseen environments in test-time. Demo video and source code are available at: https://decisionforce.github.io/EGPO/",,,,,"safe driving via expert guided policy optimization safe reinforcement learning, human-in-the-loop, imitation learning when learning common skills like driving, beginners usually have domain experts standing by to ensure the safety of the learning process. we formulate such learning scheme under the expert-in-the-loop reinforcement learning where a guardian is introduced to safeguard the exploration of the learning agent. while allowing the sufficient exploration in the uncertain environment, the guardian intervenes under dangerous situations and demonstrates the correct actions to avoid potential accidents. thus erl enables both exploration and expert's partial demonstration as two training sources. following such a setting, we develop a novel expert guided policy optimization (egpo) method which integrates the guardian in the loop of reinforcement learning. the guardian is composed of an expert policy to generate demonstration and a switch function to decide when to intervene. particularly, a constrained optimization technique is used to tackle the trivial solution that the agent deliberately behaves dangerously to deceive the expert into taking over. offline rl technique is further used to learn from the partial demonstration generated by the expert. safe driving experiments show that our method achieves superior training and test-time safety, outperforms baselines with a substantial margin in sample efficiency, and preserves the generalizabiliy to unseen environments in test-time. demo video and source code are available at: https://decisionforce.github.io/egpo/",-3.6620488,3.38984,guardian,reinforce,148,"id:148 (guardian)<br><b>Safe Driving via Expert Guided Policy Optimization</b><br>kw:Safe Reinforcement Learning, Human-in-the-loop, Imitation Learning"
149,149,375,https://openreview.net/forum?id=hu7b7MPCqiC,Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals,"Nachiket Deo, Eric Wolff, Oscar Beijbom",8.0,,,,,,,False,,"Motion prediction, autonomous vehicles, graph neural networks","Accurately predicting the future motion of surrounding vehicles requires reasoning about the inherent uncertainty in driving behavior. This uncertainty can be loosely decoupled into lateral (e.g., keeping lane, turning) and longitudinal (e.g., accelerating, braking). We present a novel method that combines learned discrete policy rollouts with a focused decoder on subsets of the lane graph. The policy rollouts explore different goals given current observations, ensuring that the model captures lateral variability. Longitudinal variability is captured by our latent variable model decoder that is conditioned on various subsets of the lane graph. Our model achieves state-of-the-art performance on the nuScenes motion prediction dataset, and qualitatively demonstrates excellent scene compliance. Detailed ablations highlight the importance of the policy rollouts and the decoder architecture.",,,,,"multimodal trajectory prediction conditioned on lane-graph traversals motion prediction, autonomous vehicles, graph neural networks accurately predicting the future motion of surrounding vehicles requires reasoning about the inherent uncertainty in driving behavior. this uncertainty can be loosely decoupled into lateral (e.g., keeping lane, turning) and longitudinal (e.g., accelerating, braking). we present a novel method that combines learned discrete policy rollouts with a focused decoder on subsets of the lane graph. the policy rollouts explore different goals given current observations, ensuring that the model captures lateral variability. longitudinal variability is captured by our latent variable model decoder that is conditioned on various subsets of the lane graph. our model achieves state-of-the-art performance on the nuscenes motion prediction dataset, and qualitatively demonstrates excellent scene compliance. detailed ablations highlight the importance of the policy rollouts and the decoder architecture.",-2.316521,3.93606,rollouts,no reinforce,149,"id:149 (rollouts)<br><b>Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals</b><br>kw:Motion prediction, autonomous vehicles, graph neural networks"
150,150,383,https://openreview.net/forum?id=AL4FPs84YdQ,Semantic Terrain Classification for Off-Road Autonomous Driving,"Amirreza Shaban, Xiangyun Meng, JoonHo Lee, Byron Boots, Dieter Fox",8.0,,,,,,,False,,"Off-road Driving, Autonomous Driving, Deep Learning, Perception","Producing dense and accurate traversability maps is crucial for autonomous off-road navigation. In this paper, we focus on the problem of classifying terrains into 4 cost classes (free, low-cost, medium-cost, obstacle) for traversability assessment. This requires a robot to reason about both semantics (what objects are present?) and geometric properties (where are the objects located?) of the environment. To achieve this goal, we develop a novel Bird's Eye View Network (BEVNet), a deep neural network that directly predicts a local map encoding terrain classes from sparse LiDAR inputs. BEVNet processes both geometric and semantic information in a temporally consistent fashion. More importantly, it uses learned prior and history to predict terrain classes in unseen space and into the future, allowing a robot to better appraise its situation. We quantitatively evaluate BEVNet on both on-road and off-road scenarios and show that it outperforms a variety of strong baselines.",,,,,"semantic terrain classification for off-road autonomous driving off-road driving, autonomous driving, deep learning, perception producing dense and accurate traversability maps is crucial for autonomous off-road navigation. in this paper, we focus on the problem of classifying terrains into 4 cost classes (free, low-cost, medium-cost, obstacle) for traversability assessment. this requires a robot to reason about both semantics (what objects are present?) and geometric properties (where are the objects located?) of the environment. to achieve this goal, we develop a novel bird's eye view network (bevnet), a deep neural network that directly predicts a local map encoding terrain classes from sparse lidar inputs. bevnet processes both geometric and semantic information in a temporally consistent fashion. more importantly, it uses learned prior and history to predict terrain classes in unseen space and into the future, allowing a robot to better appraise its situation. we quantitatively evaluate bevnet on both on-road and off-road scenarios and show that it outperforms a variety of strong baselines.",-2.2019982,3.732053,off-road,no reinforce,150,"id:150 (off-road)<br><b>Semantic Terrain Classification for Off-Road Autonomous Driving</b><br>kw:Off-road Driving, Autonomous Driving, Deep Learning, Perception"
151,151,385,https://openreview.net/forum?id=UIaodSPHNFN,Influencing Behavioral Attributions to Robot Motion During Task Execution,"Nick Walker, Christoforos Mavrogiannis, Siddhartha Srinivasa, Maya Cakmak",8.0,,,,,,,False,,"human-robot interaction, behavioral attribution, robot motion, human impressions","Recent literature has proposed algorithms for autonomous generation of robot motion that communicates functional attributes of a robot's state such as intent or incapability. However, less is known about how to automate the generation of motion for communicating higher-level behavioral attributes such as curiosity or competence. We propose a framework that jointly addresses the challenges of modeling human attributions to robot motion, generating trajectories that elicit attributions, and selecting trajectories that balance attribution and task completion. The insight underpinning our approach is that many attributions can be traced to salient features of the robot's motion. We illustrate the framework in a coverage task resembling household vacuum cleaning. Through a virtual interface, we collect a dataset of human attributions to robot trajectories during task execution and learn a probabilistic model that maps robot trajectories to human attributions. We then incorporate this model into a trajectory generation mechanism that balances between task completion and communication of a desired behavioral attribution. Through an online user study on a different household layout, we find that our prediction model accurately captures human attribution for coverage tasks.",,,,,"influencing behavioral attributions to robot motion during task execution human-robot interaction, behavioral attribution, robot motion, human impressions recent literature has proposed algorithms for autonomous generation of robot motion that communicates functional attributes of a robot's state such as intent or incapability. however, less is known about how to automate the generation of motion for communicating higher-level behavioral attributes such as curiosity or competence. we propose a framework that jointly addresses the challenges of modeling human attributions to robot motion, generating trajectories that elicit attributions, and selecting trajectories that balance attribution and task completion. the insight underpinning our approach is that many attributions can be traced to salient features of the robot's motion. we illustrate the framework in a coverage task resembling household vacuum cleaning. through a virtual interface, we collect a dataset of human attributions to robot trajectories during task execution and learn a probabilistic model that maps robot trajectories to human attributions. we then incorporate this model into a trajectory generation mechanism that balances between task completion and communication of a desired behavioral attribution. through an online user study on a different household layout, we find that our prediction model accurately captures human attribution for coverage tasks.",-1.9719758,3.8747451,attributions,no reinforce,151,"id:151 (attributions)<br><b>Influencing Behavioral Attributions to Robot Motion During Task Execution</b><br>kw:human-robot interaction, behavioral attribution, robot motion, human impressions"
152,152,412,https://openreview.net/forum?id=kR8UWCty--O,Visual Learning Towards Soft Robot Force Control using a 3D Metamaterial with Differential Stiffness,"Fang Wan, Xiaobo Liu, Ning Guo, Xudong Han, Feng Tian, Chaoyang Song",8.0,,,,,,,False,,--,--,,,,,visual learning towards soft robot force control using a 3d metamaterial with differential stiffness -- --,-0.40107894,0.6591289,metamaterial,no reinforce,152,id:152 (metamaterial)<br><b>Visual Learning Towards Soft Robot Force Control using a 3D Metamaterial with Differential Stiffness</b><br>kw:--
153,0,9,https://openreview.net/forum?id=QEG0rEN8uQS,Back to Reality for Imitation Learning,Edward Johns,,,,,,,,False,,--,--,1.0,1.0,,,back to reality for imitation learning -- --,-0.013020863,0.69810224,back,no reinforce,153,id:153 (back)<br><b>Back to Reality for Imitation Learning</b><br>kw:--
154,1,16,https://openreview.net/forum?id=cBdnThrYkV7,The Task Specification Problem ,Pulkit Agrawal,,,,,,,,False,,--,--,1.0,2.0,,,the task specification problem  -- --,0.09684226,0.10189611,specification,no reinforce,154,id:154 (specification)<br><b>The Task Specification Problem </b><br>kw:--
155,2,2,https://openreview.net/forum?id=ZdghnQLpqcD,Continuous then discrete: A recommendation for building robotic brains,"Chris Eliasmith, P. Michael Furlong",,,,,,,,False,,--,--,1.0,3.0,,,continuous then discrete: a recommendation for building robotic brains -- --,-0.42111933,0.5146435,recommendation,no reinforce,155,id:155 (recommendation)<br><b>Continuous then discrete: A recommendation for building robotic brains</b><br>kw:--
156,3,25,https://openreview.net/forum?id=DMevLBCwnRS,From Robot Learning To Robot Understanding: Leveraging Causal Graphical Models For Robotics,"Kaylene Caswell Stocking, Alison Gopnik, Claire Tomlin",,,,,,,,False,,--,--,1.0,4.0,,,from robot learning to robot understanding: leveraging causal graphical models for robotics -- --,0.08045352,1.0088496,understanding:,no reinforce,156,id:156 (understanding:)<br><b>From Robot Learning To Robot Understanding: Leveraging Causal Graphical Models For Robotics</b><br>kw:--
157,4,29,https://openreview.net/forum?id=BozDOMTsSNd,Decentralized Sharing and Valuation of Fleet Robotic Data,"Yuchong Geng, Dongyue Zhang, Po-han Li, Oguzhan Akcin, Ao Tang, Sandeep P. Chinchali",,,,,,,,False,,--,--,1.0,5.0,,,decentralized sharing and valuation of fleet robotic data -- --,-0.12709609,0.6369658,valuation,no reinforce,157,id:157 (valuation)<br><b>Decentralized Sharing and Valuation of Fleet Robotic Data</b><br>kw:--
158,5,14,https://openreview.net/forum?id=n3AW_ISWCXf,Toward robots that learn to summarize their actions in natural language: a set of tasks,"Chad DeChant, Daniel Bauer",,,,,,,,False,,--,--,2.0,1.0,,,toward robots that learn to summarize their actions in natural language: a set of tasks -- --,0.14633936,0.9091327,toward,no reinforce,158,id:158 (toward)<br><b>Toward robots that learn to summarize their actions in natural language: a set of tasks</b><br>kw:--
159,6,19,https://openreview.net/forum?id=W_TkB-1eNbs,Robots on Demand: A Democratized Robotics Research Cloud,"Victoria Dean, Yonadav G Shavit, Abhinav Gupta",,,,,,,,False,,--,--,2.0,2.0,,,robots on demand: a democratized robotics research cloud -- --,-0.089196324,1.0960077,democratized,no reinforce,159,id:159 (democratized)<br><b>Robots on Demand: A Democratized Robotics Research Cloud</b><br>kw:--
160,7,20,https://openreview.net/forum?id=qscEfLT5VJK,Collect & Infer - a fresh look at data-efficient Reinforcement Learning,"Martin Riedmiller, Jost Tobias Springenberg, Roland Hafner, Nicolas Heess",,,,,,,,False,,--,--,2.0,3.0,,,collect & infer - a fresh look at data-efficient reinforcement learning -- --,-0.7681345,1.2810057,fresh,reinforce,160,id:160 (fresh)<br><b>Collect & Infer - a fresh look at data-efficient Reinforcement Learning</b><br>kw:--
161,8,22,https://openreview.net/forum?id=FI2HrMozlo_,Dual-Arm Adversarial Robot Learning,Elie Aljalbout,,,,,,,,False,,--,--,2.0,4.0,,,dual-arm adversarial robot learning -- --,-0.1001356,0.75917846,dual-arm,no reinforce,161,id:161 (dual-arm)<br><b>Dual-Arm Adversarial Robot Learning</b><br>kw:--
162,9,12,https://openreview.net/forum?id=X5s0D3E-skk,RoboFlow: a Data-centric Workflow ManagementSystem for Developing AI-enhanced Robots,"Qinjie Lin, Guo Ye, Jiayi Wang, Han Liu",,,,,,,,False,,--,--,3.0,1.0,,,roboflow: a data-centric workflow managementsystem for developing ai-enhanced robots -- --,-0.39183843,1.402901,roboflow:,no reinforce,162,id:162 (roboflow:)<br><b>RoboFlow: a Data-centric Workflow ManagementSystem for Developing AI-enhanced Robots</b><br>kw:--
163,10,5,https://openreview.net/forum?id=57HG8dDs4t4,Auditing Robot Learning for Safety and Compliance during Deployment,Homanga Bharadhwaj,,,,,,,,False,,--,--,3.0,2.0,,,auditing robot learning for safety and compliance during deployment -- --,-0.40759322,0.5372758,auditing,no reinforce,163,id:163 (auditing)<br><b>Auditing Robot Learning for Safety and Compliance during Deployment</b><br>kw:--
164,11,10,https://openreview.net/forum?id=L55-yn1iwrm,Understanding the World Through Action,Sergey Levine,,,,,,,,False,,--,--,3.0,3.0,,,understanding the world through action -- --,-0.039001483,0.12805972,--,no reinforce,164,id:164 (--)<br><b>Understanding the World Through Action</b><br>kw:--
165,12,15,https://openreview.net/forum?id=jaoAqmrabvO,Learning to be Multimodal : Co-evolving Sensory Modalities and Sensor Properties,"Rika Antonova, Jeannette Bohg",,,,,,,,False,,--,--,3.0,4.0,,,learning to be multimodal : co-evolving sensory modalities and sensor properties -- --,-0.427492,0.88745433,co-evolving,no reinforce,165,id:165 (co-evolving)<br><b>Learning to be Multimodal : Co-evolving Sensory Modalities and Sensor Properties</b><br>kw:--
166,0,209,https://openreview.net/forum?id=87_OJU4sw3V,"ReSkin: versatile, replaceable, lasting tactile skins","Raunaq Bhirangi, Tess Hellebrekers, Carmel Majidi, Abhinav Gupta",,,,,☆,,,False,,"Tactile Skin, Self-supervised Learning, Magnetic Sensing, Soft Sensors","Soft sensors have continued growing interest in robotics, due to their ability to enable both passive conformal contact from the material properties and active contact data from the sensor properties. However, the same properties of conformal contact result in faster deterioration of soft sensors and larger variations in their response characteristics over time and across samples, inhibiting their ability to be long-lasting and replaceable. ReSkin is a tactile soft sensor that leverages machine learning and magnetic sensing to offer a low-cost, diverse and compact solution for long-term use. Magnetic sensing separates the electronic circuitry from the passive interface, making it easier to replace interfaces as they wear out while allowing for a wide variety of form factors. Machine learning allows us to learn sensor response models that are robust to variations across fabrication and time, and our self-supervised learning algorithm enables finer performance enhancement with small, inexpensive data collection procedures. We believe that ReSkin opens the door to more versatile, scalable and inexpensive tactile sensation modules than existing alternatives.",,1.0,,1.0,"reskin: versatile, replaceable, lasting tactile skins tactile skin, self-supervised learning, magnetic sensing, soft sensors soft sensors have continued growing interest in robotics, due to their ability to enable both passive conformal contact from the material properties and active contact data from the sensor properties. however, the same properties of conformal contact result in faster deterioration of soft sensors and larger variations in their response characteristics over time and across samples, inhibiting their ability to be long-lasting and replaceable. reskin is a tactile soft sensor that leverages machine learning and magnetic sensing to offer a low-cost, diverse and compact solution for long-term use. magnetic sensing separates the electronic circuitry from the passive interface, making it easier to replace interfaces as they wear out while allowing for a wide variety of form factors. machine learning allows us to learn sensor response models that are robust to variations across fabrication and time, and our self-supervised learning algorithm enables finer performance enhancement with small, inexpensive data collection procedures. we believe that reskin opens the door to more versatile, scalable and inexpensive tactile sensation modules than existing alternatives.",-1.5412633,4.668739,soft,no reinforce,166,"id:166 (soft)<br><b>ReSkin: versatile, replaceable, lasting tactile skins</b><br>kw:Tactile Skin, Self-supervised Learning, Magnetic Sensing, Soft Sensors"
167,1,153,https://openreview.net/forum?id=zwo1-MdMl1P,Robot Reinforcement Learning on the Constraint Manifold,"Puze Liu, Davide Tateo, Haitham Bou Ammar, Jan Peters",,,,,,,,False,,"Robot Learning, Reinforcement Learning, Constrained Markov Decision Process, Safe Exploration","Reinforcement learning in robotics is extremely challenging due to many practical issues, including safety, mechanical constraints, and wear and tear. Typically, these issues are not considered in the machine learning literature. One crucial problem in applying reinforcement learning in the real world is Safe Exploration, which requires physical and safety constraints satisfaction throughout the learning process.  To explore in such a safety-critical environment, leveraging known information such as robot models and constraints is beneficial to provide more robust safety guarantees. Exploiting this knowledge, we propose a novel method to learn robotics tasks in simulation efficiently while satisfying the constraints during the learning process.",,2.0,,1.0,"robot reinforcement learning on the constraint manifold robot learning, reinforcement learning, constrained markov decision process, safe exploration reinforcement learning in robotics is extremely challenging due to many practical issues, including safety, mechanical constraints, and wear and tear. typically, these issues are not considered in the machine learning literature. one crucial problem in applying reinforcement learning in the real world is safe exploration, which requires physical and safety constraints satisfaction throughout the learning process.  to explore in such a safety-critical environment, leveraging known information such as robot models and constraints is beneficial to provide more robust safety guarantees. exploiting this knowledge, we propose a novel method to learn robotics tasks in simulation efficiently while satisfying the constraints during the learning process.",-2.591646,2.3688412,constraints,reinforce,167,"id:167 (constraints)<br><b>Robot Reinforcement Learning on the Constraint Manifold</b><br>kw:Robot Learning, Reinforcement Learning, Constrained Markov Decision Process, Safe Exploration"
168,2,104,https://openreview.net/forum?id=4u25M570Iji,Motion Forecasting with Unlikelihood Training in Continuous Space,"Deyao Zhu, Mohamed Zahran, Li Erran Li, Mohamed Elhoseiny",,,,,,,,False,,,"Motion forecasting is essential for making safe and intelligent decisions in robotic applications such as autonomous driving. Existing methods often formulate it as a sequence-to-sequence prediction problem, solved in an encoder-decoder framework with a maximum likelihood estimation objective. State-of-the-art models leverage contextual information including the map and states of surrounding agents. However,  we observe that they still assign a high probability to unlikely trajectories resulting in unsafe behaviors including road boundary violations. Orthogonally, we propose a new objective, unlikelihood training, which forces predicted trajectories that conflict with contextual information to be assigned a lower probability. We demonstrate that our method can improve state-of-art models' performance on challenging real-world trajectory forecasting datasets (nuScenes and Argoverse) by avoiding up to 56% context-violated prediction and improving up to 9% prediction accuracy.  Code will be made available.",,3.0,,1.0,"motion forecasting with unlikelihood training in continuous space nan motion forecasting is essential for making safe and intelligent decisions in robotic applications such as autonomous driving. existing methods often formulate it as a sequence-to-sequence prediction problem, solved in an encoder-decoder framework with a maximum likelihood estimation objective. state-of-the-art models leverage contextual information including the map and states of surrounding agents. however,  we observe that they still assign a high probability to unlikely trajectories resulting in unsafe behaviors including road boundary violations. orthogonally, we propose a new objective, unlikelihood training, which forces predicted trajectories that conflict with contextual information to be assigned a lower probability. we demonstrate that our method can improve state-of-art models' performance on challenging real-world trajectory forecasting datasets (nuscenes and argoverse) by avoiding up to 56% context-violated prediction and improving up to 9% prediction accuracy.  code will be made available.",-2.5434182,3.4657474,forecasting,no reinforce,168,id:168 (forecasting)<br><b>Motion Forecasting with Unlikelihood Training in Continuous Space</b><br>kw:nan
169,3,320,https://openreview.net/forum?id=knObbYqSowX,Group-based Motion Prediction for Navigation in Crowded Environments,"Allan Wang, Christoforos Mavrogiannis, Aaron Steinfeld",,,,,☆,,,False,,"Social Navigation, Group-based Navigation, Applications of Robot Learning in Navigation","We focus on the problem of planning the motion of a robot in a dynamic multiagent environment such as a pedestrian scene. Enabling the robot to navigate safely and in a socially compliant fashion in such scenes requires a representation that accounts for the unfolding multiagent dynamics. Existing approaches to this problem tend to employ microscopic models of motion prediction that reason about the individual behavior of other agents. While such models may achieve high tracking accuracy in trajectory prediction benchmarks, they often lack an understanding of the group structures unfolding in crowded scenes. Inspired by the Gestalt theory from psychology, we build a Model Predictive Control framework (G-MPC) that leverages group-based prediction for robot motion planning. We conduct an extensive simulation study involving a series of challenging navigation tasks in scenes extracted from two real-world pedestrian datasets. We illustrate that G-MPC enables a robot to achieve statistically significantly higher safety and lower number of group intrusions than a series of baselines featuring individual pedestrian motion prediction models. Finally, we show that G-MPC can handle noisy lidar-scan estimates without significant performance losses.",,4.0,,1.0,"group-based motion prediction for navigation in crowded environments social navigation, group-based navigation, applications of robot learning in navigation we focus on the problem of planning the motion of a robot in a dynamic multiagent environment such as a pedestrian scene. enabling the robot to navigate safely and in a socially compliant fashion in such scenes requires a representation that accounts for the unfolding multiagent dynamics. existing approaches to this problem tend to employ microscopic models of motion prediction that reason about the individual behavior of other agents. while such models may achieve high tracking accuracy in trajectory prediction benchmarks, they often lack an understanding of the group structures unfolding in crowded scenes. inspired by the gestalt theory from psychology, we build a model predictive control framework (g-mpc) that leverages group-based prediction for robot motion planning. we conduct an extensive simulation study involving a series of challenging navigation tasks in scenes extracted from two real-world pedestrian datasets. we illustrate that g-mpc enables a robot to achieve statistically significantly higher safety and lower number of group intrusions than a series of baselines featuring individual pedestrian motion prediction models. finally, we show that g-mpc can handle noisy lidar-scan estimates without significant performance losses.",-2.628184,3.6072786,group-based,no reinforce,169,"id:169 (group-based)<br><b>Group-based Motion Prediction for Navigation in Crowded Environments</b><br>kw:Social Navigation, Group-based Navigation, Applications of Robot Learning in Navigation"
170,4,108,https://openreview.net/forum?id=vm8Hr9YJHZ1,Fast and Efficient Locomotion via Learned Gait Transitions,"Yuxiang Yang, Tingnan Zhang, Erwin Coumans, Jie Tan, Byron Boots",,,,,,,,False,,"Legged Locomotion, Hierarchical Control, Reinforcement Learning","We focus on the problem of developing energy efficient controllers for quadrupedal robots. Animals can actively switch gaits at different speeds to lower their energy consumption. In this paper, we devise a hierarchical learning framework, in which distinctive locomotion gaits and natural gait transitions emerge automatically with a simple reward of energy minimization.  We use evolutionary strategies (ES) to train a high-level gait policy that specifies gait patterns of each foot, while the low-level convex MPC controller optimizes the motor commands so that the robot can walk at a desired velocity using that gait pattern. We test our learning framework on a quadruped robot and demonstrate automatic gait transitions, from walking to trotting and to fly-trotting, as the robot increases its speed. We show that the learned hierarchical controller consumes much less energy across a wide range of locomotion speed than baseline controllers.",,5.0,,1.0,"fast and efficient locomotion via learned gait transitions legged locomotion, hierarchical control, reinforcement learning we focus on the problem of developing energy efficient controllers for quadrupedal robots. animals can actively switch gaits at different speeds to lower their energy consumption. in this paper, we devise a hierarchical learning framework, in which distinctive locomotion gaits and natural gait transitions emerge automatically with a simple reward of energy minimization.  we use evolutionary strategies (es) to train a high-level gait policy that specifies gait patterns of each foot, while the low-level convex mpc controller optimizes the motor commands so that the robot can walk at a desired velocity using that gait pattern. we test our learning framework on a quadruped robot and demonstrate automatic gait transitions, from walking to trotting and to fly-trotting, as the robot increases its speed. we show that the learned hierarchical controller consumes much less energy across a wide range of locomotion speed than baseline controllers.",-2.0211794,3.7954414,gait,reinforce,170,"id:170 (gait)<br><b>Fast and Efficient Locomotion via Learned Gait Transitions</b><br>kw:Legged Locomotion, Hierarchical Control, Reinforcement Learning"
171,5,19,https://openreview.net/forum?id=d_SWJhyKfVw,Rapid Exploration for Open-World Navigation with Latent Goal Models,"Dhruv Shah, Benjamin Eysenbach, Nicholas Rhinehart, Sergey Levine",,,,,,,,False,,"inverse reinforcement learning, imitation learning, self-supervised learning","We investigate the visual cross-embodiment imitation setting, in which agents learn policies from videos of other agents (such as humans) demonstrating the same task, but with stark differences in their embodiments -- shape, actions, end-effector dynamics, etc. In this work, we demonstrate that it is possible to automatically discover and learn vision-based reward functions from cross-embodiment demonstration videos that are robust to these differences. Specifically, we present a self-supervised method for Cross-embodiment Inverse Reinforcement Learning (XIRL) that leverages temporal cycle-consistency constraints to learn deep visual embeddings that capture task progression from offline videos of demonstrations across multiple expert agents, each performing the same task differently due to embodiment differences. Prior to our work, producing rewards from self-supervised embeddings typically required alignment with a reference trajectory, which may be difficult to acquire under stark embodiment differences. We show empirically that if the embeddings are aware of task-progress, simply taking the negative distance between the current state and goal state in the learned embedding space is useful as a reward for training policies with reinforcement learning. We find our learned reward function not only works for embodiments seen during training, but also generalizes to entirely new embodiments. Additionally, when transferring real-world human demonstrations to a simulated robot, we find that XIRL is more sample efficient than current best methods.",,1.0,,2.0,"rapid exploration for open-world navigation with latent goal models inverse reinforcement learning, imitation learning, self-supervised learning we investigate the visual cross-embodiment imitation setting, in which agents learn policies from videos of other agents (such as humans) demonstrating the same task, but with stark differences in their embodiments -- shape, actions, end-effector dynamics, etc. in this work, we demonstrate that it is possible to automatically discover and learn vision-based reward functions from cross-embodiment demonstration videos that are robust to these differences. specifically, we present a self-supervised method for cross-embodiment inverse reinforcement learning (xirl) that leverages temporal cycle-consistency constraints to learn deep visual embeddings that capture task progression from offline videos of demonstrations across multiple expert agents, each performing the same task differently due to embodiment differences. prior to our work, producing rewards from self-supervised embeddings typically required alignment with a reference trajectory, which may be difficult to acquire under stark embodiment differences. we show empirically that if the embeddings are aware of task-progress, simply taking the negative distance between the current state and goal state in the learned embedding space is useful as a reward for training policies with reinforcement learning. we find our learned reward function not only works for embodiments seen during training, but also generalizes to entirely new embodiments. additionally, when transferring real-world human demonstrations to a simulated robot, we find that xirl is more sample efficient than current best methods.",-2.1652713,3.4725118,differences.,reinforce,171,"id:171 (differences.)<br><b>Rapid Exploration for Open-World Navigation with Latent Goal Models</b><br>kw:inverse reinforcement learning, imitation learning, self-supervised learning"
172,6,18,https://openreview.net/forum?id=ht3aHpc1hUt,Structure from Silence: Learning Scene Structure from Ambient Sound,"Ziyang Chen, Xixi Hu, Andrew Owens",,,,,,,,False,,"audio perception, multi-modal learning, self-supervision, navigation","The sounds that a robot hears within a scene subtly vary as it moves through it. In this paper, we ask whether these ambient sounds convey information about scene structure. To study this, we collect a dataset of ``in-the-wild'' paired audio and RGB-D recordings in a variety of quiet, indoor scenes, typical of what a robot would encounter when performing navigation tasks. Using this data, we train models that successfully estimate depth in a number of evaluation settings. Finally, we show that these sounds provide a useful learning signal, and that we can obtain a useful representation by associating a visual signal with sound. These results suggest that ambient sound conveys a surprising amount of information about scene structure, and that this information can be successfully exploited by learning systems.",,2.0,,2.0,"structure from silence: learning scene structure from ambient sound audio perception, multi-modal learning, self-supervision, navigation the sounds that a robot hears within a scene subtly vary as it moves through it. in this paper, we ask whether these ambient sounds convey information about scene structure. to study this, we collect a dataset of ``in-the-wild'' paired audio and rgb-d recordings in a variety of quiet, indoor scenes, typical of what a robot would encounter when performing navigation tasks. using this data, we train models that successfully estimate depth in a number of evaluation settings. finally, we show that these sounds provide a useful learning signal, and that we can obtain a useful representation by associating a visual signal with sound. these results suggest that ambient sound conveys a surprising amount of information about scene structure, and that this information can be successfully exploited by learning systems.",-1.5939242,3.7915123,ambient,no reinforce,172,"id:172 (ambient)<br><b>Structure from Silence: Learning Scene Structure from Ambient Sound</b><br>kw:audio perception, multi-modal learning, self-supervision, navigation"
173,7,28,https://openreview.net/forum?id=zv3NYgRZ7Qo,3D Neural Scene Representations for Visuomotor Control,"Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, Antonio Torralba",,,,,,,,False,,"learning-based dynamics modeling, 3d-aware representation learning, neural radiance field, robotic manipulation","Humans have a strong intuitive understanding of the 3D environment around us. The mental model of the physics in our brain applies to objects of different materials and enables us to perform a wide range of manipulation tasks that are far beyond the reach of current robots. In this work, we desire to learn models for dynamic 3D scenes purely from 2D visual observations. Our model combines Neural Radiance Fields (NeRF) and time contrastive learning with an autoencoding framework, which learns viewpoint-invariant 3D-aware scene representations. We show that a dynamics model, constructed over the learned representation space, enables visuomotor control for challenging manipulation tasks involving both rigid bodies and fluids, where the target is specified in a viewpoint different from what the robot operates on. When coupled with an auto-decoding framework, it can even support goal specification from camera viewpoints that are outside the training distribution. We further demonstrate the richness of the learned 3D dynamics model by performing future prediction and novel view synthesis. Finally, we provide detailed ablation studies regarding different system designs and qualitative analysis of the learned representations.",,3.0,,2.0,"3d neural scene representations for visuomotor control learning-based dynamics modeling, 3d-aware representation learning, neural radiance field, robotic manipulation humans have a strong intuitive understanding of the 3d environment around us. the mental model of the physics in our brain applies to objects of different materials and enables us to perform a wide range of manipulation tasks that are far beyond the reach of current robots. in this work, we desire to learn models for dynamic 3d scenes purely from 2d visual observations. our model combines neural radiance fields (nerf) and time contrastive learning with an autoencoding framework, which learns viewpoint-invariant 3d-aware scene representations. we show that a dynamics model, constructed over the learned representation space, enables visuomotor control for challenging manipulation tasks involving both rigid bodies and fluids, where the target is specified in a viewpoint different from what the robot operates on. when coupled with an auto-decoding framework, it can even support goal specification from camera viewpoints that are outside the training distribution. we further demonstrate the richness of the learned 3d dynamics model by performing future prediction and novel view synthesis. finally, we provide detailed ablation studies regarding different system designs and qualitative analysis of the learned representations.",-1.680139,3.7461455,3d-aware,no reinforce,173,"id:173 (3d-aware)<br><b>3D Neural Scene Representations for Visuomotor Control</b><br>kw:learning-based dynamics modeling, 3d-aware representation learning, neural radiance field, robotic manipulation"
174,8,406,https://openreview.net/forum?id=OQMXb0xiCrt,SeqMatchNet: Contrastive Learning with Sequence Matching for Place Recognition & Relocalization,"Sourav Garg, Madhu Vankadari, Michael Milford",,,,,,,,False,,"Relocalization, Localization, Visual Place Recognition, Sequence Matching, Contrastive Learning","Visual Place Recognition (VPR) for mobile robot global relocalization is a well-studied problem, where contrastive learning based representation training methods have led to state-of-the-art performance. However, these methods are mainly designed for single image based VPR, where sequential information, which is ubiquitous in robotics, is only used as a post-processing step for filtering single image match scores, but is never used to guide the representation learning process itself. In this work, for the first time, we bridge the gap between single image representation learning and sequence matching through SeqMatchNet which transforms the single image descriptors such that they become more responsive to the sequence matching metric. First, we propose a novel triplet loss formulation where the distance metric is based on sequence matching, that is, the aggregation of temporal order-based single-image Euclidean distances. We use the same metric for mining negatives online during the training which helps the optimization process by selecting appropriate positives and harder negatives. To overcome the computational overhead of sequence matching for negative mining, we propose a 2D convolution based formulation of sequence matching for efficiently aggregating distances within a distance matrix computed using single images. We show that our proposed method achieves consistent gains in performance as demonstrated on four benchmark datasets. Source code will be made publicly available.",,4.0,,2.0,"seqmatchnet: contrastive learning with sequence matching for place recognition & relocalization relocalization, localization, visual place recognition, sequence matching, contrastive learning visual place recognition (vpr) for mobile robot global relocalization is a well-studied problem, where contrastive learning based representation training methods have led to state-of-the-art performance. however, these methods are mainly designed for single image based vpr, where sequential information, which is ubiquitous in robotics, is only used as a post-processing step for filtering single image match scores, but is never used to guide the representation learning process itself. in this work, for the first time, we bridge the gap between single image representation learning and sequence matching through seqmatchnet which transforms the single image descriptors such that they become more responsive to the sequence matching metric. first, we propose a novel triplet loss formulation where the distance metric is based on sequence matching, that is, the aggregation of temporal order-based single-image euclidean distances. we use the same metric for mining negatives online during the training which helps the optimization process by selecting appropriate positives and harder negatives. to overcome the computational overhead of sequence matching for negative mining, we propose a 2d convolution based formulation of sequence matching for efficiently aggregating distances within a distance matrix computed using single images. we show that our proposed method achieves consistent gains in performance as demonstrated on four benchmark datasets. source code will be made publicly available.",-3.282158,3.6097918,matching,no reinforce,174,"id:174 (matching)<br><b>SeqMatchNet: Contrastive Learning with Sequence Matching for Place Recognition & Relocalization</b><br>kw:Relocalization, Localization, Visual Place Recognition, Sequence Matching, Contrastive Learning"
175,9,45,https://openreview.net/forum?id=0QJeE5hkyFZ,FlingBot: The Unreasonable Effectiveness of Dynamic Manipulation for Cloth Unfolding ,"Huy Ha, Shuran Song",,,,,☆,,,False,,"Dynamic manipulation, Cloth manipulation, Self-supervised learning","High-velocity dynamic actions (e.g., fling or throw) play a crucial role in our everyday interaction with deformable objects by improving our efficiency and effectively expanding our physical reach range. Yet, most prior works have tackled cloth manipulation using exclusively single-arm quasi-static actions, which requires a large number of interactions for challenging initial cloth configurations and strictly limits the maximum cloth size by the robot's reach range. In this work, we demonstrate the effectiveness of dynamic flinging actions for cloth unfolding with our proposed self-supervised learning framework, FlingBot.        Our approach learns how to unfold a piece of fabric from arbitrary initial configurations using a pick, stretch, and fling primitive for a dual-arm setup from visual observations. The final system achieves over 80% coverage within 3 actions on novel cloths,  can unfold cloths larger than the system's reach range, and generalizes to T-shirts despite being trained on only rectangular cloths.        We also finetuned FlingBot on a real-world dual-arm robot platform, where it increased the cloth coverage over 4 times more than the quasi-static baseline did. The simplicity of FlingBot combined with its superior performance over quasi-static baselines demonstrates the effectiveness of dynamic actions for deformable object manipulation.",,1.0,,3.0,"flingbot: the unreasonable effectiveness of dynamic manipulation for cloth unfolding  dynamic manipulation, cloth manipulation, self-supervised learning high-velocity dynamic actions (e.g., fling or throw) play a crucial role in our everyday interaction with deformable objects by improving our efficiency and effectively expanding our physical reach range. yet, most prior works have tackled cloth manipulation using exclusively single-arm quasi-static actions, which requires a large number of interactions for challenging initial cloth configurations and strictly limits the maximum cloth size by the robot's reach range. in this work, we demonstrate the effectiveness of dynamic flinging actions for cloth unfolding with our proposed self-supervised learning framework, flingbot.        our approach learns how to unfold a piece of fabric from arbitrary initial configurations using a pick, stretch, and fling primitive for a dual-arm setup from visual observations. the final system achieves over 80% coverage within 3 actions on novel cloths,  can unfold cloths larger than the system's reach range, and generalizes to t-shirts despite being trained on only rectangular cloths.        we also finetuned flingbot on a real-world dual-arm robot platform, where it increased the cloth coverage over 4 times more than the quasi-static baseline did. the simplicity of flingbot combined with its superior performance over quasi-static baselines demonstrates the effectiveness of dynamic actions for deformable object manipulation.",-2.9285975,3.593177,cloth,no reinforce,175,"id:175 (cloth)<br><b>FlingBot: The Unreasonable Effectiveness of Dynamic Manipulation for Cloth Unfolding </b><br>kw:Dynamic manipulation, Cloth manipulation, Self-supervised learning"
176,10,129,https://openreview.net/forum?id=JrsfBJtDFdI,What Matters in Learning from Offline Human Demonstrations for Robot Manipulation,"Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, Roberto Mart?n-Mart?n",,,,,☆,,,False,,"Imitation Learning, Offline Reinforcement Learning, Robot Manipulation","Imitating human demonstrations is a promising approach to endow robots with various manipulation capabilities. While recent advances have been made in imitation learning and batch (offline) reinforcement learning, a lack of open-source human datasets and reproducible learning methods make assessing the state of the field difficult. In this paper, we conduct an extensive study of six offline learning algorithms for robot manipulation on five simulated and three real-world multi-stage manipulation tasks of varying complexity, and with datasets of varying quality. Our study analyzes the most critical challenges when learning from offline human data for manipulation. Based on the study, we derive a series of lessons including the sensitivity to different algorithmic design choices, the dependence on the quality of the demonstrations, and the variability based on the stopping criteria due to the different objectives in training and evaluation. We also highlight opportunities for learning from human datasets, such as the ability to learn proficient policies on challenging, multi-stage tasks beyond the scope of current reinforcement learning methods, and the ability to easily scale to natural, real-world manipulation scenarios where only raw sensory signals are available. We have open-sourced our datasets and all algorithm implementations to facilitate future research and fair comparisons in learning from human demonstration data at https://arise-initiative.github.io/robomimic-web/",,2.0,,3.0,"what matters in learning from offline human demonstrations for robot manipulation imitation learning, offline reinforcement learning, robot manipulation imitating human demonstrations is a promising approach to endow robots with various manipulation capabilities. while recent advances have been made in imitation learning and batch (offline) reinforcement learning, a lack of open-source human datasets and reproducible learning methods make assessing the state of the field difficult. in this paper, we conduct an extensive study of six offline learning algorithms for robot manipulation on five simulated and three real-world multi-stage manipulation tasks of varying complexity, and with datasets of varying quality. our study analyzes the most critical challenges when learning from offline human data for manipulation. based on the study, we derive a series of lessons including the sensitivity to different algorithmic design choices, the dependence on the quality of the demonstrations, and the variability based on the stopping criteria due to the different objectives in training and evaluation. we also highlight opportunities for learning from human datasets, such as the ability to learn proficient policies on challenging, multi-stage tasks beyond the scope of current reinforcement learning methods, and the ability to easily scale to natural, real-world manipulation scenarios where only raw sensory signals are available. we have open-sourced our datasets and all algorithm implementations to facilitate future research and fair comparisons in learning from human demonstration data at https://arise-initiative.github.io/robomimic-web/",-3.0473073,3.0924008,human,reinforce,176,"id:176 (human)<br><b>What Matters in Learning from Offline Human Demonstrations for Robot Manipulation</b><br>kw:Imitation Learning, Offline Reinforcement Learning, Robot Manipulation"
177,11,284,https://openreview.net/forum?id=7uSBJDoP7tY,A Simple Method for Complex In-hand Manipulation,"Tao Chen, Jie Xu, Pulkit Agrawal",,,,,☆,,,False,,"In-hand manipulation, dexterous manipulation, object reorientation",In-hand object reorientation has been a challenging problem in robotics due to high dimensional actuation space and the frequent change in contact state between the fingers and the objects. We present a simple model-free framework that can learn to reorient objects with both the hand facing upwards and downwards. We demonstrate the capability of reorienting over 2000 geometrically different objects in both cases. The learned policies show strong zero-shot transfer performance on new objects. We provide evidence that these policies are amenable to real-world operation by distilling them to use observations easily available in the real world. The videos of the learned policies are available at: https://sites.google.com/view/in-hand-reorientation.,,3.0,,3.0,"a simple method for complex in-hand manipulation in-hand manipulation, dexterous manipulation, object reorientation in-hand object reorientation has been a challenging problem in robotics due to high dimensional actuation space and the frequent change in contact state between the fingers and the objects. we present a simple model-free framework that can learn to reorient objects with both the hand facing upwards and downwards. we demonstrate the capability of reorienting over 2000 geometrically different objects in both cases. the learned policies show strong zero-shot transfer performance on new objects. we provide evidence that these policies are amenable to real-world operation by distilling them to use observations easily available in the real world. the videos of the learned policies are available at: https://sites.google.com/view/in-hand-reorientation.",-1.0408518,3.517508,in-hand,no reinforce,177,"id:177 (in-hand)<br><b>A Simple Method for Complex In-hand Manipulation</b><br>kw:In-hand manipulation, dexterous manipulation, object reorientation"
178,12,344,https://openreview.net/forum?id=mOLu2rODIJF,SORNet: Spatial Object-Centric Representations for Sequential Manipulation,"Wentao Yuan, Chris Paxton, Karthik Desingh, Dieter Fox",,,,,☆,,,False,,"Object-centric Representation, Spatial Reasoning, Manipulation","Sequential manipulation tasks require a robot to perceive the state of an environment and plan a sequence of actions leading to a desired goal state, where the ability to reason about spatial relationships among object entities from raw sensor inputs is crucial. Prior works relying on explicit state estimation or end-to-end learning struggle with novel objects.         In this work, we propose SORNet (Spatial Object-Centric Representation Network), which extracts object-centric representations from RGB images % trained from large amounts of simulated robotic manipulation data, conditioned on canonical views of the objects of interest. We show that the object embeddings learned by SORNet generalize zero-shot to unseen object entities on three spatial reasoning tasks: spatial relationship classification, skill precondition classification and relative direction regression, significantly outperforming baselines. Further, we present real-world robotic experiments demonstrating the usage of the learned object embeddings in task planning for sequential manipulation.",,4.0,,3.0,"sornet: spatial object-centric representations for sequential manipulation object-centric representation, spatial reasoning, manipulation sequential manipulation tasks require a robot to perceive the state of an environment and plan a sequence of actions leading to a desired goal state, where the ability to reason about spatial relationships among object entities from raw sensor inputs is crucial. prior works relying on explicit state estimation or end-to-end learning struggle with novel objects.         in this work, we propose sornet (spatial object-centric representation network), which extracts object-centric representations from rgb images % trained from large amounts of simulated robotic manipulation data, conditioned on canonical views of the objects of interest. we show that the object embeddings learned by sornet generalize zero-shot to unseen object entities on three spatial reasoning tasks: spatial relationship classification, skill precondition classification and relative direction regression, significantly outperforming baselines. further, we present real-world robotic experiments demonstrating the usage of the learned object embeddings in task planning for sequential manipulation.",-1.7885004,3.5598872,spatial,no reinforce,178,"id:178 (spatial)<br><b>SORNet: Spatial Object-Centric Representations for Sequential Manipulation</b><br>kw:Object-centric Representation, Spatial Reasoning, Manipulation"
179,13,70,https://openreview.net/forum?id=KKBfrCzCVOn,ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,"Ryan Hoque, Ashwin Balakrishna, Ellen Novoseller, Daniel S. Brown, Albert Wilcox, Ken Goldberg",,,,,,,,False,,"Imitation Learning, Fleet Learning, Human Robot Interaction","Effective robot learning often requires online human feedback and interventions that can cost significant human time, giving rise to the central challenge in interactive imitation learning: is it possible to control the timing and length of interventions to both facilitate learning and limit burden on the human supervisor? This paper presents ThriftyDAgger, an algorithm for actively querying a human supervisor given a desired budget of human interventions. ThriftyDAgger uses a learned switching policy to solicit interventions only at states that are sufficiently (1) novel, where the robot policy has no reference behavior to imitate, or (2) risky, where the robot has low confidence in task completion. To detect the latter, we introduce a novel metric for estimating risk under the current robot policy. Experiments in simulation and on a physical cable routing experiment suggest that ThriftyDAgger's intervention criteria balances task performance and supervisor burden more effectively than prior algorithms. ThriftyDAgger can also be applied at execution time, where it achieves a 100% success rate on both the simulation and physical tasks. A user study (N=10) in which users control a three-robot fleet while also performing a concentration task suggests that ThriftyDAgger increases human and robot performance by 58% and 80% respectively compared to the next best algorithm while reducing supervisor burden. See https://tinyurl.com/thrifty-dagger for supplementary material.",,1.0,,4.0,"thriftydagger: budget-aware novelty and risk gating for interactive imitation learning imitation learning, fleet learning, human robot interaction effective robot learning often requires online human feedback and interventions that can cost significant human time, giving rise to the central challenge in interactive imitation learning: is it possible to control the timing and length of interventions to both facilitate learning and limit burden on the human supervisor? this paper presents thriftydagger, an algorithm for actively querying a human supervisor given a desired budget of human interventions. thriftydagger uses a learned switching policy to solicit interventions only at states that are sufficiently (1) novel, where the robot policy has no reference behavior to imitate, or (2) risky, where the robot has low confidence in task completion. to detect the latter, we introduce a novel metric for estimating risk under the current robot policy. experiments in simulation and on a physical cable routing experiment suggest that thriftydagger's intervention criteria balances task performance and supervisor burden more effectively than prior algorithms. thriftydagger can also be applied at execution time, where it achieves a 100% success rate on both the simulation and physical tasks. a user study (n=10) in which users control a three-robot fleet while also performing a concentration task suggests that thriftydagger increases human and robot performance by 58% and 80% respectively compared to the next best algorithm while reducing supervisor burden. see https://tinyurl.com/thrifty-dagger for supplementary material.",-3.6187756,3.365725,thriftydagger,no reinforce,179,"id:179 (thriftydagger)<br><b>ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning</b><br>kw:Imitation Learning, Fleet Learning, Human Robot Interaction"
180,14,81,https://openreview.net/forum?id=YwDvofEWlEx,Learning Behaviors through Physics-driven Latent Imagination,"Antoine Richard, Stephanie ARAVECCHIA, Matthieu Geist, C?dric Pradalier",,,,,,,,False,,"Model-Based Reinforcement Learning, Field Robotics, Latent Models","Model-based reinforcement learning (MBRL) consists in learning a so-called world model, a representation of the environment through interactions with it, then use it to train an agent. This approach is particularly interesting in the context of field robotics, as it alleviates the need to train online, and reduces the risks inherent to directly training agents on real robots. However, in such approaches, the world encompasses both the part related to the robot itself and the rest of the environment.  We argue that decoupling the environment representation (for example, images or laser scans) from the dynamics of the physical system (that is, the robot and its physical state) may be beneficial:it can increase the flexibility of world models and open the door to further robustification. In this paper, we apply this concept to a strong latent-agent, Dreamer. We then showcase the increase of flexibility by transferring the environmental part of the world model from one robot (a boat) to another (a rover), simply by adapting the physical model in the imagination. We also demonstrate the robustness of our method through real-world experiments on a boat.",,2.0,,4.0,"learning behaviors through physics-driven latent imagination model-based reinforcement learning, field robotics, latent models model-based reinforcement learning (mbrl) consists in learning a so-called world model, a representation of the environment through interactions with it, then use it to train an agent. this approach is particularly interesting in the context of field robotics, as it alleviates the need to train online, and reduces the risks inherent to directly training agents on real robots. however, in such approaches, the world encompasses both the part related to the robot itself and the rest of the environment.  we argue that decoupling the environment representation (for example, images or laser scans) from the dynamics of the physical system (that is, the robot and its physical state) may be beneficial:it can increase the flexibility of world models and open the door to further robustification. in this paper, we apply this concept to a strong latent-agent, dreamer. we then showcase the increase of flexibility by transferring the environmental part of the world model from one robot (a boat) to another (a rover), simply by adapting the physical model in the imagination. we also demonstrate the robustness of our method through real-world experiments on a boat.",-1.5537577,3.5645466,world,reinforce,180,"id:180 (world)<br><b>Learning Behaviors through Physics-driven Latent Imagination</b><br>kw:Model-Based Reinforcement Learning, Field Robotics, Latent Models"
181,15,251,https://openreview.net/forum?id=n6xYib0irVR,Influencing Towards Stable Multi-Agent Interactions,"Woodrow Zhouyuan Wang, Andy Shih, Annie Xie, Dorsa Sadigh",,,,,,,,False,,"multi-agent interactions, human-robot interaction, non-stationarity","Learning in multi-agent environments is difficult due to the non-stationarity introduced by an opponent's or partner's changing behaviors. Instead of reactively adapting to the other agent's (opponent or partner) behavior, we propose an algorithm to proactively influence the other agent's strategy to stabilize -- which can restrain the non-stationarity caused by the other agent. We learn a low-dimensional latent representation of the other agent's strategy and the dynamics of how the latent strategy evolves with respect to our robot's behavior. With this learned dynamics model, we can define an unsupervised stability reward to train our robot to deliberately influence the other agent to stabilize towards a single strategy. We demonstrate the effectiveness of stabilizing in improving efficiency of maximizing the task reward in a variety of simulated environments, including autonomous driving, emergent communication, and robotic manipulation.",,3.0,,4.0,"influencing towards stable multi-agent interactions multi-agent interactions, human-robot interaction, non-stationarity learning in multi-agent environments is difficult due to the non-stationarity introduced by an opponent's or partner's changing behaviors. instead of reactively adapting to the other agent's (opponent or partner) behavior, we propose an algorithm to proactively influence the other agent's strategy to stabilize -- which can restrain the non-stationarity caused by the other agent. we learn a low-dimensional latent representation of the other agent's strategy and the dynamics of how the latent strategy evolves with respect to our robot's behavior. with this learned dynamics model, we can define an unsupervised stability reward to train our robot to deliberately influence the other agent to stabilize towards a single strategy. we demonstrate the effectiveness of stabilizing in improving efficiency of maximizing the task reward in a variety of simulated environments, including autonomous driving, emergent communication, and robotic manipulation.",-2.1446939,3.4133003,non-stationarity,no reinforce,181,"id:181 (non-stationarity)<br><b>Influencing Towards Stable Multi-Agent Interactions</b><br>kw:multi-agent interactions, human-robot interaction, non-stationarity"
182,16,328,https://openreview.net/forum?id=RO4DM85Z4P7,XIRL: Cross-embodiment Inverse Reinforcement Learning,"Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, Debidatta Dwibedi",,,,,,,,False,,--,--,,4.0,,4.0,xirl: cross-embodiment inverse reinforcement learning -- --,-0.03476818,0.6317616,xirl:,reinforce,182,id:182 (xirl:)<br><b>XIRL: Cross-embodiment Inverse Reinforcement Learning</b><br>kw:--
183,17,324,https://openreview.net/forum?id=s3tu6Ma1jga,Motivating Physical Activity via Competitive Human-Robot Interaction,"Boling Yang, Golnaz Habibi, Patrick Lancaster, Byron Boots, Joshua Smith",,,,,,,,False,,"Competitive Human-robot Interaction, Reinforcement Learning, HRI, Adversarial Learning, Multi-agent System","This project aims to motivate research in competitive human-robot interaction by creating a robot competitor that can challenge human users in certain scenarios such as physical exercise and games. With this goal in mind, we introduce the Fencing Game, a human-robot competition used to evaluate both the capabilities of the robot competitor and user experience. We develop the robot competitor through multi-agent reinforcement learning and show that it can perform well against human competitors. Our user study additionally found that our system was able to continuously create challenging and enjoyable interactions for humans and the majority of human subjects considered the system to be entertaining and useful for improving the quality of their exercise.",,1.0,,5.0,"motivating physical activity via competitive human-robot interaction competitive human-robot interaction, reinforcement learning, hri, adversarial learning, multi-agent system this project aims to motivate research in competitive human-robot interaction by creating a robot competitor that can challenge human users in certain scenarios such as physical exercise and games. with this goal in mind, we introduce the fencing game, a human-robot competition used to evaluate both the capabilities of the robot competitor and user experience. we develop the robot competitor through multi-agent reinforcement learning and show that it can perform well against human competitors. our user study additionally found that our system was able to continuously create challenging and enjoyable interactions for humans and the majority of human subjects considered the system to be entertaining and useful for improving the quality of their exercise.",-2.2521248,3.3945682,competitor,reinforce,183,"id:183 (competitor)<br><b>Motivating Physical Activity via Competitive Human-Robot Interaction</b><br>kw:Competitive Human-robot Interaction, Reinforcement Learning, HRI, Adversarial Learning, Multi-agent System"
184,18,372,https://openreview.net/forum?id=s8xjoLghedM,Learning Multimodal Rewards from Rankings,"Vivek Myers, Erdem Biyik, Nima Anari, Dorsa Sadigh",,,,,,,,False,,"HRI, reward learning, multi-modality, rankings, active learning","Learning from human feedback has shown to be a useful approach in acquiring robot reward functions. However, expert feedback is often assumed to be drawn from an underlying unimodal reward function. This assumption does not always hold including in settings where multiple experts provide data or when a single expert provides data for different tasks---we thus go beyond learning a unimodal reward and focus on learning a multimodal reward function. We formulate the multimodal reward learning as a mixture learning problem and develop a novel ranking-based learning approach, where the experts are only required to rank a given set of trajectories. Furthermore, as access to interaction data is often expensive in robotics, we develop an active querying approach to accelerate the learning process. We conduct experiments and user studies using a multi-task variant of OpenAI's LunarLander and a real Fetch robot, where we collect data from multiple users with different preferences. The results suggest that our approach can efficiently learn multimodal reward functions, and improve data-efficiency over benchmark methods that we adapt to our learning problem.",,2.0,,5.0,"learning multimodal rewards from rankings hri, reward learning, multi-modality, rankings, active learning learning from human feedback has shown to be a useful approach in acquiring robot reward functions. however, expert feedback is often assumed to be drawn from an underlying unimodal reward function. this assumption does not always hold including in settings where multiple experts provide data or when a single expert provides data for different tasks---we thus go beyond learning a unimodal reward and focus on learning a multimodal reward function. we formulate the multimodal reward learning as a mixture learning problem and develop a novel ranking-based learning approach, where the experts are only required to rank a given set of trajectories. furthermore, as access to interaction data is often expensive in robotics, we develop an active querying approach to accelerate the learning process. we conduct experiments and user studies using a multi-task variant of openai's lunarlander and a real fetch robot, where we collect data from multiple users with different preferences. the results suggest that our approach can efficiently learn multimodal reward functions, and improve data-efficiency over benchmark methods that we adapt to our learning problem.",-1.7898998,3.6684418,reward,no reinforce,184,"id:184 (reward)<br><b>Learning Multimodal Rewards from Rankings</b><br>kw:HRI, reward learning, multi-modality, rankings, active learning"
185,19,384,https://openreview.net/forum?id=fy4ZBWxYbIo,A Workflow for Offline Model-Free Robotic Reinforcement Learning,"Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, Sergey Levine",,,,,,,,False,,"workflow, offline RL, no online tuning","Offline reinforcement learning (RL) enables learning control policies by utilizing only prior experience, without any online interaction. This can allow robots to acquire generalizable skills from large and diverse datasets, without costly online data collection. Despite recent algorithmic advances in offline RL, applying these methods to real-world problems has proven challenging. Although offline RL methods can learn from prior data, there is no clear and well-understood process for making various design choices, from model architecture to algorithm hyperparameters, without actually evaluating the learned policies online. In this paper, our aim is to develop a practical workflow for applying offline RL analogous to the relatively well-understood workflows for supervised learning problems. To this end, we devise a set of metrics and conditions that can be tracked over the course of offline training, and can inform the practitioner about how the algorithm and model architecture should be adjusted to improve final performance. Our workflow is derived from a conceptual understanding of the behavior of conservative offline RL algorithms and cross-validation in supervised learning. We demonstrate the efficacy of this workflow in producing effective policies without any online tuning, both in several simulated robotic learning scenarios and for three tasks on two distinct real robots, focusing on learning manipulation skills with raw image observations with sparse binary rewards.",,3.0,,5.0,"a workflow for offline model-free robotic reinforcement learning workflow, offline rl, no online tuning offline reinforcement learning (rl) enables learning control policies by utilizing only prior experience, without any online interaction. this can allow robots to acquire generalizable skills from large and diverse datasets, without costly online data collection. despite recent algorithmic advances in offline rl, applying these methods to real-world problems has proven challenging. although offline rl methods can learn from prior data, there is no clear and well-understood process for making various design choices, from model architecture to algorithm hyperparameters, without actually evaluating the learned policies online. in this paper, our aim is to develop a practical workflow for applying offline rl analogous to the relatively well-understood workflows for supervised learning problems. to this end, we devise a set of metrics and conditions that can be tracked over the course of offline training, and can inform the practitioner about how the algorithm and model architecture should be adjusted to improve final performance. our workflow is derived from a conceptual understanding of the behavior of conservative offline rl algorithms and cross-validation in supervised learning. we demonstrate the efficacy of this workflow in producing effective policies without any online tuning, both in several simulated robotic learning scenarios and for three tasks on two distinct real robots, focusing on learning manipulation skills with raw image observations with sparse binary rewards.",-3.3267567,3.3792932,offline,reinforce,185,"id:185 (offline)<br><b>A Workflow for Offline Model-Free Robotic Reinforcement Learning</b><br>kw:workflow, offline RL, no online tuning"
186,20,158,https://openreview.net/forum?id=1GNV9SW95eJ,Learning Off-Policy with Online Planning,"Harshit Sikchi, Wenxuan Zhou, David Held",,,,,,,,False,,"Reinforcement Learning, Trajectory Optimization, Safety","Reinforcement learning (RL) in low-data and risk-sensitive domains requires performant and flexible deployment policies that can readily incorporate constraints during deployment. One such class of policies are the semi-parametric H-step lookahead policies, which select actions using trajectory optimization over a dynamics model for a fixed horizon with a terminal value function. In this work, we investigate a novel instantiation of H-step lookahead with a learned model and a terminal value function learned by a model-free off-policy algorithm, named Learning Off-Policy with Online Planning (LOOP). We provide a theoretical analysis of this method, suggesting a tradeoff between model errors and value function errors, and empirically demonstrate this tradeoff to be beneficial in deep reinforcement learning. Furthermore, we identify the ""Actor Divergence"" issue in this framework and propose Actor Regularized Control (ARC), a modified trajectory optimization procedure. We evaluate our method on a set of robotic tasks for Offline and Online RL and demonstrate improved performance. We also show the flexibility of LOOP to incorporate safety constraints during deployment with a set of navigation environments. We demonstrate that LOOP is a desirable framework for robotics applications based on its strong performance in various important RL settings.",,4.0,,5.0,"learning off-policy with online planning reinforcement learning, trajectory optimization, safety reinforcement learning (rl) in low-data and risk-sensitive domains requires performant and flexible deployment policies that can readily incorporate constraints during deployment. one such class of policies are the semi-parametric h-step lookahead policies, which select actions using trajectory optimization over a dynamics model for a fixed horizon with a terminal value function. in this work, we investigate a novel instantiation of h-step lookahead with a learned model and a terminal value function learned by a model-free off-policy algorithm, named learning off-policy with online planning (loop). we provide a theoretical analysis of this method, suggesting a tradeoff between model errors and value function errors, and empirically demonstrate this tradeoff to be beneficial in deep reinforcement learning. furthermore, we identify the ""actor divergence"" issue in this framework and propose actor regularized control (arc), a modified trajectory optimization procedure. we evaluate our method on a set of robotic tasks for offline and online rl and demonstrate improved performance. we also show the flexibility of loop to incorporate safety constraints during deployment with a set of navigation environments. we demonstrate that loop is a desirable framework for robotics applications based on its strong performance in various important rl settings.",-2.9199264,3.151841,off-policy,reinforce,186,"id:186 (off-policy)<br><b>Learning Off-Policy with Online Planning</b><br>kw:Reinforcement Learning, Trajectory Optimization, Safety"
187,21,96,https://openreview.net/forum?id=WIE9t_UwOpM,Enhancing Consistent Ground Maneuverability by Robot Adaptation to Complex Off-Road Terrains,"Sriram Siva, Maggie Wigness, John Rogers, Hao Zhang",,,,,,,,False,,"Robot Learning, Off-road Navigation, Terrain Adaptation","Terrain adaptation is a critical ability for a ground robot to effectively traverse unstructured off-road terrain in real-world field environments such as forests. However, the expected or planned maneuvering behaviors cannot always be accurately executed due to setbacks such as reduced tire pressure. This inconsistency negatively affects the robot's ground maneuverability and can cause slower traversal time or errors in localization.  To address this shortcoming, we propose a novel method for consistent behavior generation that enables a ground robot's actual behaviors to more accurately match expected behaviors while adapting to a variety of complex off-road terrains. Our method learns offset behaviors in a self-supervised fashion to compensate for the inconsistency between the actual and expected behaviors without requiring the explicit modeling of various setbacks. To evaluate the method, we perform extensive experiments using a physical ground robot over diverse complex off-road terrain in real-world field environments.  Experimental results show that our method enables a robot to improve its ground maneuverability on complex unstructured off-road terrain with more navigational behavior consistency, and outperforms previous and baseline methods, particularly so on challenging terrain such as that which is seen in forests.",,5.0,,5.0,"enhancing consistent ground maneuverability by robot adaptation to complex off-road terrains robot learning, off-road navigation, terrain adaptation terrain adaptation is a critical ability for a ground robot to effectively traverse unstructured off-road terrain in real-world field environments such as forests. however, the expected or planned maneuvering behaviors cannot always be accurately executed due to setbacks such as reduced tire pressure. this inconsistency negatively affects the robot's ground maneuverability and can cause slower traversal time or errors in localization.  to address this shortcoming, we propose a novel method for consistent behavior generation that enables a ground robot's actual behaviors to more accurately match expected behaviors while adapting to a variety of complex off-road terrains. our method learns offset behaviors in a self-supervised fashion to compensate for the inconsistency between the actual and expected behaviors without requiring the explicit modeling of various setbacks. to evaluate the method, we perform extensive experiments using a physical ground robot over diverse complex off-road terrain in real-world field environments.  experimental results show that our method enables a robot to improve its ground maneuverability on complex unstructured off-road terrain with more navigational behavior consistency, and outperforms previous and baseline methods, particularly so on challenging terrain such as that which is seen in forests.",-0.95743066,4.848297,off-road,no reinforce,187,"id:187 (off-road)<br><b>Enhancing Consistent Ground Maneuverability by Robot Adaptation to Complex Off-Road Terrains</b><br>kw:Robot Learning, Off-road Navigation, Terrain Adaptation"
188,22,174,https://openreview.net/forum?id=tCfLLiP7vje,Seeing Glass: Joint Point-Cloud and Depth Completion for Transparent Objects,"Haoping Xu, Yi Ru Wang, Sagi Eppel, Alan Aspuru-Guzik, Florian Shkurti, Animesh Garg",,,,,☆,,,False,,"Transparent Objects, Depth Completion, 3D Perception, Data Collection","The basis of many object manipulation algorithms is RGB-D input. Yet, commodity RGB-D sensors can only provide distorted depth maps for a wide range of transparent objects due light refraction and absorption. To tackle the perception challenges posed by transparent objects, we propose TranspareNet, a joint point cloud and depth completion method, with the ability to complete the depth of transparent objects in cluttered and complex scenes, even with partially filled fluid contents within the vessels. To address the shortcomings of existing transparent object data collection schemes in literature, we also propose an automated dataset creation workflow that consists of robot-controlled image collection and vision-based automatic annotation. Through this automated workflow, we created Transparent Object Depth Dataset (TODD), which consists of nearly 15000 RGB-D images. Our experimental evaluation demonstrates that TranspareNet outperforms existing state-of-the-art depth completion methods on multiple datasets, including ClearGrasp, and that it also handles cluttered scenes when trained on TODD. Code and dataset will be released at https://anonymous.4open.science/r/TranspareNet-567C/",,1.0,,6.0,"seeing glass: joint point-cloud and depth completion for transparent objects transparent objects, depth completion, 3d perception, data collection the basis of many object manipulation algorithms is rgb-d input. yet, commodity rgb-d sensors can only provide distorted depth maps for a wide range of transparent objects due light refraction and absorption. to tackle the perception challenges posed by transparent objects, we propose transparenet, a joint point cloud and depth completion method, with the ability to complete the depth of transparent objects in cluttered and complex scenes, even with partially filled fluid contents within the vessels. to address the shortcomings of existing transparent object data collection schemes in literature, we also propose an automated dataset creation workflow that consists of robot-controlled image collection and vision-based automatic annotation. through this automated workflow, we created transparent object depth dataset (todd), which consists of nearly 15000 rgb-d images. our experimental evaluation demonstrates that transparenet outperforms existing state-of-the-art depth completion methods on multiple datasets, including cleargrasp, and that it also handles cluttered scenes when trained on todd. code and dataset will be released at https://anonymous.4open.science/r/transparenet-567c/",-1.0672092,4.4811516,transparent,no reinforce,188,"id:188 (transparent)<br><b>Seeing Glass: Joint Point-Cloud and Depth Completion for Transparent Objects</b><br>kw:Transparent Objects, Depth Completion, 3D Perception, Data Collection"
189,23,294,https://openreview.net/forum?id=wVIqlSqKu2D,Differentiable Rendering and Identification of Impact Sounds,"Samuel Clarke, Negin Heravi, Mark Rau, Ruohan Gao, Jiajun Wu, Doug James, Jeannette Bohg",,,,,,,,False,,"Differentiable Sound Rendering, Auditory Scene Analysis","Rigid objects make distinctive sounds during manipulation. These sounds are a function of object features, such as shape and material, and of contact forces during manipulation. Being able to infer from sound an object's acoustic properties, how it is being manipulated, and what events it is participating in could augment and complement what robots can perceive from vision, especially in case of occlusion, low visual resolution, poor lighting, or blurred focus. Annotations on sound data are rare. Therefore, existing inference systems mostly include a sound renderer in the loop, and use analysis-by-synthesis to optimize for object acoustic properties. Optimizing parameters with respect to a non-differentiable renderer is slow and hard to scale to complex scenes. We present DiffImpact, a fully differentiable model for sounds rigid objects make during impacts, based on physical principles of impact forces, rigid object vibration, and other acoustic effects. Its differentiability enables gradient-based, efficient joint inference of acoustic properties of the objects and characteristics and timings of each individual impact. DiffImpact can also be plugged in as the decoder of an autoencoder, and trained end-to-end on real audio data, so that the encoder can learn to solve the inverse problem in a self-supervised way. Experiments demonstrate that our model's physics-based inductive biases make it more resource efficient and expressive than state-of-the-art pure learning-based alternatives, on both forward rendering of impact sounds and inverse tasks such as acoustic property inference and blind source separation of impact sounds.",,2.0,,6.0,"differentiable rendering and identification of impact sounds differentiable sound rendering, auditory scene analysis rigid objects make distinctive sounds during manipulation. these sounds are a function of object features, such as shape and material, and of contact forces during manipulation. being able to infer from sound an object's acoustic properties, how it is being manipulated, and what events it is participating in could augment and complement what robots can perceive from vision, especially in case of occlusion, low visual resolution, poor lighting, or blurred focus. annotations on sound data are rare. therefore, existing inference systems mostly include a sound renderer in the loop, and use analysis-by-synthesis to optimize for object acoustic properties. optimizing parameters with respect to a non-differentiable renderer is slow and hard to scale to complex scenes. we present diffimpact, a fully differentiable model for sounds rigid objects make during impacts, based on physical principles of impact forces, rigid object vibration, and other acoustic effects. its differentiability enables gradient-based, efficient joint inference of acoustic properties of the objects and characteristics and timings of each individual impact. diffimpact can also be plugged in as the decoder of an autoencoder, and trained end-to-end on real audio data, so that the encoder can learn to solve the inverse problem in a self-supervised way. experiments demonstrate that our model's physics-based inductive biases make it more resource efficient and expressive than state-of-the-art pure learning-based alternatives, on both forward rendering of impact sounds and inverse tasks such as acoustic property inference and blind source separation of impact sounds.",-0.6345263,5.0932326,acoustic,no reinforce,189,"id:189 (acoustic)<br><b>Differentiable Rendering and Identification of Impact Sounds</b><br>kw:Differentiable Sound Rendering, Auditory Scene Analysis"
190,24,389,https://openreview.net/forum?id=-JwmfQC6IRt,Guided Imitation of Task and Motion Planning,"Michael James McDonald, Dylan Hadfield-Menell",,,,,☆,,,False,,"task and motion planning, mobile manipulation, imitation learning","While modern policy optimization methods can do complex manipulation from sensor data, they struggle on problems with extended time horizons and multiple sub-goals. On the other hand, task and motion planning (TAMP) methods scale to long horizons but they are computationally expensive and need to precisely track world state. We propose a method that tries to get the best of both worlds: we train a policy to imitate a TAMP solver's output. This produces a feed-forward policy that can accomplish multi-step tasks from sensory data. First, we build an asynchronous distributed TAMP solver that can produce supervision data fast enough for imitation learning. Then, we propose a hierarchical policy architecture that lets us use partially trained control policies to speed up the TAMP solver. In robotic manipulation tasks, the partially trained policies reduce the time needed for planning by a factor of up to 2.5. We show results in a range of pick-place tasks, solving the 4 object pick-place task from RoboSuite 88% of the time from object pose observations and solving the RoboDesk 9-goal multitask benchmark 67% of the time from RGB images.",,3.0,,6.0,"guided imitation of task and motion planning task and motion planning, mobile manipulation, imitation learning while modern policy optimization methods can do complex manipulation from sensor data, they struggle on problems with extended time horizons and multiple sub-goals. on the other hand, task and motion planning (tamp) methods scale to long horizons but they are computationally expensive and need to precisely track world state. we propose a method that tries to get the best of both worlds: we train a policy to imitate a tamp solver's output. this produces a feed-forward policy that can accomplish multi-step tasks from sensory data. first, we build an asynchronous distributed tamp solver that can produce supervision data fast enough for imitation learning. then, we propose a hierarchical policy architecture that lets us use partially trained control policies to speed up the tamp solver. in robotic manipulation tasks, the partially trained policies reduce the time needed for planning by a factor of up to 2.5. we show results in a range of pick-place tasks, solving the 4 object pick-place task from robosuite 88% of the time from object pose observations and solving the robodesk 9-goal multitask benchmark 67% of the time from rgb images.",-2.790208,3.3653357,tamp,no reinforce,190,"id:190 (tamp)<br><b>Guided Imitation of Task and Motion Planning</b><br>kw:task and motion planning, mobile manipulation, imitation learning"
191,25,331,https://openreview.net/forum?id=ceOmpjMhlyS,STORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation,"Mohak Bhardwaj, Balakumar Sundaralingam, Arsalan Mousavian, Nathan D. Ratliff, Dieter Fox, Fabio Ramos, Byron Boots",,,,,☆,,,False,,"model-predictive control, manipulation","Sampling-based model-predictive control (MPC) is a promising tool for feedback control of robots with complex, non-smooth dynamics, and cost functions. However, the computationally demanding nature of sampling-based MPC algorithms has been a key bottleneck in their application to high-dimensional robotic manipulation problems in the real world. Previous methods have addressed this issue by running MPC in the task space while relying on a low-level operational space controller for joint control. However, by not using the joint space of the robot in the MPC formulation, existing methods cannot directly account for non-task space related constraints such as avoiding joint limits, singular configurations, and link collisions. In this paper, we develop a system for fast, joint space sampling-based MPC for manipulators that is efficiently parallelized using GPUs. Our approach can handle task and joint space constraints while taking less than 8ms~(125Hz) to compute the next control command. Further, our method can tightly integrate perception into the control problem by utilizing learned cost functions from raw sensor data. We validate our approach by deploying it on a Franka Panda robot for a variety of dynamic manipulation tasks. We study the effect of different cost formulations and MPC parameters on the synthesized behavior and provide key insights that pave the way for the application of sampling-based MPC for manipulators in a principled manner. We also provide highly optimized, open-source code to be used by the wider robot learning and control community. Videos of experiments can be found at: https://sites.google.com/view/manipulation-mpc",,4.0,,6.0,"storm: an integrated framework for fast joint-space model-predictive control for reactive manipulation model-predictive control, manipulation sampling-based model-predictive control (mpc) is a promising tool for feedback control of robots with complex, non-smooth dynamics, and cost functions. however, the computationally demanding nature of sampling-based mpc algorithms has been a key bottleneck in their application to high-dimensional robotic manipulation problems in the real world. previous methods have addressed this issue by running mpc in the task space while relying on a low-level operational space controller for joint control. however, by not using the joint space of the robot in the mpc formulation, existing methods cannot directly account for non-task space related constraints such as avoiding joint limits, singular configurations, and link collisions. in this paper, we develop a system for fast, joint space sampling-based mpc for manipulators that is efficiently parallelized using gpus. our approach can handle task and joint space constraints while taking less than 8ms~(125hz) to compute the next control command. further, our method can tightly integrate perception into the control problem by utilizing learned cost functions from raw sensor data. we validate our approach by deploying it on a franka panda robot for a variety of dynamic manipulation tasks. we study the effect of different cost formulations and mpc parameters on the synthesized behavior and provide key insights that pave the way for the application of sampling-based mpc for manipulators in a principled manner. we also provide highly optimized, open-source code to be used by the wider robot learning and control community. videos of experiments can be found at: https://sites.google.com/view/manipulation-mpc",-2.136981,3.8388066,mpc,no reinforce,191,"id:191 (mpc)<br><b>STORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation</b><br>kw:model-predictive control, manipulation"
