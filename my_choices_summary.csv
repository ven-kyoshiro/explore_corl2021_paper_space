,id,Title,OR Link,Keyword,Abstract
18,18,Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning,https://openreview.net/forum?id=wK2fDDJ5VcF,"Reinforcement Learning, Legged Robots, Sim-to-Real","In this work, we present and study a training set-up that achieves fast policy generation for real-world robotic tasks by using massive parallelism on a single workstation GPU. We analyze and discuss the impact of different training algorithm components in the massively parallel regime on the final policy performance and training times. In addition, we present a novel game-inspired curriculum that is well suited for training with thousands of simulated robots in parallel. We evaluate the approach by training the quadrupedal robot ANYmal to walk on challenging terrain. The parallel approach allows training policies for flat terrain in under four minutes, and in twenty minutes for uneven terrain. This represents a speedup of multiple orders of magnitude compared to previous work. Finally, we transfer the policies to the real robot to validate the approach."
65,65,Learning Behaviors through Physics-driven Latent Imagination,https://openreview.net/forum?id=YwDvofEWlEx,"Model-Based Reinforcement Learning, Field Robotics, Latent Models","Model-based reinforcement learning (MBRL) consists in learning a so-called world model, a representation of the environment through interactions with it, then use it to train an agent. This approach is particularly interesting in the context of field robotics, as it alleviates the need to train online, and reduces the risks inherent to directly training agents on real robots. However, in such approaches, the world encompasses both the part related to the robot itself and the rest of the environment.  We argue that decoupling the environment representation (for example, images or laser scans) from the dynamics of the physical system (that is, the robot and its physical state) may be beneficial:it can increase the flexibility of world models and open the door to further robustification. In this paper, we apply this concept to a strong latent-agent, Dreamer. We then showcase the increase of flexibility by transferring the environmental part of the world model from one robot (a boat) to another (a rover), simply by adapting the physical model in the imagination. We also demonstrate the robustness of our method through real-world experiments on a boat."
71,71,Self-supervised Reinforcement Learning with Independently Controllable Subgoals,https://openreview.net/forum?id=TEQWRlncJVm,"object-centric representations, relations, self-supervised reinforcement learning","To successfully tackle challenging manipulation tasks, autonomous agents must learn a diverse set of skills and how to combine them.        Recently, self-supervised agents that set their own abstract goals by exploiting the discovered structure in the environment were shown to perform well on many different tasks.        In particular, some of them were applied to learn basic manipulation skills in compositional multi-object environments.         However, these methods learn skills without taking the dependencies between objects into account. Thus, the learned skills are difficult to combine in realistic environments.        We propose a novel self-supervised agent that estimates relations between environment components and uses them to independently control different parts of the environment state. In addition, the estimated relations between objects can be used to decompose a complex goal into a compatible sequence of subgoals.        We show that, by using this framework, an agent can efficiently and automatically learn manipulation tasks in multi-object environments with different relations between objects."
92,92,Motivating Physical Activity via Competitive Human-Robot Interaction,https://openreview.net/forum?id=s3tu6Ma1jga,"Competitive Human-robot Interaction, Reinforcement Learning, HRI, Adversarial Learning, Multi-agent System","This project aims to motivate research in competitive human-robot interaction by creating a robot competitor that can challenge human users in certain scenarios such as physical exercise and games. With this goal in mind, we introduce the Fencing Game, a human-robot competition used to evaluate both the capabilities of the robot competitor and user experience. We develop the robot competitor through multi-agent reinforcement learning and show that it can perform well against human competitors. Our user study additionally found that our system was able to continuously create challenging and enjoyable interactions for humans and the majority of human subjects considered the system to be entertaining and useful for improving the quality of their exercise."
100,100,Learning to Predict Vehicle Trajectories with Model-based Planning,https://openreview.net/forum?id=GhMZNcr54zt,"Trajectory Prediction, Autonomous Driving","Predicting the future trajectories of on-road vehicles is critical for autonomous driving. In this paper, we introduce a novel prediction framework called PRIME, which stands for Prediction with Model-based Planning. Unlike recent prediction works that utilize neural networks to model scene context and produce unconstrained trajectories, PRIME is designed to generate accurate and feasibility-guaranteed future trajectory predictions. PRIME guarantees the trajectory feasibility by exploiting a model-based generator to produce future trajectories under explicit constraints and enables accurate multimodal prediction by utilizing a learning-based evaluator to select future trajectories. We conduct experiments on the large-scale Argoverse Motion Forecasting Benchmark, where PRIME outperforms the state-of-the-art methods in prediction accuracy, feasibility, and robustness under imperfect tracking."
107,107,Learning to Plan Optimistically: Uncertainty-Guided Deep Exploration via Latent Model Ensembles,https://openreview.net/forum?id=pXpytHo_GC1,"Learning Control, Sample Efficiency, Exploration","Learning complex robot behaviors through interaction requires structured exploration. Planning should target interactions with the potential to optimize long-term performance, while only reducing uncertainty where conducive to this objective. This paper presents Latent Optimistic Value Exploration (LOVE), a strategy that enables deep exploration through optimism in the face of uncertain long-term rewards. We combine latent world models with value function estimation to predict infinite-horizon returns and recover associated uncertainty via ensembling. The policy is then trained on an upper confidence bound (UCB) objective to identify and select the interactions most promising to improve long-term performance. We apply LOVE to visual robot control tasks in continuous action spaces and demonstrate on average more than 15% improved sample efficiency in comparison to state-of-the-art and other exploration objectives. In sparse and hard to explore environments we achieve an average improvement of over 30%."
171,171,Rapid Exploration for Open-World Navigation with Latent Goal Models,https://openreview.net/forum?id=d_SWJhyKfVw,"inverse reinforcement learning, imitation learning, self-supervised learning","We investigate the visual cross-embodiment imitation setting, in which agents learn policies from videos of other agents (such as humans) demonstrating the same task, but with stark differences in their embodiments -- shape, actions, end-effector dynamics, etc. In this work, we demonstrate that it is possible to automatically discover and learn vision-based reward functions from cross-embodiment demonstration videos that are robust to these differences. Specifically, we present a self-supervised method for Cross-embodiment Inverse Reinforcement Learning (XIRL) that leverages temporal cycle-consistency constraints to learn deep visual embeddings that capture task progression from offline videos of demonstrations across multiple expert agents, each performing the same task differently due to embodiment differences. Prior to our work, producing rewards from self-supervised embeddings typically required alignment with a reference trajectory, which may be difficult to acquire under stark embodiment differences. We show empirically that if the embeddings are aware of task-progress, simply taking the negative distance between the current state and goal state in the learned embedding space is useful as a reward for training policies with reinforcement learning. We find our learned reward function not only works for embodiments seen during training, but also generalizes to entirely new embodiments. Additionally, when transferring real-world human demonstrations to a simulated robot, we find that XIRL is more sample efficient than current best methods."
183,183,Motivating Physical Activity via Competitive Human-Robot Interaction,https://openreview.net/forum?id=s3tu6Ma1jga,"Competitive Human-robot Interaction, Reinforcement Learning, HRI, Adversarial Learning, Multi-agent System","This project aims to motivate research in competitive human-robot interaction by creating a robot competitor that can challenge human users in certain scenarios such as physical exercise and games. With this goal in mind, we introduce the Fencing Game, a human-robot competition used to evaluate both the capabilities of the robot competitor and user experience. We develop the robot competitor through multi-agent reinforcement learning and show that it can perform well against human competitors. Our user study additionally found that our system was able to continuously create challenging and enjoyable interactions for humans and the majority of human subjects considered the system to be entertaining and useful for improving the quality of their exercise."
